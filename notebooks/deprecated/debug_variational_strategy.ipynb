{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data\n",
    "Currently supported datasets: power, protein, kin8nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import geometric_kernels.torch \n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str = DEFAULT_URL):\n",
    "        super().__init__(name='kin8nm', path=path, normalize=normalize, seed=seed)\n",
    "        self.url = url \n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model\n",
    "\n",
    "This is done using the same model arguments as for the benchmarking experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = torch.tensor(2.0)\n",
    "        self.register_parameter('b', torch.nn.Parameter(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "from gpytorch.means import Mean\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "\n",
    "\n",
    "import torch \n",
    "from gpytorch import Module, settings\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.utils.memoize import cached, clear_cache_hook\n",
    "from linear_operator.operators import DiagLinearOperator\n",
    "from functools import cached_property\n",
    "# TODO Maybe just move the functions from spherical_harmonic_features.py into this file?\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree, matern_Kuu, matern_LT_Phi\n",
    "\n",
    "\n",
    "class SphericalHarmonicFeaturesVariationalStrategy(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: CholeskyVariationalDistribution,\n",
    "        dimension: int, \n",
    "        num_spherical_harmonics: int, \n",
    "        jitter_val: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # model, set via object.__setattr__ to avoid treatment as a module, parameter, or buffer\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "        # spherical harmonics \n",
    "        self.dimension = dimension \n",
    "        self.degree, self.num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @property\n",
    "    def model(self) -> ApproximateGP:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def covar_module(self) -> ScaleKernel | GeometricMaternKernel:\n",
    "        return self.model.covar_module\n",
    "    \n",
    "    @cached_property\n",
    "    def base_kernel(self) -> GeometricMaternKernel:\n",
    "        if isinstance(self.covar_module, ScaleKernel):\n",
    "            return self.covar_module.base_kernel\n",
    "        else:\n",
    "            return self.covar_module\n",
    "    \n",
    "    @property\n",
    "    def mean_module(self) -> Mean:\n",
    "        return self.model.mean_module\n",
    "\n",
    "    @property\n",
    "    def kappa(self) -> Tensor:\n",
    "        return self.base_kernel.lengthscale\n",
    "    \n",
    "    @property\n",
    "    def nu(self) -> Tensor | float:\n",
    "        return self.base_kernel.nu\n",
    "    \n",
    "    @property \n",
    "    def outputscale(self) -> Tensor:\n",
    "        return self.covar_module.outputscale if hasattr(self.covar_module, \"outputscale\") else torch.tensor(1.0)\n",
    "\n",
    "    @property \n",
    "    def sigma(self) -> Tensor: \n",
    "        return self.outputscale.sqrt()\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    @cached(name=\"prior_distribution_memo\")\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        covariance_matrix = DiagLinearOperator(torch.ones(self.num_spherical_harmonics))\n",
    "        mean = torch.zeros(self.num_spherical_harmonics)\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=covariance_matrix)\n",
    "    \n",
    "    @property \n",
    "    @cached(name=\"cholesky_factor_prior_memo\")\n",
    "    def cholesky_factor_prior(self) -> DiagLinearOperator:\n",
    "        Kuu = matern_Kuu(max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma)\n",
    "        return Kuu.cholesky() # Kuu is a DiagLinearOperator, so .cholesky() is equivalent to .sqrt() \n",
    "        \n",
    "    @property\n",
    "    @cached(name=\"variational_distribution_memo\")\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs) -> MultivariateNormal:\n",
    "        # inducing-inducing prior\n",
    "        pu = self.prior_distribution\n",
    "        invL_muu, invL_Kuu_invLt = pu.mean, pu.lazy_covariance_matrix\n",
    "\n",
    "        # input-input prior\n",
    "        px = self.model.forward(x)\n",
    "        mux, Kxx = px.mean, px.lazy_covariance_matrix\n",
    "\n",
    "        # inducing-inducing variational\n",
    "        qu = self.variational_distribution\n",
    "        invL_m, invL_S_invLt = qu.mean, qu.lazy_covariance_matrix\n",
    "\n",
    "        # Add jitter to Kxx and invL_Kuu_invLt for numerical stability\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "        invL_Kuu_invLt = invL_Kuu_invLt.add_jitter(self.jitter_val)\n",
    "\n",
    "        # inducing-input prior  \n",
    "        LT_Phi = matern_LT_Phi(\n",
    "            x, max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma,\n",
    "        ) # [..., O, num_harmonics, N]\n",
    "        \n",
    "        # Update the mean\n",
    "        mean_update = torch.einsum('...ij,...i->...j', LT_Phi, invL_m - invL_muu) # [..., O, num_harmonics, N] @ [O, num_harmonics] -> [..., O, N]\n",
    "        updated_mean = mux + mean_update # [..., O, N] + [..., O, N] -> [..., O, N]\n",
    "\n",
    "        # Update the covariance matrix\n",
    "        covariance_matrix_update = LT_Phi.mT @ (invL_S_invLt - invL_Kuu_invLt) @ LT_Phi # [O, num_harmonics, num_harmonics] @ [O, num_harmonics, num_harmonics] @ [O, num_harmonics, N] -> [O, num_harmonics, N]\n",
    "        updated_covariance_matrix = Kxx + covariance_matrix_update # [..., O, N, N] + [..., O, N, N] -> [..., O, N, N]\n",
    "\n",
    "        return MultivariateNormal(mean=updated_mean, covariance_matrix=updated_covariance_matrix)\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor \n",
    "from gpytorch.means import Mean\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "\n",
    "\n",
    "import torch \n",
    "from gpytorch import Module, settings\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.utils.memoize import cached, clear_cache_hook\n",
    "from linear_operator.operators import DiagLinearOperator\n",
    "from functools import cached_property\n",
    "# TODO Maybe just move the functions from spherical_harmonic_features.py into this file?\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree, matern_Kuu, matern_LT_Phi\n",
    "\n",
    "\n",
    "class SphericalHarmonicFeaturesVariationalStrategy(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: CholeskyVariationalDistribution,\n",
    "        dimension: int, \n",
    "        num_spherical_harmonics: int, \n",
    "        jitter_val: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # model, set via object.__setattr__ to avoid treatment as a module, parameter, or buffer\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self._variational_distribution.initialize_variational_distribution(self.prior_distribution)\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "        # spherical harmonics \n",
    "        self.dimension = dimension \n",
    "        self.degree, self.num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @property\n",
    "    def model(self) -> ApproximateGP:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def covar_module(self) -> ScaleKernel | GeometricMaternKernel:\n",
    "        return self.model.covar_module\n",
    "    \n",
    "    @cached_property\n",
    "    def base_kernel(self) -> GeometricMaternKernel:\n",
    "        if isinstance(self.covar_module, ScaleKernel):\n",
    "            return self.covar_module.base_kernel\n",
    "        else:\n",
    "            return self.covar_module\n",
    "    \n",
    "    @property\n",
    "    def mean_module(self) -> Mean:\n",
    "        return self.model.mean_module\n",
    "\n",
    "    @property\n",
    "    def kappa(self) -> Tensor:\n",
    "        return self.base_kernel.lengthscale\n",
    "    \n",
    "    @property\n",
    "    def nu(self) -> Tensor | float:\n",
    "        return self.base_kernel.nu\n",
    "    \n",
    "    @property \n",
    "    def outputscale(self) -> Tensor:\n",
    "        return self.covar_module.outputscale if hasattr(self.covar_module, \"outputscale\") else torch.tensor(1.0)\n",
    "\n",
    "    @property \n",
    "    def sigma(self) -> Tensor: \n",
    "        return self.outputscale.sqrt()\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    @cached(name=\"prior_distribution_memo\")\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        covariance_matrix = DiagLinearOperator(torch.ones(self.num_spherical_harmonics))\n",
    "        mean = torch.zeros(self.num_spherical_harmonics)\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=covariance_matrix)\n",
    "    \n",
    "    @property \n",
    "    @cached(name=\"cholesky_factor_prior_memo\")\n",
    "    def cholesky_factor_prior(self) -> DiagLinearOperator:\n",
    "        Kuu = matern_Kuu(max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma)\n",
    "        return Kuu.cholesky() # Kuu is a DiagLinearOperator, so .cholesky() is equivalent to .sqrt() \n",
    "        \n",
    "    @property\n",
    "    @cached(name=\"variational_distribution_memo\")\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs) -> MultivariateNormal:\n",
    "        # inducing-inducing prior\n",
    "        pu = self.prior_distribution\n",
    "        invL_Kuu_invLt = pu.lazy_covariance_matrix\n",
    "\n",
    "        # input-input prior\n",
    "        mux, Kxx = self.model.mean_module(x), self.model.covar_module(x)\n",
    "\n",
    "        # inducing-inducing variational\n",
    "        qu = self.variational_distribution\n",
    "        invL_S_invLt = qu.lazy_covariance_matrix\n",
    "\n",
    "        # Add jitter to Kxx and invL_Kuu_invLt for numerical stability\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "        invL_Kuu_invLt = invL_Kuu_invLt.add_jitter(self.jitter_val)\n",
    "\n",
    "        # inducing-input prior  \n",
    "        LT_Phi = matern_LT_Phi(\n",
    "            x, max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma,\n",
    "        ) # [..., O, num_harmonics, N]\n",
    "        \n",
    "        # Update the mean\n",
    "        updated_mean = mux\n",
    "\n",
    "        # Update the covariance matrix\n",
    "        covariance_matrix_update = LT_Phi.mT @ (invL_S_invLt - invL_Kuu_invLt) @ LT_Phi # [O, num_harmonics, num_harmonics] @ [O, num_harmonics, num_harmonics] @ [O, num_harmonics, N] -> [O, num_harmonics, N]\n",
    "        updated_covariance_matrix = Kxx + covariance_matrix_update # [..., O, N, N] + [..., O, N, N] -> [..., O, N, N]\n",
    "\n",
    "        return MultivariateNormal(mean=updated_mean, covariance_matrix=updated_covariance_matrix)\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalHarmonicFeaturesVariationalStrategy(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: CholeskyVariationalDistribution,\n",
    "        dimension: int, \n",
    "        num_spherical_harmonics: int, \n",
    "        jitter_val: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # model, set via object.__setattr__ to avoid treatment as a module, parameter, or buffer\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "        # spherical harmonics \n",
    "        self.dimension = dimension \n",
    "        self.degree, self.num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @property\n",
    "    def model(self) -> ApproximateGP:\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def kappa(self) -> Tensor:\n",
    "        return torch.tensor([[0.001]])\n",
    "    \n",
    "    @property\n",
    "    def nu(self) -> Tensor | float:\n",
    "        return torch.tensor([[2.5]])\n",
    "\n",
    "    @property \n",
    "    def sigma(self) -> Tensor: \n",
    "        return torch.tensor(1.0)\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        covariance_matrix = DiagLinearOperator(torch.ones(self.num_spherical_harmonics))\n",
    "        mean = torch.zeros(self.num_spherical_harmonics)\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=covariance_matrix)\n",
    "        \n",
    "    @property\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs) -> MultivariateNormal:\n",
    "        # inducing-inducing prior\n",
    "        pu = self.prior_distribution\n",
    "        invL_Kuu_invLt = pu.lazy_covariance_matrix\n",
    "\n",
    "        # input-input prior\n",
    "        mux, Kxx = self.model.mean_module(x), self.model.covar_module(x)\n",
    "\n",
    "        # inducing-inducing variational\n",
    "        qu = self.variational_distribution\n",
    "        invL_S_invLt = qu.lazy_covariance_matrix\n",
    "\n",
    "        # Add jitter to Kxx and invL_Kuu_invLt for numerical stability\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "        invL_Kuu_invLt = invL_Kuu_invLt.add_jitter(self.jitter_val)\n",
    "\n",
    "        # inducing-input prior  \n",
    "        LT_Phi = matern_LT_Phi(\n",
    "            x, max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma,\n",
    "        ) # [..., O, num_harmonics, N]\n",
    "        \n",
    "        # Update the mean\n",
    "        updated_mean = mux\n",
    "\n",
    "        # Update the covariance matrix\n",
    "        covariance_matrix_update = LT_Phi.mT @ (invL_S_invLt - invL_Kuu_invLt) @ LT_Phi # [O, num_harmonics, num_harmonics] @ [O, num_harmonics, num_harmonics] @ [O, num_harmonics, N] -> [O, num_harmonics, N]\n",
    "        updated_covariance_matrix = Kxx + covariance_matrix_update # [..., O, N, N] + [..., O, N, N] -> [..., O, N, N]\n",
    "\n",
    "        return MultivariateNormal(mean=updated_mean, covariance_matrix=updated_covariance_matrix)\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.utils.spherical_harmonic_features import matern_Kux, matern_repeated_ahat\n",
    "\n",
    "def matern_Phi(x: Tensor, max_ell: int, d: int, kappa: float, nu: float, rsh: bool = True, sigma: float = 1.0) -> Tensor: \n",
    "    \"\"\"\n",
    "    Returns the feature vector of spherical harmonics evaluated at x. \n",
    "    \"\"\"\n",
    "    Kux = matern_Kux(x, max_ell=max_ell, d=d) # [*B, num_harmonics, n]\n",
    "    ahat = matern_repeated_ahat(max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma).unsqueeze(-1) # [num_harmonics, 1]\n",
    "    return Kux * ahat # [*B, num_harmonics, n]\n",
    "\n",
    "\n",
    "class SphericalHarmonicFeaturesVariationalStrategy(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: CholeskyVariationalDistribution,\n",
    "        dimension: int, \n",
    "        num_spherical_harmonics: int, \n",
    "        jitter_val: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # model, set via object.__setattr__ to avoid treatment as a module, parameter, or buffer\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "        # spherical harmonics \n",
    "        self.dimension = dimension \n",
    "        self.degree, self.num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @property\n",
    "    def model(self) -> ApproximateGP:\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def kappa(self) -> Tensor:\n",
    "        return torch.tensor([[0.001]])\n",
    "    \n",
    "    @property\n",
    "    def nu(self) -> Tensor | float:\n",
    "        return torch.tensor([[2.5]])\n",
    "\n",
    "    @property \n",
    "    def sigma(self) -> Tensor: \n",
    "        return torch.tensor(1.0)\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        covariance_matrix = matern_Kuu(\n",
    "            max_ell=self.degree, \n",
    "            d=self.dimension,\n",
    "            kappa=self.kappa,\n",
    "            nu=self.nu,\n",
    "            sigma=self.sigma,\n",
    "        )\n",
    "        mean = torch.zeros(self.num_spherical_harmonics)\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=covariance_matrix)\n",
    "        \n",
    "    @property\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        **kwargs,\n",
    "    ) -> MultivariateNormal:\n",
    "        \n",
    "        # inducing-inducing prior\n",
    "        qu = self.variational_distribution\n",
    "        S = qu.lazy_covariance_matrix\n",
    "\n",
    "        # inducing variables prior\n",
    "        fu_mvn = self.prior_distribution\n",
    "        muu, Kuu = fu_mvn.mean, fu_mvn.lazy_covariance_matrix\n",
    "\n",
    "        # input points prior \n",
    "        mux, Kxx = self.model.mean_module(x), self.model.covar_module(x)\n",
    "\n",
    "        # compute Phi, consdering that matern_Phi does not accept batch dimensions \n",
    "        Phi = matern_Phi(x, self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma) \n",
    "        updated_covariance_matrix = Kxx + Phi.mT @ (S - Kuu) @ Phi\n",
    "        updated_mean = mux\n",
    "\n",
    "        return MultivariateNormal(mean=updated_mean, covariance_matrix=updated_covariance_matrix)\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple shallow variational GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "\n",
    "class SHApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, dimension: int, num_spherical_harmonics: int, \n",
    "                 mean: Mean, covar_module: GeometricMaternKernel, variational_distribution: CholeskyVariationalDistribution, jitter_val: float | None = None):\n",
    "        variational_strategy = SphericalHarmonicFeaturesVariationalStrategy(\n",
    "            model=self,\n",
    "            variational_distribution=variational_distribution,\n",
    "            dimension=dimension,\n",
    "            num_spherical_harmonics=num_spherical_harmonics,\n",
    "            jitter_val=jitter_val\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = mean\n",
    "        self.covar_module = covar_module\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "class IPApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, mean: Mean, covar_module: GeometricMaternKernel, inducing_points: Tensor, variational_distribution: CholeskyVariationalDistribution):\n",
    "        variational_strategy = gpytorch.variational.UnwhitenedVariationalStrategy(\n",
    "            self, \n",
    "            inducing_points=inducing_points,\n",
    "            variational_distribution=variational_distribution, \n",
    "            learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = mean\n",
    "        self.covar_module = covar_module\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Training has to be done differently from the bechmarking experiment, because we need minibatch SGD with the larger datasets and minibatch metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_smallest_eigenvalues(covar):\n",
    "    # mvn = projector.inverse(mvn)\n",
    "    smallest_eigenvalues = torch.linalg.eigvalsh(covar).min()\n",
    "    print(f\"Smallest eigenvalue: {smallest_eigenvalues.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 54, which includes all spherical harmonics up to degree 3 (incl.)\n"
     ]
    }
   ],
   "source": [
    "from geometric_kernels.spaces import Hypersphere\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree\n",
    "\n",
    "\n",
    "dataset = Kin8mn()\n",
    "\n",
    "# Generic parameters\n",
    "num_spherical_harmonics = 50\n",
    "dimension = dataset.dimension + 1\n",
    "\n",
    "degree, num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "space = Hypersphere(dimension)\n",
    "\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Model with spherical harmonic features\n",
    "mean = gpytorch.means.ZeroMean()\n",
    "covar_module = GeometricMaternKernel(nu=2.5, space=space, num_eigenfunctions=4, batch_shape=batch_shape)\n",
    "covar_module.initialize(lengthscale=0.001)\n",
    "variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "    num_inducing_points=num_spherical_harmonics, batch_shape=batch_shape\n",
    ")\n",
    "\n",
    "model_sh = SHApproximateGP(\n",
    "    dimension=dimension,\n",
    "    num_spherical_harmonics=num_spherical_harmonics,\n",
    "    mean=mean,\n",
    "    covar_module=covar_module,\n",
    "    variational_distribution=variational_distribution,\n",
    ")\n",
    "\n",
    "# Model with inducing points \n",
    "mean = gpytorch.means.ZeroMean()\n",
    "covar_module = GeometricMaternKernel(nu=2.5, space=space, num_eigenfunctions=4, batch_shape=batch_shape)\n",
    "covar_module.initialize(lengthscale=0.001)\n",
    "variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "    num_inducing_points=num_spherical_harmonics, batch_shape=batch_shape\n",
    ")\n",
    "\n",
    "inducing_points = torch.randn(num_spherical_harmonics, dimension)\n",
    "inducing_points = inducing_points / inducing_points.norm(dim=-1, keepdim=True)\n",
    "model_ip = IPApproximateGP(\n",
    "    mean=mean,\n",
    "    covar_module=covar_module,\n",
    "    inducing_points=inducing_points,\n",
    "    variational_distribution=variational_distribution\n",
    ")\n",
    "\n",
    "\n",
    "# Arbitrary projector \n",
    "projector = SphereProjector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue: -9.077262895129363e-16\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print_smallest_eigenvalues(model.variational_strategy.variational_distribution.covariance_matrix)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_batch)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "model = model_sh\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# \n",
    "parameters = [\n",
    "    model.variational_strategy._variational_distribution.chol_variational_covar,\n",
    "]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_x.size(0))\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    x_batch, y_batch = projector(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    output = model.variational_strategy(x_batch)\n",
    "    print_smallest_eigenvalues(output.covariance_matrix)\n",
    "    # print_smallest_eigenvalues(model.variational_strategy.variational_distribution.covariance_matrix)\n",
    "    loss = -mll(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue: 9.999999988798501e-07\n",
      "1.5182695768211714\n",
      "Smallest eigenvalue: 9.999999989562332e-07\n",
      "1.4947433100617336\n",
      "Smallest eigenvalue: 9.99999998908287e-07\n",
      "1.4414993314065088\n",
      "Smallest eigenvalue: 9.999999988947179e-07\n",
      "1.4041950480701977\n",
      "Smallest eigenvalue: 9.999999990619052e-07\n",
      "1.3796913437218985\n",
      "Smallest eigenvalue: 9.999999991144903e-07\n",
      "1.3637745642671706\n",
      "Smallest eigenvalue: 9.999999990971003e-07\n",
      "1.3576423747766195\n",
      "Smallest eigenvalue: 9.999999990506075e-07\n",
      "1.3314937268999756\n",
      "Smallest eigenvalue: 9.9999999894546e-07\n",
      "1.3163540589840574\n",
      "Smallest eigenvalue: 9.99999999111952e-07\n",
      "1.3129889125774872\n",
      "Smallest eigenvalue: 9.999999990953514e-07\n",
      "1.3158454601627185\n",
      "Smallest eigenvalue: 9.999999990809995e-07\n",
      "1.323727298242035\n",
      "Smallest eigenvalue: 9.999999991838973e-07\n",
      "1.321055025913\n",
      "Smallest eigenvalue: 9.999999990535063e-07\n",
      "1.3224700873685378\n",
      "Smallest eigenvalue: 9.999999991874656e-07\n",
      "1.314687115369776\n",
      "Smallest eigenvalue: 9.999999991031666e-07\n",
      "1.3197032585534363\n",
      "Smallest eigenvalue: 9.999999990905072e-07\n",
      "1.3025987587707133\n",
      "Smallest eigenvalue: 9.999999991054512e-07\n",
      "1.3050405370506881\n",
      "Smallest eigenvalue: 9.999999992019376e-07\n",
      "1.3067107869455115\n",
      "Smallest eigenvalue: 9.999999990118287e-07\n",
      "1.304530700425213\n",
      "Smallest eigenvalue: 9.999999990095876e-07\n",
      "1.3092560000679492\n",
      "Smallest eigenvalue: 9.99999999079284e-07\n",
      "1.3116082613670794\n",
      "Smallest eigenvalue: 9.999999990838131e-07\n",
      "1.2969262832222974\n",
      "Smallest eigenvalue: 9.999999991489845e-07\n",
      "1.3213335387188043\n",
      "Smallest eigenvalue: 9.999999991351312e-07\n",
      "1.299386153966815\n",
      "Smallest eigenvalue: 9.99999999031657e-07\n",
      "1.2998667549983527\n",
      "Smallest eigenvalue: 9.999999991025478e-07\n",
      "1.2892492662864095\n",
      "Smallest eigenvalue: 9.999999989155013e-07\n",
      "1.3155163803022312\n",
      "Smallest eigenvalue: 1.0161330979551277e-06\n",
      "1.307991938396744\n"
     ]
    }
   ],
   "source": [
    "model = model_ip\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# \n",
    "parameters = [\n",
    "    model.variational_strategy._variational_distribution.chol_variational_covar,\n",
    "]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_x.size(0))\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    x_batch, y_batch = projector(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_batch)\n",
    "    print_smallest_eigenvalues(output.covariance_matrix)\n",
    "    loss = -mll(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def plot_kernel_vs_angle(kernel, dimension: int, n: int = 100): \n",
    "    pole = torch.zeros(1, dimension)\n",
    "    pole[:, -1] = 1.\n",
    "    theta = torch.linspace(0, torch.pi, n)\n",
    "    x = torch.cat([torch.zeros(n, dimension - 2), theta.cos().unsqueeze(-1), theta.sin().unsqueeze(-1)], dim=-1)\n",
    "    with torch.no_grad():\n",
    "        y = kernel(x).lazy_covariance_matrix[..., 0]\n",
    "        if y.ndim == 2: \n",
    "            y = y.mean(0)\n",
    "\n",
    "    data = pd.DataFrame({'theta': theta.squeeze().numpy(), 'y': y})\n",
    "    fig = px.line(data, x='theta', y='y')\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
