{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the spherical harmonics paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n",
      "/tmp/ipykernel_167315/3809226695.py:14: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "\n",
    "import torch \n",
    "import gpytorch \n",
    "import geometric_kernels.torch \n",
    "from math import comb \n",
    "from spherical_harmonics import SphericalHarmonics\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from linear_operator.operators import DiagLinearOperator, LinearOperator\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import negative_log_predictive_density, mean_squared_error\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_harmonics_single(ell: int, d: int) -> int:\n",
    "    r\"\"\"\n",
    "    Number of spherical harmonics of degree ell on S^{d - 1}.\n",
    "    \"\"\"\n",
    "    if ell == 0:\n",
    "        return 1\n",
    "    if d == 3:\n",
    "        return 2 * ell + 1\n",
    "    else:\n",
    "        return (2 * ell + d - 2) * comb(ell + d - 3, ell - 1) // ell\n",
    "\n",
    "\n",
    "def num_harmonics(ell: Tensor | int, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized version of num_harmonics_single\n",
    "    \"\"\"\n",
    "    if isinstance(ell, int):\n",
    "        return num_harmonics_single(ell, d)\n",
    "    return ell.apply_(lambda e: num_harmonics_single(ell=e, d=d))\n",
    "\n",
    "\n",
    "def total_num_harmonics(max_ell: int, d: int) -> int:\n",
    "    \"\"\"\n",
    "    Total number of spherical harmonics on S^{d-1} with degree <= max_ell\n",
    "    \"\"\"\n",
    "    return int(sum(num_harmonics(ell=torch.arange(max_ell + 1), d=d)))\n",
    "\n",
    "\n",
    "def eigenvalue_laplacian(ell: Tensor, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Eigenvalue of the Laplace-Beltrami operator for a spherical harmonic of degree ell on S_{d-1}\n",
    "    ell: [...]\n",
    "    d: []\n",
    "    return: [...]\n",
    "    \"\"\"\n",
    "    return ell * (ell + d - 2)\n",
    "\n",
    "\n",
    "def unnormalized_matern_spectral_density(n: Tensor, d: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor | float: \n",
    "    \"\"\"\n",
    "    compute (unnormalized) spectral density of the matern kernel on S_{d-1}\n",
    "    n: [N]\n",
    "    d: []\n",
    "    kappa: [O, 1, 1]\n",
    "    nu: [O, 1, 1]\n",
    "    return: [O, 1, N]\n",
    "    \"\"\"\n",
    "    # Squared exponential kernel \n",
    "    if torch.all(nu.isinf()):\n",
    "        exponent = -kappa ** 2 / 2 * eigenvalue_laplacian(ell=n, d=d) # [O, N, 1]\n",
    "        return torch.exp(exponent)\n",
    "    # Matern kernel\n",
    "    else:\n",
    "        base = (\n",
    "            2.0 * nu / kappa**2 + # [O, 1, 1]\n",
    "            eigenvalue_laplacian(ell=n, d=d).unsqueeze(-1) # [N, 1]\n",
    "        ) # [O, N, 1]\n",
    "        exponent = -nu - (d - 1) / 2.0 # [O, 1, 1]\n",
    "        return base ** exponent # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density_normalizer(d: int, max_ell: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalizing constant for the spectral density of the Matern kernel on S^{d-1}. \n",
    "    Depends on kappa and nu. Also depends on max_ell, as truncation of the infinite \n",
    "    sum from Karhunen-Loeve decomposition. \n",
    "    \"\"\"\n",
    "    n = torch.arange(max_ell + 1)\n",
    "    spectral_values = unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) # [O, max_ell + 1, 1]\n",
    "    num_harmonics_per_level = num_harmonics(torch.arange(max_ell + 1), d=d).type(spectral_values.dtype) # [max_ell + 1]\n",
    "    normalizer = spectral_values.mT @ num_harmonics_per_level # [O, 1, max_ell + 1] @ [max_ell + 1] -> [O, 1]\n",
    "    return normalizer.unsqueeze(-2) # [O, 1, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density(n: Tensor, d: int, kappa: Tensor, nu: Tensor, max_ell: int, max_ell_prior: int, sigma: float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Spectral density of the Matern kernel on S^{d-1}\n",
    "    \"\"\"\n",
    "    return (\n",
    "        unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) / # [O, N, 1]\n",
    "        matern_spectral_density_normalizer(d=d, max_ell=max_ell_prior, kappa=kappa, nu=nu) * # [O, 1, 1]\n",
    "        (sigma ** 2)[..., *(None,) * (kappa.ndim - 1)] # [O, 1, 1]\n",
    "    ) # [O, N, 1] / [O, 1, 1] * [O, 1, 1] -> [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_ahat(ell: Tensor, d: int, max_ell: int, max_ell_prior: int, kappa: Tensor | float, nu: Tensor | float,\n",
    "                m: int | None = None, sigma: Tensor | float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    :math: `\\hat{a} = \\rho(\\ell)` where :math: `\\rho` is the spectral density on S^{d-1}\n",
    "    \"\"\"\n",
    "    return matern_spectral_density(n=ell, d=d, kappa=kappa, nu=nu, max_ell=max_ell, max_ell_prior=max_ell_prior, sigma=sigma) # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_repeated_ahat(max_ell: int, max_ell_prior: int, d: int, kappa: Tensor | float, nu: Tensor | float, sigma: Tensor | float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of repeated ahat values for each ell. \n",
    "    \"\"\"\n",
    "    ells = torch.arange(max_ell + 1) # [max_ell + 1]\n",
    "    ahat = matern_ahat(ell=ells, d=d, max_ell=max_ell, max_ell_prior=max_ell_prior, kappa=kappa, nu=nu, sigma=sigma) # [O, max_ell + 1, 1]\n",
    "    repeats = num_harmonics(ell=ells, d=d) # [max_ell + 1]\n",
    "    return torch.repeat_interleave(ahat, repeats=repeats, dim=-2) # [O, num_harmonics, 1]\n",
    "\n",
    "\n",
    "def matern_Kuu(max_ell: int, d: int, kappa: float, nu: float, sigma: float = 1.0) -> Tensor: \n",
    "    \"\"\"\n",
    "    Returns the covariance matrix, which is a diagonal matrix with entries \n",
    "    equal to inv_ahat of the corresponding ell. \n",
    "    \"\"\"\n",
    "    return torch.diag(1 / matern_repeated_ahat(max_ell, d, kappa, nu, sigma=sigma).squeeze(-1)) # [O, num_harmonics, num_harmonics]\n",
    "\n",
    "\n",
    "def spherical_harmonics(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    # Make sure that x is at least 2d and flatten it\n",
    "    x = torch.atleast_2d(x)\n",
    "    batch_shape, n = x.shape[:-2], x.shape[-2]\n",
    "    x = x.flatten(0, -2)\n",
    "\n",
    "    # Get spherical harmonics callable\n",
    "    f = SphericalHarmonics(dimension=d, degrees=max_ell + 1) # [... * O, N, num_harmonics]\n",
    "\n",
    "    # Evaluate x and reintroduce batch dimensions\n",
    "    return f(x).reshape(*batch_shape, n, total_num_harmonics(max_ell, d)) # [..., O, N, num_harmonics]\n",
    "\n",
    "\n",
    "def matern_Kux(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    return spherical_harmonics(x, max_ell=max_ell, d=d).mT # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def num_spherical_harmonics_to_degree(num_spherical_harmonics: int, dimension: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the minimum degree for which there are at least\n",
    "    `num_eigenfunctions` in the collection.\n",
    "    \"\"\"\n",
    "    n, degree = 0, 0  # n: number of harmonics, d: degree (or level)\n",
    "    while n < num_spherical_harmonics:\n",
    "        n += num_harmonics(d=dimension, ell=degree)\n",
    "        degree += 1\n",
    "\n",
    "    if n > num_spherical_harmonics:\n",
    "        print(\n",
    "            \"The number of spherical harmonics requested does not lead to complete \"\n",
    "            \"levels of spherical harmonics. We have thus increased the number to \"\n",
    "            f\"{n}, which includes all spherical harmonics up to degree {degree} (incl.)\"\n",
    "        )\n",
    "    return degree - 1, n\n",
    "\n",
    "\n",
    "from gpytorch.kernels import Kernel, ScaleKernel\n",
    "from mdgp.kernels.matern_kernel import _GeometricMaternKernel\n",
    "\n",
    "\n",
    "def matern_LT_Phi(x: Tensor, max_ell: int, d: int, kappa: float, nu: float, max_ell_prior: int, sigma: float = 1.0) -> Tensor: \n",
    "    Kux = matern_Kux(x, max_ell=max_ell, d=d) # [..., O, num_harmonics, N]\n",
    "    ahat_sqrt = matern_repeated_ahat(max_ell=max_ell, max_ell_prior=max_ell_prior, d=d, kappa=kappa, nu=nu, sigma=sigma).sqrt() # [O, num_harmonics, 1]\n",
    "    return Kux * ahat_sqrt # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def matern_LT_Phi_from_kernel(x: Tensor, covar_module: Kernel, num_levels: int, num_levels_prior: int) -> Tensor: \n",
    "    # Extract kernel parameters  \n",
    "    if isinstance(covar_module, ScaleKernel):\n",
    "        sigma = covar_module.outputscale.sqrt()\n",
    "        base_kernel = covar_module.base_kernel\n",
    "    else:\n",
    "        sigma = torch.tensor(1.0, dtype=x.dtype, device=x.device)\n",
    "        base_kernel = covar_module\n",
    "    kappa = base_kernel.lengthscale\n",
    "    nu = base_kernel.nu \n",
    "\n",
    "    # Extract constants \n",
    "    d = base_kernel.space.dimension + 1\n",
    "    max_ell = num_levels\n",
    "    max_ell_prior = num_levels_prior\n",
    "\n",
    "    return matern_LT_Phi(x, max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma, max_ell_prior=max_ell_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpytorchSGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, max_ell, d, max_ell_prior, epsilon_sigma=1.0, kappa=1.0, nu=2.5, sigma=1.0, batch_shape=torch.Size([]), jitter_val=1e-6, optimize_nu: bool = True):\n",
    "        m = total_num_harmonics(max_ell, d)\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=m, batch_shape=batch_shape)\n",
    "        variational_strategy = SGPVariationalStrategy(self, variational_distribution, num_levels=max_ell, num_levels_prior=max_ell_prior, jitter_val=jitter_val)\n",
    "        super().__init__(variational_strategy=variational_strategy)\n",
    "\n",
    "        # constants \n",
    "        self.jitter_val = jitter_val\n",
    "        self.max_ell = max_ell\n",
    "        self.d = d\n",
    "\n",
    "        # modules \n",
    "        base_kernel = GeometricMaternKernel(\n",
    "            space=Hypersphere(d - 1),\n",
    "            lengthscale=kappa, \n",
    "            nu=nu, \n",
    "            trainable_nu=optimize_nu, \n",
    "            num_eigenfunctions=max_ell_prior + 1,\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "        # prior hyperparams \n",
    "        self.covar_module.outputscale = sigma ** 2\n",
    "        self.likelihood.noise = epsilon_sigma ** 2\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        p_sigma = self.covar_module(x)\n",
    "        p_mu = self.mean_module(x)\n",
    "        return MultivariateNormal(p_mu, p_sigma)\n",
    "\n",
    "\n",
    "class SGPVariationalStrategy(torch.nn.Module):\n",
    "    def __init__(self, model: gpytorchSGP, variational_distribution: CholeskyVariationalDistribution, num_levels, num_levels_prior, jitter_val=1e-6):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.jitter_val = jitter_val\n",
    "\n",
    "        # Extract references to kernel parameters \n",
    "        self.num_levels = num_levels\n",
    "        self.num_levels_prior = num_levels_prior\n",
    "\n",
    "    @property\n",
    "    def model(self) -> gpytorchSGP:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "    \n",
    "    @property\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        zeros = torch.zeros(\n",
    "            self._variational_distribution.shape(),\n",
    "            dtype=self._variational_distribution.dtype,\n",
    "            device=self._variational_distribution.device,\n",
    "        )\n",
    "        ones = torch.ones_like(zeros)\n",
    "        res = MultivariateNormal(zeros, DiagLinearOperator(ones))\n",
    "        return res\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        # prior at x \n",
    "        px = self.model.forward(x)\n",
    "        mu_x, Kxx = px.mean, px.lazy_covariance_matrix\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "\n",
    "        # whitened prior at u\n",
    "        Linv_pu = self.prior_distribution\n",
    "        Linv_mu, Linv_Kuu_LTinv = Linv_pu.mean, Linv_pu.lazy_covariance_matrix\n",
    "\n",
    "        # whitened variational posterior at u\n",
    "        Linv_qu = self.variational_distribution\n",
    "        Linv_m, Linv_S_LTinv = Linv_qu.mean, Linv_qu.lazy_covariance_matrix\n",
    "\n",
    "        # unwhitening + projection and vice-versa\n",
    "        LT_Phiux = matern_LT_Phi_from_kernel(x, self.model.covar_module, num_levels=self.num_levels, num_levels_prior=self.num_levels_prior)\n",
    "        Phixu_L = LT_Phiux.mT\n",
    "\n",
    "        # posterior at x \n",
    "        qx_sigma = Kxx + Phixu_L @ (Linv_S_LTinv - Linv_Kuu_LTinv) @ LT_Phiux\n",
    "        qx_sigma = qx_sigma.add_jitter(self.jitter_val)\n",
    "        qx_mu = mu_x + Phixu_L @ (Linv_m - Linv_mu)\n",
    "        return MultivariateNormal(qx_mu, qx_sigma)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        with gpytorch.settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        if prior:\n",
    "            return self.model.forward(x)\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self, b: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_psd(S, tol=1e-8):\n",
    "    if isinstance(S, LinearOperator):\n",
    "        S = S.to_dense()\n",
    "    assert torch.allclose(S, S.mT), \"K should be symmetric.\"\n",
    "\n",
    "    eigs = torch.linalg.eigvalsh(S)\n",
    "    assert (eigs > -tol).all(), \"K should be positive definite.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, url: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.url = url\n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Kin8mn.DEFAULT_URL\n",
    "        super().__init__(name='kin8nm', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "import csv \n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "uci_base = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "\n",
    "class Power(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = uci_base + \"00294/CCPP.zip\"\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Power.DEFAULT_URL\n",
    "        super().__init__(name='power', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        with urlopen(self.url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall('/tmp/')\n",
    "\n",
    "        df = pd.read_excel('/tmp/CCPP//Folds5x2_pp.xlsx')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "class Concrete(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = uci_base + 'concrete/compressive/Concrete_Data.xls'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Concrete.DEFAULT_URL\n",
    "        super().__init__(name='concrete', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        df = pd.read_excel(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters as in the spherical harmonics paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets \n",
    "kin8mn = Kin8mn()\n",
    "power = Power()\n",
    "concrete = Concrete()\n",
    "\n",
    "# Variational parameters\n",
    "dimension_to_num_harmonics_variational = {\n",
    "    4: 336,\n",
    "    6: 294,\n",
    "    8: 210,\n",
    "}\n",
    "\n",
    "# Prior parameters \n",
    "\"\"\"\n",
    "NOTE \n",
    "- It is difficult to say what number of spherical harmonics was used for the prior.\n",
    "  I set it to be the same as the number of inducing variables.\n",
    "- It is also difficult to say what lengthscale initialisation was used in the paper. \n",
    "  I set it to 1.0 for now, although a lower number, e.g. 0.001, would likely be better for higher dimensions.\n",
    "\"\"\"\n",
    "dimension_to_num_harmonics_prior = {\n",
    "    4: 336,\n",
    "    6: 294,\n",
    "    8: 211, \n",
    "}\n",
    "nu = 1.5\n",
    "optimize_nu = False\n",
    "kappa = 1.0\n",
    "\n",
    "# Other model parameters\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Training parameters \n",
    "batch_size = 256 \n",
    "LR = 0.01\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to degree 5 (incl.)\n"
     ]
    }
   ],
   "source": [
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree\n",
    "\n",
    "\n",
    "def get_model_and_projector(dataset: UCIDataset):\n",
    "    sphere_dimension = dataset.dimension + 1\n",
    "\n",
    "    # number of levels for variational inference \n",
    "    num_spherical_harmonics = dimension_to_num_harmonics_variational[dataset.dimension]\n",
    "    max_ell, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics, sphere_dimension)\n",
    "\n",
    "    # number of levels for prior\n",
    "    num_spherical_harmonics_prior = dimension_to_num_harmonics_prior[dataset.dimension]\n",
    "    max_ell_prior, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics_prior, sphere_dimension)\n",
    "\n",
    "    model = gpytorchSGP(max_ell=max_ell, d=sphere_dimension, max_ell_prior=max_ell_prior, kappa=kappa, nu=nu, optimize_nu=optimize_nu, batch_shape=batch_shape)\n",
    "    projector = SphereProjector()\n",
    "    return model, projector\n",
    "\n",
    "\n",
    "model, projector = get_model_and_projector(kin8mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, projector, optimizer, mll) -> float:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x, y = projector(x, y)\n",
    "    output = model(x)\n",
    "    test_psd(output.lazy_covariance_matrix)\n",
    "    loss = mll(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset, model, projector, num_epochs=NUM_EPOCHS, lr=LR) -> list[float]: \n",
    "    # optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, maximize=True)\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_y.size(0))\n",
    "\n",
    "    # data\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, projector=projector, optimizer=optimizer, mll=mll)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': losses[-1]})\n",
    "\n",
    "    return losses \n",
    "\n",
    "\n",
    "def evaluate(dataset, model, projector):\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = projector(dataset.test_x), dataset.test_y\n",
    "        out = model.likelihood(model(test_x))\n",
    "        out = projector.inverse(out)\n",
    "        nlpd = negative_log_predictive_density(out, test_y)\n",
    "        mse = mean_squared_error(out, test_y)\n",
    "        metrics = {\n",
    "            'nlpd': nlpd.item(), \n",
    "            'mse': mse.item(),\n",
    "        }\n",
    "        print(f\"NLPD: {metrics['nlpd']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_runs: int = 5, num_epochs=NUM_EPOCHS, lr=LR):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "\n",
    "        torch.random.manual_seed(run)\n",
    "        model, projector = get_model_and_projector(dataset)\n",
    "        train(dataset, model, projector, num_epochs=num_epochs, lr=lr)\n",
    "        run_metrics = evaluate(dataset, model, projector)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Mapping\n",
    "\n",
    "\n",
    "def gen_permutations(c: Mapping[str, int]):\n",
    "    c = {k: v for k, v in c.items() if v > 0}\n",
    "    if len(c) == 0:\n",
    "        yield \"\"\n",
    "        return \n",
    "    for k, v in c.items():\n",
    "        c[k] -= 1\n",
    "        for p in gen_permutations(c):\n",
    "            yield k + p\n",
    "        c[k] += 1\n",
    "\n",
    "\n",
    "def permutations(s):\n",
    "    return list(gen_permutations(Counter(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_smaller_than(l: list[int], m: int, start: int = 0) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the largest number in l that is smaller than m.\n",
    "    \"\"\"\n",
    "    return max(filter(lambda i: l[i] < m, range(start, len(l))))\n",
    "\n",
    "\n",
    "from itertools import pairwise \n",
    "\n",
    "\n",
    "def next_smaller(n: int) -> int:\n",
    "    # Scan from right to left until we find a number larger than . \n",
    "    # Replace it with the largest previously seen number. \n",
    "    # Sort the rest in a descending order.\n",
    "    n = str(n)\n",
    "    for i, (d_right, d) in zip(reversed(range(len(n) - 1)), (pairwise(reversed(n)))):\n",
    "        if d > d_right:\n",
    "            j = argmax_smaller_than(n, d, start=i + 1)\n",
    "            return int(n[:i] + n[j] + ''.join(sorted(n[i:j] + n[j+1:], reverse=True)))\n",
    "        print(d, d_right)\n",
    "        if d > d_right:\n",
    "            print(d_right, d, i)\n",
    "            j = argmax_smaller_than(n, d, start=len(n) - i)\n",
    "            return int(n[:len(n) - i - 1] + n[j] + ''.join(sorted(n[len(n) - i - 1:j] + n[j+1:], reverse=True)))\n",
    "\n",
    "\n",
    "    for i, i_minus_1 in pairwise(reversed(range(len(n)))):\n",
    "        if n[i] < n[i_minus_1]:\n",
    "            j = argmax_smaller_than(n, n[i_minus_1], start=i)\n",
    "            return int(n[:i_minus_1] + n[j] + ''.join(sorted(n[i_minus_1:j] + n[j+1:], reverse=True)))\n",
    "    for i in reversed(range(len(n) - 1)):\n",
    "        if n[i] > n[i + 1]:\n",
    "            j = argmax_smaller_than(n, n[i], start=i + 1)\n",
    "            return int(n[:i] + n[j] + ''.join(sorted(n[i:j] + n[j+1:], reverse=True)))\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_smaller(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aabc',\n",
       " 'aacb',\n",
       " 'abac',\n",
       " 'abca',\n",
       " 'acab',\n",
       " 'acba',\n",
       " 'baac',\n",
       " 'baca',\n",
       " 'bcaa',\n",
       " 'caab',\n",
       " 'caba',\n",
       " 'cbaa']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations('aabc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for kin8nm-------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n",
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to degree 5 (incl.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreproduce_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkin8mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 58\u001b[0m, in \u001b[0;36mreproduce_results\u001b[0;34m(dataset, num_runs, num_epochs, lr)\u001b[0m\n\u001b[1;32m     56\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(run)\n\u001b[1;32m     57\u001b[0m model, projector \u001b[38;5;241m=\u001b[39m get_model_and_projector(dataset)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m run_metrics \u001b[38;5;241m=\u001b[39m evaluate(dataset, model, projector)\n\u001b[1;32m     60\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(run_metrics)\n",
      "Cell \u001b[0;32mIn[34], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, projector, num_epochs, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, projector, optimizer, mll)\u001b[0m\n\u001b[1;32m      3\u001b[0m x, y \u001b[38;5;241m=\u001b[39m projector(x, y)\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m mll(output, y)\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_psd\u001b[39m(S, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(S, LinearOperator):\n\u001b[0;32m----> 3\u001b[0m         S \u001b[38;5;241m=\u001b[39m \u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36mSumLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36mSumLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:410\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/kernels/scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[0;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/geometric_kernels/frontends/pytorch/gpytorch.py:94\u001b[0m, in \u001b[0;36mGPytorchGeometricKernel.forward\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernel\u001b[38;5;241m.\u001b[39mK_diag(params, x1)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 94\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape_scaling_factor\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/geometric_kernels/kernels/geometric_kernels.py:155\u001b[0m, in \u001b[0;36mMaternKarhunenLoeveKernel.K\u001b[0;34m(self, params, X, X2, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m weights \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mcast(B\u001b[38;5;241m.\u001b[39mdtype(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnu\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues(params))  \u001b[38;5;66;03m# [M, 1]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m Phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenfunctions\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPhi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweighted_outerproduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/geometric_kernels/spaces/eigenfunctions.py:156\u001b[0m, in \u001b[0;36mEigenfunctionWithAdditionTheorem.weighted_outerproduct\u001b[0;34m(self, weights, X, X2, **parameters)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     X2 \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m--> 156\u001b[0m sum_phi_phi_for_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_addition_theorem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [..., batch_shape, N, N2, L] or [N, N2, L]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# FIXME Adapt operations to use LAB \u001b[39;00m\n\u001b[1;32m    159\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [L, 1] -> [L]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/geometric_kernels/spaces/hypersphere.py:99\u001b[0m, in \u001b[0;36mSphericalHarmonics._addition_theorem\u001b[0;34m(self, X, X2, **parameters)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_addition_theorem\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: B\u001b[38;5;241m.\u001b[39mNumeric, X2: B\u001b[38;5;241m.\u001b[39mNumeric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Returns the result of applying the additional theorem when\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    summing over all the eigenfunctions within a level, for each level\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        a value for each level [N, N2, L]\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [N1, N2, 1]\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spherical_harmonics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mharmonic_levels\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39mvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/geometric_kernels/spaces/hypersphere.py:100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_addition_theorem\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: B\u001b[38;5;241m.\u001b[39mNumeric, X2: B\u001b[38;5;241m.\u001b[39mNumeric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Returns the result of applying the additional theorem when\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    summing over all the eigenfunctions within a level, for each level\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        a value for each level [N, N2, L]\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 100\u001b[0m         \u001b[43mlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# [N1, N2, 1]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spherical_harmonics\u001b[38;5;241m.\u001b[39mharmonic_levels\n\u001b[1;32m    102\u001b[0m     ]\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39mvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:179\u001b[0m, in \u001b[0;36mSphericalHarmonicsLevel.addition\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    177\u001b[0m     Y \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    178\u001b[0m XYT \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mmatmul(X, Y, tr_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# [N1, N2]\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgegenbauer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXYT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N1, N2]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m*\u001b[39m c\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/gegenbauer_polynomial.py:157\u001b[0m, in \u001b[0;36mGegenbauerScipyCoefficients.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m x\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolyval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/plum/function.py:399\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kw_args)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw_args):\n\u001b[1;32m    398\u001b[0m     method, return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_method_with_cache(args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert(\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m, return_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/lab_extras/torch/extras.py:24\u001b[0m, in \u001b[0;36mpolyval\u001b[0;34m(coeffs, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m curVal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(coeffs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     curVal \u001b[38;5;241m=\u001b[39m (curVal \u001b[38;5;241m+\u001b[39m coeffs[i]) \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m curVal \u001b[38;5;241m+\u001b[39m coeffs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reproduce_results(kin8mn, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for power--------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [02:20<00:00,  7.04s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.018200022938892634, MSE: 0.0604323361222259\n",
      "-------------------------------------Run 2--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [02:12<00:00,  6.63s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.020585911675719538, MSE: 0.06111090834631231\n",
      "-------------------------------------Run 3--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [02:17<00:00,  6.85s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.029410862008512097, MSE: 0.06202288396624321\n",
      "-------------------------------------Run 4--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [02:17<00:00,  6.89s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.011682565872741057, MSE: 0.05958469777710783\n",
      "-------------------------------------Run 5--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [02:14<00:00,  6.71s/it, ELBO=34.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.018373846236252843, MSE: 0.06075983045730417\n",
      "----------------------------------Metrics mean----------------------------------\n",
      "nlpd    0.019651\n",
      "mse     0.060782\n",
      "dtype: float64\n",
      "----------------------------------Metrics STD-----------------------------------\n",
      "nlpd    0.006391\n",
      "mse     0.000895\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlpd</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.060432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.061111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029411</td>\n",
       "      <td>0.062023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.059585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018374</td>\n",
       "      <td>0.060760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nlpd       mse\n",
       "0  0.018200  0.060432\n",
       "1  0.020586  0.061111\n",
       "2  0.029411  0.062023\n",
       "3  0.011683  0.059585\n",
       "4  0.018374  0.060760"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduce_results(power, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Reproducing results for concrete------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [01:26<00:00,  1.44it/s, ELBO=2.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3623051474366128, MSE: 0.1097215590579564\n",
      "-------------------------------------Run 2--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [01:32<00:00,  1.35it/s, ELBO=2.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3629351313556932, MSE: 0.10951533423101913\n",
      "-------------------------------------Run 3--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [01:30<00:00,  1.39it/s, ELBO=2.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3790285364454814, MSE: 0.11759354105563866\n",
      "-------------------------------------Run 4--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [01:28<00:00,  1.41it/s, ELBO=2.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.37304157369488944, MSE: 0.1139838157709856\n",
      "-------------------------------------Run 5--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 125/125 [01:28<00:00,  1.41it/s, ELBO=2.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.37610820039944604, MSE: 0.11769848257163341\n",
      "----------------------------------Metrics mean----------------------------------\n",
      "nlpd    0.370684\n",
      "mse     0.113703\n",
      "dtype: float64\n",
      "----------------------------------Metrics STD-----------------------------------\n",
      "nlpd    0.007663\n",
      "mse     0.004018\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlpd</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.362305</td>\n",
       "      <td>0.109722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.362935</td>\n",
       "      <td>0.109515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379029</td>\n",
       "      <td>0.117594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373042</td>\n",
       "      <td>0.113984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.376108</td>\n",
       "      <td>0.117698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nlpd       mse\n",
       "0  0.362305  0.109722\n",
       "1  0.362935  0.109515\n",
       "2  0.379029  0.117594\n",
       "3  0.373042  0.113984\n",
       "4  0.376108  0.117698"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduce_results(concrete, num_epochs=125)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
