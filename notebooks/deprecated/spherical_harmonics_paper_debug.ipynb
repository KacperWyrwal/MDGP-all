{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the spherical harmonics paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n",
      "/tmp/ipykernel_25320/3809226695.py:14: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "\n",
    "import torch \n",
    "import gpytorch \n",
    "import geometric_kernels.torch \n",
    "from math import comb \n",
    "from spherical_harmonics import SphericalHarmonics\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from linear_operator.operators import DiagLinearOperator, LinearOperator\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import negative_log_predictive_density, mean_squared_error\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_harmonics_single(ell: int, d: int) -> int:\n",
    "    r\"\"\"\n",
    "    Number of spherical harmonics of degree ell on S^{d - 1}.\n",
    "    \"\"\"\n",
    "    if ell == 0:\n",
    "        return 1\n",
    "    if d == 3:\n",
    "        return 2 * ell + 1\n",
    "    else:\n",
    "        return (2 * ell + d - 2) * comb(ell + d - 3, ell - 1) // ell\n",
    "\n",
    "\n",
    "def num_harmonics(ell: Tensor | int, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized version of num_harmonics_single\n",
    "    \"\"\"\n",
    "    if isinstance(ell, int):\n",
    "        return num_harmonics_single(ell, d)\n",
    "    return ell.apply_(lambda e: num_harmonics_single(ell=e, d=d))\n",
    "\n",
    "\n",
    "def total_num_harmonics(max_ell: int, d: int) -> int:\n",
    "    \"\"\"\n",
    "    Total number of spherical harmonics on S^{d-1} with degree <= max_ell\n",
    "    \"\"\"\n",
    "    return int(sum(num_harmonics(ell=torch.arange(max_ell + 1), d=d)))\n",
    "\n",
    "\n",
    "def eigenvalue_laplacian(ell: Tensor, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Eigenvalue of the Laplace-Beltrami operator for a spherical harmonic of degree ell on S_{d-1}\n",
    "    ell: [...]\n",
    "    d: []\n",
    "    return: [...]\n",
    "    \"\"\"\n",
    "    return ell * (ell + d - 2)\n",
    "\n",
    "\n",
    "def unnormalized_matern_spectral_density(n: Tensor, d: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor | float: \n",
    "    \"\"\"\n",
    "    compute (unnormalized) spectral density of the matern kernel on S_{d-1}\n",
    "    n: [N]\n",
    "    d: []\n",
    "    kappa: [O, 1, 1]\n",
    "    nu: [O, 1, 1]\n",
    "    return: [O, 1, N]\n",
    "    \"\"\"\n",
    "    # Squared exponential kernel \n",
    "    if torch.all(nu.isinf()):\n",
    "        exponent = -kappa ** 2 / 2 * eigenvalue_laplacian(ell=n, d=d) # [O, N, 1]\n",
    "        return torch.exp(exponent)\n",
    "    # Matern kernel\n",
    "    else:\n",
    "        base = (\n",
    "            2.0 * nu / kappa**2 + # [O, 1, 1]\n",
    "            eigenvalue_laplacian(ell=n, d=d).unsqueeze(-1) # [N, 1]\n",
    "        ) # [O, N, 1]\n",
    "        exponent = -nu - (d - 1) / 2.0 # [O, 1, 1]\n",
    "        return base ** exponent # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density_normalizer(d: int, max_ell: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalizing constant for the spectral density of the Matern kernel on S^{d-1}. \n",
    "    Depends on kappa and nu. Also depends on max_ell, as truncation of the infinite \n",
    "    sum from Karhunen-Loeve decomposition. \n",
    "    \"\"\"\n",
    "    n = torch.arange(max_ell + 1)\n",
    "    spectral_values = unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) # [O, max_ell + 1, 1]\n",
    "    num_harmonics_per_level = num_harmonics(torch.arange(max_ell + 1), d=d).type(spectral_values.dtype) # [max_ell + 1]\n",
    "    normalizer = spectral_values.mT @ num_harmonics_per_level # [O, 1, max_ell + 1] @ [max_ell + 1] -> [O, 1]\n",
    "    return normalizer.unsqueeze(-2) # [O, 1, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density(n: Tensor, d: int, kappa: Tensor, nu: Tensor, max_ell: int, sigma: float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Spectral density of the Matern kernel on S^{d-1}\n",
    "    \"\"\"\n",
    "    return (\n",
    "        unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) / # [O, N, 1]\n",
    "        matern_spectral_density_normalizer(d=d, max_ell=max_ell, kappa=kappa, nu=nu) * # [O, 1, 1]\n",
    "        (sigma ** 2)[..., *(None,) * (kappa.ndim - 1)] # [O, 1, 1]\n",
    "    ) # [O, N, 1] / [O, 1, 1] * [O, 1, 1] -> [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_ahat(ell: Tensor, d: int, max_ell: int, kappa: Tensor | float, nu: Tensor | float, \n",
    "                m: int | None = None, sigma: Tensor | float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    :math: `\\hat{a} = \\rho(\\ell)` where :math: `\\rho` is the spectral density on S^{d-1}\n",
    "    \"\"\"\n",
    "    return matern_spectral_density(n=ell, d=d, kappa=kappa, nu=nu, max_ell=max_ell, sigma=sigma) # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_repeated_ahat(max_ell: int, d: int, kappa: Tensor | float, nu: Tensor | float, sigma: Tensor | float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of repeated ahat values for each ell. \n",
    "    \"\"\"\n",
    "    ells = torch.arange(max_ell + 1) # [max_ell + 1]\n",
    "    ahat = matern_ahat(ell=ells, d=d, max_ell=max_ell, kappa=kappa, nu=nu, sigma=sigma) # [O, max_ell + 1, 1]\n",
    "    repeats = num_harmonics(ell=ells, d=d) # [max_ell + 1]\n",
    "    return torch.repeat_interleave(ahat, repeats=repeats, dim=-2) # [O, num_harmonics, 1]\n",
    "\n",
    "\n",
    "def matern_Kuu(max_ell: int, d: int, kappa: float, nu: float, sigma: float = 1.0) -> Tensor: \n",
    "    \"\"\"\n",
    "    Returns the covariance matrix, which is a diagonal matrix with entries \n",
    "    equal to inv_ahat of the corresponding ell. \n",
    "    \"\"\"\n",
    "    return torch.diag(1 / matern_repeated_ahat(max_ell, d, kappa, nu, sigma=sigma).squeeze(-1)) # [O, num_harmonics, num_harmonics]\n",
    "\n",
    "\n",
    "def spherical_harmonics(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    # Make sure that x is at least 2d and flatten it\n",
    "    x = torch.atleast_2d(x)\n",
    "    batch_shape, n = x.shape[:-2], x.shape[-2]\n",
    "    x = x.flatten(0, -2)\n",
    "\n",
    "    # Get spherical harmonics callable\n",
    "    f = SphericalHarmonics(dimension=d, degrees=max_ell + 1) # [... * O, N, num_harmonics]\n",
    "\n",
    "    # Evaluate x and reintroduce batch dimensions\n",
    "    return f(x).reshape(*batch_shape, n, total_num_harmonics(max_ell, d)) # [..., O, N, num_harmonics]\n",
    "\n",
    "\n",
    "def matern_Kux(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    return spherical_harmonics(x, max_ell=max_ell, d=d).mT # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def num_spherical_harmonics_to_degree(num_spherical_harmonics: int, dimension: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the minimum degree for which there are at least\n",
    "    `num_eigenfunctions` in the collection.\n",
    "    \"\"\"\n",
    "    n, degree = 0, 0  # n: number of harmonics, d: degree (or level)\n",
    "    while n < num_spherical_harmonics:\n",
    "        n += num_harmonics(d=dimension, ell=degree)\n",
    "        degree += 1\n",
    "\n",
    "    if n > num_spherical_harmonics:\n",
    "        print(\n",
    "            \"The number of spherical harmonics requested does not lead to complete \"\n",
    "            \"levels of spherical harmonics. We have thus increased the number to \"\n",
    "            f\"{n}, which includes all spherical harmonics up to degree {degree} (incl.)\"\n",
    "        )\n",
    "    return degree - 1, n\n",
    "\n",
    "\n",
    "from gpytorch.kernels import Kernel, ScaleKernel\n",
    "from mdgp.kernels.matern_kernel import _GeometricMaternKernel\n",
    "\n",
    "\n",
    "def matern_LT_Phi(x: Tensor, max_ell: int, d: int, kappa: float, nu: float, sigma: float = 1.0) -> Tensor: \n",
    "    Kux = matern_Kux(x, max_ell=max_ell, d=d) # [..., O, num_harmonics, N]\n",
    "    ahat_sqrt = matern_repeated_ahat(max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma).sqrt() # [O, num_harmonics, 1]\n",
    "    return Kux * ahat_sqrt # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def matern_LT_Phi_from_kernel(x: Tensor, covar_module: Kernel, num_levels: int) -> Tensor: \n",
    "    # Extract kernel parameters  \n",
    "    if isinstance(covar_module, ScaleKernel):\n",
    "        sigma = covar_module.outputscale.sqrt()\n",
    "        base_kernel = covar_module.base_kernel\n",
    "    else:\n",
    "        sigma = torch.tensor(1.0, dtype=x.dtype, device=x.device)\n",
    "        base_kernel = covar_module\n",
    "    kappa = base_kernel.lengthscale\n",
    "    nu = base_kernel.nu \n",
    "\n",
    "    # Extract constants \n",
    "    d = base_kernel.space.dimension + 1\n",
    "    max_ell = num_levels\n",
    "    \n",
    "    return matern_LT_Phi(x, max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpytorchSGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, max_ell, d, max_ell_prior, epsilon_sigma=1.0, kappa=1.0, nu=2.5, sigma=1.0, batch_shape=torch.Size([]), jitter_val=1e-6, optimize_nu: bool = True):\n",
    "        m = total_num_harmonics(max_ell, d)\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=m, batch_shape=batch_shape)\n",
    "        variational_strategy = SGPVariationalStrategy(self, variational_distribution, num_levels=max_ell, jitter_val=jitter_val)\n",
    "        super().__init__(variational_strategy=variational_strategy)\n",
    "\n",
    "        # constants \n",
    "        self.jitter_val = jitter_val\n",
    "        self.max_ell = max_ell\n",
    "        self.d = d\n",
    "\n",
    "        # modules \n",
    "        base_kernel = GeometricMaternKernel(\n",
    "            space=Hypersphere(d - 1),\n",
    "            lengthscale=kappa, \n",
    "            nu=nu, \n",
    "            trainable_nu=optimize_nu, \n",
    "            num_eigenfunctions=max_ell_prior + 1,\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "        # prior hyperparams \n",
    "        self.covar_module.outputscale = sigma ** 2\n",
    "        self.likelihood.noise = epsilon_sigma ** 2\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        p_sigma = self.covar_module(x)\n",
    "        p_mu = self.mean_module(x)\n",
    "        return MultivariateNormal(p_mu, p_sigma)\n",
    "\n",
    "\n",
    "class SGPVariationalStrategy(torch.nn.Module):\n",
    "    def __init__(self, model: gpytorchSGP, variational_distribution: CholeskyVariationalDistribution, num_levels, jitter_val=1e-6):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.jitter_val = jitter_val\n",
    "\n",
    "        # Extract references to kernel parameters \n",
    "        self.num_levels = num_levels\n",
    "\n",
    "    @property\n",
    "    def model(self) -> gpytorchSGP:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "    \n",
    "    @property\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        zeros = torch.zeros(\n",
    "            self._variational_distribution.shape(),\n",
    "            dtype=self._variational_distribution.dtype,\n",
    "            device=self._variational_distribution.device,\n",
    "        )\n",
    "        ones = torch.ones_like(zeros)\n",
    "        res = MultivariateNormal(zeros, DiagLinearOperator(ones))\n",
    "        return res\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        # prior at x \n",
    "        px = self.model.forward(x)\n",
    "        mu_x, Kxx = px.mean, px.lazy_covariance_matrix\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "\n",
    "        # whitened prior at u\n",
    "        Linv_pu = self.prior_distribution\n",
    "        Linv_mu, Linv_Kuu_LTinv = Linv_pu.mean, Linv_pu.lazy_covariance_matrix\n",
    "\n",
    "        # whitened variational posterior at u\n",
    "        Linv_qu = self.variational_distribution\n",
    "        Linv_m, Linv_S_LTinv = Linv_qu.mean, Linv_qu.lazy_covariance_matrix\n",
    "\n",
    "        # unwhitening + projection and vice-versa\n",
    "        LT_Phiux = matern_LT_Phi_from_kernel(x, self.model.covar_module, num_levels=self.num_levels)\n",
    "        Phixu_L = LT_Phiux.mT\n",
    "\n",
    "        # posterior at x \n",
    "        qx_sigma = Kxx + Phixu_L @ (Linv_S_LTinv - Linv_Kuu_LTinv) @ LT_Phiux\n",
    "        qx_sigma = qx_sigma.add_jitter(self.jitter_val)\n",
    "        qx_mu = mu_x + Phixu_L @ (Linv_m - Linv_mu)\n",
    "        return MultivariateNormal(qx_mu, qx_sigma)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        with gpytorch.settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        if prior:\n",
    "            return self.model.forward(x)\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self, b: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_psd(S, tol=1e-8):\n",
    "    if isinstance(S, LinearOperator):\n",
    "        S = S.to_dense()\n",
    "    assert torch.allclose(S, S.mT), \"K should be symmetric.\"\n",
    "\n",
    "    eigs = torch.linalg.eigvalsh(S)\n",
    "    assert (eigs > -tol).all(), \"K should be positive definite.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, url: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.url = url\n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "uci_base = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "\n",
    "class Power(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = uci_base + \"00294/CCPP.zip\"\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Power.DEFAULT_URL\n",
    "        super().__init__(name='power', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        with urlopen(self.url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall('/tmp/')\n",
    "\n",
    "        df = pd.read_excel('/tmp/CCPP//Folds5x2_pp.xlsx')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters as in the spherical harmonics paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets \n",
    "kin8mn = Kin8mn()\n",
    "power = Power()\n",
    "concrete = Concrete()\n",
    "\n",
    "# Variational parameters\n",
    "dimension_to_num_harmonics_variational = {\n",
    "    # 4: 336,\n",
    "    # 6: 294,\n",
    "    # 8: 210,\n",
    "    4: 1,\n",
    "    6: 2, \n",
    "    8: 2, \n",
    "}\n",
    "\n",
    "# Prior parameters \n",
    "\"\"\"\n",
    "NOTE \n",
    "- It is difficult to say what number of spherical harmonics was used for the prior.\n",
    "  I set it to be the same as the number of inducing variables.\n",
    "- It is also difficult to say what lengthscale initialisation was used in the paper. \n",
    "  I set it to 1.0 for now, although a lower number, e.g. 0.001, would likely be better for higher dimensions.\n",
    "\"\"\"\n",
    "dimension_to_num_harmonics_prior = {\n",
    "    # 4: 336,\n",
    "    # 6: 294,\n",
    "    # 8: 210, \n",
    "    # 4: 1000, \n",
    "    # 6: 1000,\n",
    "    4: 2, \n",
    "    6: 1, \n",
    "    8: 1,\n",
    "}\n",
    "nu = 1.5\n",
    "optimize_nu = False\n",
    "kappa = 1.0\n",
    "\n",
    "# Other model parameters\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Training parameters \n",
    "batch_size = 2\n",
    "LR = 0.01\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 10, which includes all spherical harmonics up to degree 2 (incl.)\n"
     ]
    }
   ],
   "source": [
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree\n",
    "\n",
    "\n",
    "def get_model_and_projector(dataset: UCIDataset, jitter_val=1e-6):\n",
    "    sphere_dimension = dataset.dimension + 1\n",
    "\n",
    "    # number of levels for variational inference \n",
    "    num_spherical_harmonics = dimension_to_num_harmonics_variational[dataset.dimension]\n",
    "    max_ell, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics, sphere_dimension)\n",
    "\n",
    "    # number of levels for prior\n",
    "    num_spherical_harmonics_prior = dimension_to_num_harmonics_prior[dataset.dimension]\n",
    "    max_ell_prior, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics_prior, sphere_dimension)\n",
    "\n",
    "    model = gpytorchSGP(max_ell=max_ell, d=sphere_dimension, max_ell_prior=max_ell_prior, kappa=kappa, nu=nu, optimize_nu=optimize_nu, batch_shape=batch_shape, jitter_val=jitter_val)\n",
    "    projector = SphereProjector()\n",
    "    return model, projector\n",
    "\n",
    "\n",
    "model, projector = get_model_and_projector(kin8mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, projector, optimizer, mll) -> float:\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    x, y = projector(x, y)\n",
    "    output = model(x)\n",
    "    test_psd(output.lazy_covariance_matrix)\n",
    "    loss = mll(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 6, which includes all spherical harmonics up to degree 2 (incl.)\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "model, projector = get_model_and_projector(power, 0.)\n",
    "optimizer = torch.optim.Adam(model.variational_strategy.parameters(), lr=LR, maximize=True)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, power.train_y.size(0))\n",
    "\n",
    "S_prev = None\n",
    "try:\n",
    "    x, y = power.train_x[:2][None], power.train_y[:2][None]\n",
    "    for i in range(127):\n",
    "        test_psd(model(projector(x)).lazy_covariance_matrix)\n",
    "\n",
    "        # If PSD save parameters\n",
    "        C_prev = model.variational_strategy._variational_distribution.chol_variational_covar.detach().clone()\n",
    "        S_prev = C_prev @ C_prev.mT\n",
    "        \n",
    "        # Update model \n",
    "        train_step(x=x_batch, y=y_batch, model=model, projector=projector, optimizer=optimizer, mll=mll)\n",
    "except AssertionError:\n",
    "    pass\n",
    "print(i)\n",
    "C = model.variational_strategy._variational_distribution.chol_variational_covar.detach().clone()\n",
    "S = C @ C.mT\n",
    "x = projector(x)\n",
    "Kxx = model.covar_module(x)\n",
    "LT_Phiux = matern_LT_Phi_from_kernel(x, model.covar_module, num_levels=model.variational_strategy.num_levels)\n",
    "Phixu_L = LT_Phiux.mT\n",
    "\n",
    "\n",
    "# assert (LT_Phiux == LT_Phiux_prev).all()\n",
    "# assert (Kxx.evaluate() == Kxx_prev.evaluate()).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0163]]),\n",
       " tensor([[0.0171]]),\n",
       " tensor([[[1., 1.],\n",
       "          [1., 1.]]], grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[[1.0000, 0.9673],\n",
       "          [0.9673, 1.0000]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S, S_prev, Phixu_L @ LT_Phiux, Kxx.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0171, -0.0156],\n",
      "         [-0.0156,  0.0171]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.0163, -0.0164],\n",
      "         [-0.0164,  0.0163]]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "K should be positive definite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[669], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m test_psd(prev)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(curr)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (eigs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mtol)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: K should be positive definite."
     ]
    }
   ],
   "source": [
    "prev = (Kxx + Phixu_L @ (S_prev - 1) @ LT_Phiux).to_dense()\n",
    "curr = (Kxx + Phixu_L @ (S - 1) @ LT_Phiux).to_dense()\n",
    "print(prev)\n",
    "test_psd(prev)\n",
    "print(curr)\n",
    "test_psd(curr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "K should be positive definite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKxx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLT_Phiux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLT_Phiux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (eigs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mtol)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: K should be positive definite."
     ]
    }
   ],
   "source": [
    "test_psd((Kxx + LT_Phiux.mT @ (S - 1) @ LT_Phiux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "K should be positive definite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m test_psd(Kuu)\n\u001b[1;32m      8\u001b[0m test_psd(S)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mKuu\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (eigs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mtol)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: K should be positive definite."
     ]
    }
   ],
   "source": [
    "Kxx = model(x, prior=True).covariance_matrix\n",
    "Kuu = torch.eye(1)\n",
    "C = model.variational_strategy._variational_distribution.chol_variational_covar\n",
    "S = C @ C.mT\n",
    "\n",
    "test_psd(Kxx)\n",
    "test_psd(Kuu)\n",
    "test_psd(S)\n",
    "\n",
    "test_psd(S - Kuu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9286]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S - Kuu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "K should be positive definite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x, y \u001b[38;5;241m=\u001b[39m projector(x, y)\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (eigs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mtol)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: K should be positive definite."
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    x, y = projector(x, y)\n",
    "    output = model(x)\n",
    "    test_psd(output.lazy_covariance_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for power--------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n",
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 6, which includes all spherical harmonics up to degree 2 (incl.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "K should be positive definite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreproduce_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 43\u001b[0m, in \u001b[0;36mreproduce_results\u001b[0;34m(dataset, num_runs, num_epochs, lr)\u001b[0m\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(run)\n\u001b[1;32m     42\u001b[0m model, projector \u001b[38;5;241m=\u001b[39m get_model_and_projector(dataset)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m run_metrics \u001b[38;5;241m=\u001b[39m evaluate(dataset, model, projector)\n\u001b[1;32m     45\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(run_metrics)\n",
      "Cell \u001b[0;32mIn[44], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, projector, num_epochs, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, projector, optimizer, mll)\u001b[0m\n\u001b[1;32m      3\u001b[0m x, y \u001b[38;5;241m=\u001b[39m projector(x, y)\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m mll(output, y)\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(S, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(S, S\u001b[38;5;241m.\u001b[39mmT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be symmetric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(S)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (eigs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mtol)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK should be positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: K should be positive definite."
     ]
    }
   ],
   "source": [
    "reproduce_results(power, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
