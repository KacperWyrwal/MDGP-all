{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import geometric_kernels.torch \n",
    "import torch \n",
    "from torch import Tensor \n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from torch.func import jacfwd, vmap\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from torch import nn \n",
    "from mdgp.utils import sphere_uniform_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = Hypersphere(2)\n",
    "# NOTE need to figure out how to support likelihood samples \n",
    "base_kernel = GeometricMaternKernel(space, batch_shape=None, num_eigenfunctions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_matrix_valued_kernel(K: Tensor) -> Tensor: \n",
    "    \"\"\"\n",
    "    K: (..., N, M, D, D)\n",
    "    return: (..., D * N, D * M)\n",
    "    \"\"\"\n",
    "    return torch.cat(\n",
    "        [torch.cat([block for block in row_blocks.unbind(-1)], dim=-2)\n",
    "        for row_blocks in K.unbind(-1)], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def unflatten_matrix_valued_kernel(K: Tensor, D: int) -> Tensor: \n",
    "    \"\"\"\n",
    "    K: (..., D * N, D * M)\n",
    "    return: (..., N, M, D, D)\n",
    "    \"\"\"\n",
    "    return torch.stack(\n",
    "        [torch.stack([block for block in row_blocks.tensor_split(D, dim=-2)], dim=-1)\n",
    "         for row_blocks in K.tensor_split(D, dim=-1)], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def test_flatten_matrix_valued_kernel(): \n",
    "    K = torch.randn(2, 5, 7, 3, 3)\n",
    "    K_flat = flatten_matrix_valued_kernel(K)\n",
    "    for i in range(3): \n",
    "        for j in range(3): \n",
    "            assert torch.all(K[..., i, j] == K_flat[..., i * 5: (i + 1) * 5, j * 7: (j + 1) * 7])\n",
    "\n",
    "\n",
    "def test_unflatten_matrix_valued_kernel():\n",
    "    K = torch.randn(2, 5, 7, 3, 3)\n",
    "    K_flat = flatten_matrix_valued_kernel(K)\n",
    "    K_unflat = unflatten_matrix_valued_kernel(K_flat, 3)\n",
    "    assert torch.all(K_unflat == K)\n",
    "\n",
    "\n",
    "test_flatten_matrix_valued_kernel()\n",
    "test_unflatten_matrix_valued_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_batch_shapes(*tensors: Tensor) -> torch.Size:\n",
    "    \"\"\"\n",
    "    Given a list of tensors, returns the broadcasted batch shape.\n",
    "    \"\"\"\n",
    "    return torch.broadcast_shapes(*[t.shape[:-2] for t in tensors])\n",
    "\n",
    "\n",
    "def broadcast_batch_tensors(*tensors: Tensor) -> list[Tensor]:\n",
    "    \"\"\"\n",
    "    Given a list of tensors, return a list of tensors with the same batch shape (if broadcastable).\n",
    "    \"\"\"\n",
    "    batch_shape = broadcast_batch_shapes(*tensors)\n",
    "    return [t.broadcast_to(*batch_shape, *t.shape[-2:]) for t in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart(sph: Tensor) -> Tensor: \n",
    "    \"\"\"\n",
    "    sph: (..., 2) with first dim longitude and second dim latitude\n",
    "    return: (..., 3)\n",
    "    \"\"\"\n",
    "    lon, lat = sph.unbind(-1)\n",
    "\n",
    "    cos_lat = torch.cos(lat)\n",
    "    x = cos_lat * torch.cos(lon)\n",
    "    y = cos_lat * torch.sin(lon)\n",
    "    z = torch.sin(lat)\n",
    "    return torch.stack((x, y, z), dim=-1)\n",
    "\n",
    "\n",
    "def tangent_basis_at(sph: Tensor) -> Tensor: \n",
    "    \"\"\"\n",
    "    sph: (2,)\n",
    "    return: (3, 2)\n",
    "    \"\"\"\n",
    "    f = jacfwd(chart)\n",
    "    return f(sph)\n",
    "\n",
    "\n",
    "def tangent_basis_no_batch(sph: Tensor) -> Tensor: \n",
    "    \"\"\"\n",
    "    sph: (N, 2)\n",
    "    return: (N, 3, 2)\n",
    "    \"\"\"\n",
    "    f = vmap(tangent_basis_at)\n",
    "    return f(sph)\n",
    "\n",
    "\n",
    "def tangent_basis_batch(sph: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    sph: (..., N, 2)\n",
    "    return: (..., N, 3, 2)\n",
    "    \"\"\"\n",
    "    f = vmap(tangent_basis_no_batch)\n",
    "    return f(sph)\n",
    "\n",
    "\n",
    "def inverse_chart(car):\n",
    "    # single coordinate, non-pole\n",
    "    x, y, z = car.unbind(-1)\n",
    "    theta = torch.arccos(z)\n",
    "    phi = torch.atan2(y, x)\n",
    "    return torch.stack((theta, phi), dim=-1)\n",
    "\n",
    "\n",
    "def car_to_sph(car: Tensor, epsilon: float = 1e-6) -> Tensor:\n",
    "    # car = jnp.atleast_2d(car)\n",
    "    # assert car.ndim == 2 and car.shape[1] == 3\n",
    "    north_pole = (car[:, 2] > 1 - epsilon)\n",
    "    south_pole = (-car[:, 2] > 1 - epsilon)\n",
    "    poles = north_pole | south_pole\n",
    "    if poles.any():\n",
    "        sph = torch.empty((*car.shape[:-1], 2), dtype=car.dtype)\n",
    "        sph[~poles] = inverse_chart(car[~poles])\n",
    "        sph[north_pole] = torch.tensor([torch.pi / 2, 0])\n",
    "        sph[south_pole] = torch.tensor([-torch.pi / 2, 0])\n",
    "    else:\n",
    "        sph = inverse_chart(car)\n",
    "    # some numerical issues can lead to NaNs when y=0, remediate here\n",
    "    greenwich = (abs(car[:, 1]) < epsilon) & (~poles)\n",
    "    if greenwich.any():\n",
    "        sph[greenwich, 1] = torch.where(car[greenwich, 0] > 0, 0., torch.pi)\n",
    "    return sph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO \n",
    "[X] Project the gradient onto the coordinate frame of the sphere. <br>\n",
    "[ ] Implement a single-layer GP with the HodgeMaternKernel <br>\n",
    "[ ] Run single-layer GP on the wind dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurlFreeHodgeMaternKernel(nn.Module): \n",
    "    def __init__(self, scalar_matern_kernel): \n",
    "        super().__init__()\n",
    "        self.scalar_matern_kernel = scalar_matern_kernel\n",
    "\n",
    "    def scalar_matern_kernel_at(self, x1, x2) -> Tensor:\n",
    "        \"\"\"\n",
    "        This takes in a single point x1 and x2 i.e.\n",
    "        x1.shape = (3,)\n",
    "        x2.shape = (3,)\n",
    "        \"\"\"\n",
    "        return self.scalar_matern_kernel(x1.unsqueeze(0), x2.unsqueeze(0)).evaluate().squeeze()\n",
    "    \n",
    "    def hodge_at(self, x1, x2) -> Tensor: \n",
    "        \"\"\"\n",
    "        x1.shape = (3,)\n",
    "        x2.shape = (3,)\n",
    "        \"\"\"\n",
    "        f = jacfwd(jacfwd(self.scalar_matern_kernel_at, argnums=0), argnums=1)\n",
    "        return f(x1, x2)\n",
    "\n",
    "    def hodge_no_batch(self, x1, x2): \n",
    "        \"\"\"\n",
    "        x1.shape = (N, 3)\n",
    "        x2.shape = (M, 3)\n",
    "        \"\"\"\n",
    "        f = vmap(vmap(self.hodge_at, in_dims=(None, 0)), in_dims=(0, None))\n",
    "        return f(x1, x2)\n",
    "    \n",
    "    def hodge_batch(self, x1, x2): \n",
    "        \"\"\"\n",
    "        x1.shape = (B, N, 3)\n",
    "        x2.shape = (B, M, 3)\n",
    "        \"\"\"\n",
    "        f = vmap(self.hodge_no_batch)\n",
    "        return f(x1, x2)\n",
    "    \n",
    "    def tangent_basis(self, sph: Tensor) -> Tensor: \n",
    "        \"\"\"\n",
    "        sph.shape = (..., 2)\n",
    "        return: (..., 3, 2)\n",
    "        \"\"\"\n",
    "        if sph.ndim == 1:\n",
    "            return tangent_basis_at(sph)\n",
    "        if sph.ndim == 2:\n",
    "            return tangent_basis_no_batch(sph)\n",
    "        if sph.ndim == 3:\n",
    "            return tangent_basis_batch(sph)\n",
    "        raise ValueError(f\"sph must have at most 3 dimensions, got {sph.ndim}\")\n",
    "\n",
    "    def ambient_to_tangent_kernel(self, K_ambient: Tensor, x1: Tensor, x2: Tensor) -> Tensor: \n",
    "        \"\"\"\n",
    "        K_ambient.shape = (..., N, M, 3, 3)\n",
    "        x1.shape = (..., N, 3)\n",
    "        x2.shape = (..., M, 3)\n",
    "        return: (..., N, M, 2, 2)\n",
    "        \"\"\"\n",
    "        x1_sph = car_to_sph(x1)\n",
    "        x2_sph = car_to_sph(x2)\n",
    "\n",
    "        # Project kernel onto S^2 tangent space \n",
    "        tangent_basis_x1 = self.tangent_basis(x1_sph) # (..., N, 3, 2)\n",
    "        tangent_basis_x2 = self.tangent_basis(x2_sph) # (..., M, 3, 2)\n",
    "        # (..., N, 3, 2) @ (..., N, M, 3, 3) @ (..., M, 3, 2) -> (..., N, M, 2, 2)\n",
    "        K_tangent = torch.einsum('...nji, ...nmjk, ...mkl -> ...nmil', tangent_basis_x1, K_ambient, tangent_basis_x2)\n",
    "        return K_tangent\n",
    "\n",
    "    def forward(self, x1, x2) -> Tensor:\n",
    "        \"\"\"\n",
    "        x1.shape = (..., N, 3)\n",
    "        x2.shape = (..., M, 3)\n",
    "        return: (..., N, M, 3, 3)\n",
    "        \"\"\"\n",
    "        # TODO Currently we don't support broadcasting over batch dimensions. \n",
    "        # This introduces some redundant computation. \n",
    "        x1, x2 = broadcast_batch_tensors(x1, x2)\n",
    "\n",
    "        # Get kernel with cartesian gradient (embedded field kernel)\n",
    "        # K_cart.shape = (..., N, M, 3, 3)\n",
    "        if x1.ndim == 1: \n",
    "            K_cart = self.hodge_at(x1, x2)\n",
    "        if x1.ndim == 2:\n",
    "            K_cart = self.hodge_no_batch(x1, x2)\n",
    "        if x1.ndim == 3:\n",
    "            K_cart = self.hodge_batch(x1, x2)\n",
    "        \n",
    "        # Project kernel onto S^2 tangent space \n",
    "        K_tangent = self.ambient_to_tangent_kernel(K_cart, x1, x2)\n",
    "        return K_tangent \n",
    "\n",
    "    def __call__(self, x1, x2) -> Tensor: \n",
    "        \"\"\"\n",
    "        x1.shape = (..., N, 3)\n",
    "        x2.shape = (..., M, 3)\n",
    "        return: (..., 3 * N, 3 * M)\n",
    "        \"\"\"\n",
    "        K = super().__call__(x1, x2)\n",
    "        return flatten_matrix_valued_kernel(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 400\n",
    "ny = 400\n",
    "num_batches = 2\n",
    "x = sphere_uniform_grid(nx * num_batches).reshape(num_batches, nx, 3)\n",
    "y = sphere_uniform_grid(ny * num_batches).reshape(num_batches, ny, 3)\n",
    "hodge_kernel = CurlFreeHodgeMaternKernel(base_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 800, 800])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hodge_kernel(x, y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape requirements for a kernel in a deep GP \n",
    "input shape [S, N, D] or [N, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "from jax.scipy.special import lpmn_values\n",
    "\n",
    "# KERNEL FUNCTIONS\n",
    "\n",
    "def array(els):\n",
    "    return jnp.array(els, dtype='float64')\n",
    "\n",
    "\n",
    "MAX_ELL = 35\n",
    "\n",
    "@jax.jit\n",
    "def legendre_values(x, y):\n",
    "    # x, y in cartesian coordinates\n",
    "    legendre_vals = lpmn_values(MAX_ELL, MAX_ELL, jnp.dot(x, y)[None], False)\n",
    "    legendre_vals = jnp.squeeze(legendre_vals[0, :, :])\n",
    "    return legendre_vals\n",
    "\n",
    "\n",
    "@jax.jit \n",
    "def lambd(ell: int) -> int: \n",
    "    return ell * (ell + 1)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def phi(kappa: float, nu: float, lam: float) -> float: \n",
    "    return jnp.power(2 * nu / kappa + lam, -nu - 1)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def legendre_tilde_constant(kappa: float, nu: float, ell: int) -> float:\n",
    "    lambd_ell = lambd(ell)\n",
    "    return (2 * ell + 1) / (4 * jnp.pi * lambd_ell) * phi(kappa, nu, lambd_ell)\n",
    "\n",
    "\n",
    "@jax.jit \n",
    "def legendre_tilde_values(x, y, kappa: float = 1.0, nu: float = 2.5):\n",
    "    legendre_vals = legendre_values(x, y)[1:]\n",
    "    return jnp.multiply(\n",
    "        legendre_vals, \n",
    "        array([legendre_tilde_constant(kappa, nu, ell) for ell in jnp.arange(1, MAX_ELL + 1)])\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit \n",
    "def dd_legendre_tilde_vals(x, y): \n",
    "    dx = jax.jacfwd(legendre_values, argnums=0)(x, y)[1:]\n",
    "    dy = jax.jacfwd(legendre_values, argnums=1)(x, y)[1:]\n",
    "    dd = jnp.einsum('ij,ik->ijk', dx, dy) # batched outer product \n",
    "    return dd \n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def hodge_new(x, y, kappa: float = 1.0, nu: float = 2.5):\n",
    "    # x, y in cartesian coordinates\n",
    "    # dd_legendre_vals = jax.jacfwd(jax.jacfwd(legendre_values, argnums=0), argnums=1)(x, y)[1:]\n",
    "    dx = jax.jacfwd(legendre_tilde_values, argnums=0)(x, y, kappa, nu)\n",
    "    dy = jax.jacfwd(legendre_tilde_values, argnums=1)(x, y, kappa, nu)\n",
    "    return jnp.einsum('ij, ik -> jk', dx, dy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The first implementation is taken directly from the code for Intrinsic Gaussian Vector Fields on Manifolds. \n",
    "# The second implementation is equivalent to the first.     \n",
    "\n",
    "@jax.jit\n",
    "def hodge_matern_k(x, y, kappa: float = 1.0, nu: float = 2.5):\n",
    "    \"\"\"\n",
    "    Unnormalized hodge matern kernel on the sphere. \n",
    "    \"\"\"\n",
    "    # x, y in cartesian coordinates\n",
    "    dd_legendre_vals = jax.jacfwd(jax.jacfwd(legendre_values, argnums=0), argnums=1)(x, y)[1:]\n",
    "    # d term\n",
    "    dd = jnp.multiply(\n",
    "        dd_legendre_vals,\n",
    "        array([\n",
    "            jnp.power(2 * nu / kappa + ell * (ell + 1), -nu - 1) * (2 * ell + 1) / (4 * jnp.pi * ell * (ell + 1)) # legendre_tilde_constant\n",
    "            for ell in jnp.arange(1, MAX_ELL + 1)])[:, None, None]\n",
    "    ).sum(axis=0)\n",
    "    # vector k\n",
    "    vk = dd\n",
    "    return vk\n",
    "\n",
    "\n",
    "@jax.jit \n",
    "def scalar_matern_k(x, y, kappa: float = 1.0, nu: float = 2.5): \n",
    "    \"\"\"\n",
    "    Unnormalized matern kernel on the sphere. \n",
    "    \"\"\"\n",
    "    legendre_vals = legendre_values(x, y)[1:]\n",
    "    return jnp.multiply(\n",
    "        legendre_vals, \n",
    "        array([legendre_tilde_constant(kappa, nu, ell) for ell in jnp.arange(1, MAX_ELL + 1)])\n",
    "    ).sum(axis=0)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def hodge_matern_k_equivalent(x, y, kappa: float = 1.0, nu: float = 2.5):\n",
    "    \"\"\"\n",
    "    Unnormalized hodge matern kernel on the sphere. Equivalent to hodge_matern_k. \n",
    "\n",
    "    How does a sum of outer products of gradients relate to the second order partial derivatives? \n",
    "    \"\"\"\n",
    "    return jax.jacfwd(jax.jacfwd(scalar_matern_k, argnums=0), argnums=1)(x, y, kappa, nu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
