{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geometric_kernels.torch \n",
    "import torch \n",
    "import gpytorch \n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from gpytorch import Module\n",
    "from gpytorch.utils.memoize import cached, clear_cache_hook\n",
    "from gpytorch import settings \n",
    "from gpytorch.kernels import ScaleKernel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: We could reuse the spherical harmonics computed in the variational strategy for the kernel itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There actually seems to be a simplified form for the update when using the same number of spherical harmonics for the kernel approximation and for the inducing variables. But that might be too many inducing variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Any, Dict, Iterable, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from linear_operator import to_dense\n",
    "from linear_operator.operators import (\n",
    "    CholLinearOperator,\n",
    "    DiagLinearOperator,\n",
    "    LinearOperator,\n",
    "    MatmulLinearOperator,\n",
    "    RootLinearOperator,\n",
    "    SumLinearOperator,\n",
    "    TriangularLinearOperator,\n",
    "    DenseLinearOperator,\n",
    ")\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "from linear_operator.utils.errors import NotPSDError\n",
    "from torch import Tensor\n",
    "\n",
    "from gpytorch.variational._variational_strategy import _VariationalStrategy\n",
    "from gpytorch.variational.cholesky_variational_distribution import CholeskyVariationalDistribution\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal, Distribution\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.settings import _linalg_dtype_cholesky, trace_mode\n",
    "from gpytorch.utils.errors import CachingError\n",
    "from gpytorch.utils.memoize import cached, pop_from_cache_ignore_args\n",
    "from gpytorch.variational import _VariationalDistribution\n",
    "from abc import ABC, abstractproperty\n",
    "\n",
    "\n",
    "class _VariationalStrategy(Module, ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all Variational Strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    has_fantasy_strategy = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[ApproximateGP, \"_VariationalStrategy\"],\n",
    "        variational_distribution: _VariationalDistribution,\n",
    "        jitter_val: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # Model\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @jitter_val.setter\n",
    "    def jitter_val(self, jitter_val: float):\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "    @abstractproperty\n",
    "    @cached(name=\"prior_distribution_memo\")\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        r\"\"\"\n",
    "        The :func:`~gpytorch.variational.VariationalStrategy.prior_distribution` method determines how to compute the\n",
    "        GP prior distribution of the inducing points, e.g. :math:`p(u) \\sim N(\\mu(X_u), K(X_u, X_u))`. Most commonly,\n",
    "        this is done simply by calling the user defined GP prior on the inducing point data directly.\n",
    "\n",
    "        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`\n",
    "        :return: The distribution :math:`p( \\mathbf u)`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @cached(name=\"variational_distribution_memo\")\n",
    "    def variational_distribution(self) -> Distribution:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        inducing_values: Tensor,\n",
    "        variational_inducing_covar: Optional[LinearOperator] = None,\n",
    "        **kwargs,\n",
    "    ) -> MultivariateNormal:\n",
    "        r\"\"\"\n",
    "        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the\n",
    "        inducing point function values. Specifically, forward defines how to transform a variational distribution\n",
    "        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at\n",
    "        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`\n",
    "\n",
    "        :param x: Locations :math:`\\mathbf X` to get the\n",
    "            variational posterior of the function values at.\n",
    "        :param inducing_points: Locations :math:`\\mathbf Z` of the inducing points\n",
    "        :param inducing_values: Samples of the inducing function values :math:`\\mathbf u`\n",
    "            (or the mean of the distribution :math:`q(\\mathbf u)` if q is a Gaussian.\n",
    "        :param variational_inducing_covar: If\n",
    "            the distribuiton :math:`q(\\mathbf u)` is\n",
    "            Gaussian, then this variable is the covariance matrix of that Gaussian.\n",
    "            Otherwise, it will be None.\n",
    "\n",
    "        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`\n",
    "        :return: The distribution :math:`q( \\mathbf f(\\mathbf X))`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Compute the KL divergence between the variational inducing distribution :math:`q(\\mathbf u)`\n",
    "        and the prior inducing distribution :math:`p(\\mathbf u)`.\n",
    "        \"\"\"\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        # Get p(u)/q(u)\n",
    "        variational_dist_u = self.variational_distribution\n",
    "\n",
    "        # Get q(f)\n",
    "        if isinstance(variational_dist_u, MultivariateNormal):\n",
    "            return super().__call__(\n",
    "                x,\n",
    "                inducing_values=variational_dist_u.mean,\n",
    "                variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,\n",
    "                **kwargs,\n",
    "            )\n",
    "        raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.utils.spherical_harmonic_features import matern_Kuu, matern_Kux\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "def _ensure_updated_strategy_flag_set(\n",
    "    state_dict: Dict[str, Tensor],\n",
    "    prefix: str,\n",
    "    local_metadata: Dict[str, Any],\n",
    "    strict: bool,\n",
    "    missing_keys: Iterable[str],\n",
    "    unexpected_keys: Iterable[str],\n",
    "    error_msgs: Iterable[str],\n",
    "):\n",
    "    device = state_dict[list(state_dict.keys())[0]].device\n",
    "    if prefix + \"updated_strategy\" not in state_dict:\n",
    "        state_dict[prefix + \"updated_strategy\"] = torch.tensor(False, device=device)\n",
    "        warnings.warn(\n",
    "            \"You have loaded a variational GP model (using `VariationalStrategy`) from a previous version of \"\n",
    "            \"GPyTorch. We have updated the parameters of your model to work with the new version of \"\n",
    "            \"`VariationalStrategy` that uses whitened parameters.\\nYour model will work as expected, but we \"\n",
    "            \"recommend that you re-save your model.\",\n",
    "        )\n",
    "\n",
    "\n",
    "class VariationalStrategy(_VariationalStrategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: _VariationalDistribution,\n",
    "        max_ell: int, \n",
    "        jitter_val: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model, variational_distribution, jitter_val=jitter_val\n",
    "        )\n",
    "        self._max_ell = max_ell\n",
    "\n",
    "        self.register_buffer(\"updated_strategy\", torch.tensor(True))\n",
    "        self._register_load_state_dict_pre_hook(_ensure_updated_strategy_flag_set)\n",
    "        self.has_fantasy_strategy = True\n",
    "\n",
    "    @property\n",
    "    def max_ell(self):\n",
    "        return self._max_ell\n",
    "    \n",
    "    @property\n",
    "    def d(self):\n",
    "        return self.model.covar_module.base_kernel.space.dimension + 1\n",
    "    \n",
    "    @property\n",
    "    def kappa(self):\n",
    "        return self.model.covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    @property\n",
    "    def nu(self):\n",
    "        return self.model.covar_module.base_kernel.nu\n",
    "    \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return self.model.covar_module.outputscale.sqrt()\n",
    "\n",
    "    @cached(name=\"cholesky_factor\", ignore_args=True)\n",
    "    def _cholesky_factor(self, induc_induc_covar: LinearOperator) -> TriangularLinearOperator:\n",
    "        return induc_induc_covar.cholesky()\n",
    "\n",
    "    @property\n",
    "    @cached(name=\"prior_distribution_memo\")\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        zeros = torch.zeros(\n",
    "            self._variational_distribution.shape(),\n",
    "            dtype=self._variational_distribution.dtype,\n",
    "            device=self._variational_distribution.device,\n",
    "        )\n",
    "        ones = torch.ones_like(zeros)\n",
    "        res = MultivariateNormal(zeros, DiagLinearOperator(ones))\n",
    "        return res\n",
    "    \n",
    "    def Kuu(self) -> DiagLinearOperator: \n",
    "        return matern_Kuu(max_ell=self.max_ell, d=self.d, kappa=self.kappa, nu=self.nu, sigma=self.sigma)\n",
    "    \n",
    "    def Kux(self, x: Tensor) -> DenseLinearOperator:\n",
    "        return DenseLinearOperator(matern_Kux(x, max_ell=self.max_ell, d=self.d))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        inducing_values: Tensor,\n",
    "        variational_inducing_covar: Optional[LinearOperator] = None,\n",
    "        **kwargs,\n",
    "    ) -> MultivariateNormal:\n",
    "        \n",
    "        px = self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Prior covariance terms\n",
    "        test_mean = px.mean\n",
    "        data_data_covar = px.lazy_covariance_matrix\n",
    "        return MultivariateNormal(test_mean, data_data_covar)\n",
    "\n",
    "        induc_induc_covar = self.Kuu()#.add_jitter(self.jitter_val)\n",
    "        induc_data_covar = self.Kux(x).to_dense()\n",
    "\n",
    "        # Compute interpolation terms\n",
    "        # K_ZZ^{-1/2} K_ZX\n",
    "        # K_ZZ^{-1/2} \\mu_Z\n",
    "        # L = self._cholesky_factor(induc_induc_covar)\n",
    "        # interp_term = L.solve(induc_data_covar.type(_linalg_dtype_cholesky.value())).to(torch.get_default_dtype()) # L^-1 @ Kux \n",
    "        L = induc_induc_covar.inverse().cholesky()\n",
    "        interp_term = L @ induc_data_covar\n",
    "\n",
    "        # Compute the mean of q(f)\n",
    "        # k_XZ K_ZZ^{-1/2} (m - K_ZZ^{-1/2} \\mu_Z) + \\mu_X\n",
    "        predictive_mean = (interp_term.transpose(-1, -2) @ inducing_values.unsqueeze(-1)).squeeze(-1) + test_mean\n",
    "\n",
    "        # Compute the covariance of q(f)\n",
    "        # K_XX + k_XZ K_ZZ^{-1/2} (S - I) K_ZZ^{-1/2} k_ZX\n",
    "        middle_term = self.prior_distribution.lazy_covariance_matrix.mul(-1) # -S\n",
    "        middle_term = SumLinearOperator(variational_inducing_covar, middle_term) # I - S \n",
    "\n",
    "        predictive_covar = SumLinearOperator(\n",
    "            data_data_covar.add_jitter(self.jitter_val),\n",
    "            MatmulLinearOperator(interp_term.transpose(-1, -2), middle_term @ interp_term), # Kxu @ L.T @ (I - S) @ L @ Kux\n",
    "        )\n",
    "\n",
    "        # Return the distribution\n",
    "        return MultivariateNormal(predictive_mean, predictive_covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.utils import sphere_uniform_grid, sphere_meshgrid, spherical_harmonic\n",
    "\n",
    "\n",
    "def target_fnc(x):\n",
    "    f = spherical_harmonic(x, 2, 3)\n",
    "    return f + 0.01 * torch.randn_like(f)\n",
    "\n",
    "\n",
    "test_inputs = sphere_meshgrid(100, 100)\n",
    "test_targets = target_fnc(test_inputs)\n",
    "\n",
    "train_inputs = sphere_uniform_grid(200)\n",
    "train_targets = target_fnc(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree, total_num_harmonics\n",
    "from gpytorch.models import ApproximateGP\n",
    "\n",
    "\n",
    "class SimpleApproximateGP(ApproximateGP):\n",
    "    def __init__(self, mean_module, covar_module, likelihood, max_ell, jitter_val=None):\n",
    "        num_inducing = total_num_harmonics(max_ell, d=covar_module.base_kernel.space.dimension + 1)\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, \n",
    "            variational_distribution=CholeskyVariationalDistribution(num_inducing_points=num_inducing),\n",
    "            max_ell=max_ell,\n",
    "            jitter_val=jitter_val,\n",
    "        )\n",
    "        super().__init__(variational_strategy=variational_strategy)\n",
    "        self.covar_module = covar_module\n",
    "        self.mean_module = mean_module\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        mvn = MultivariateNormal(mean, covar)\n",
    "        return mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 81, which includes all spherical harmonics up to degree 9 (incl.)\n"
     ]
    }
   ],
   "source": [
    "num_spherical_harmonics = 80\n",
    "dimension = 3\n",
    "degree, num_inducing = num_spherical_harmonics_to_degree(num_spherical_harmonics=num_spherical_harmonics, dimension=dimension)\n",
    "\n",
    "# Variational distribution \n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# covar\n",
    "space = Hypersphere(dim=dimension - 1)\n",
    "base_kernel = GeometricMaternKernel(space=space, trainable_nu=False, num_eigenfunctions=35, nu=2.5, batch_shape=batch_shape)\n",
    "covar_module = ScaleKernel(base_kernel, batch_shape=batch_shape)\n",
    "\n",
    "# mean\n",
    "mean_module = gpytorch.means.ZeroMean(batch_shape=batch_shape)\n",
    "\n",
    "# model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = SimpleApproximateGP(\n",
    "    covar_module=covar_module,\n",
    "    mean_module=mean_module,\n",
    "    max_ell=degree,\n",
    "    likelihood=likelihood,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str = DEFAULT_URL):\n",
    "        super().__init__(name='kin8nm', path=path, normalize=normalize, seed=seed)\n",
    "        self.url = url \n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from abc import ABC, abstractmethod\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class Projector(torch.nn.Module, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdentityProjector(Projector):\n",
    "\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor]:\n",
    "        if y is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x, y\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        return mvn\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = 1. + torch.exp(torch.randn(torch.Size([])))\n",
    "        self.register_parameter('b', torch.nn.Parameter(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)\n",
    "    \n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class FunctionalProjector(torch.nn.Module):\n",
    "    def __init__(self, f: Callable[[Tensor], Tensor]):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.f(x).expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch import settings \n",
    "from torch import no_grad \n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import mean_squared_error\n",
    "from mdgp.experiments.experiment_utils.logging import log \n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "from gpytorch.models.deep_gps import DeepGP\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FitArguments: \n",
    "    num_steps: int = field(default=1000, metadata={'help': 'Number of steps to train for'})\n",
    "    sample_hidden: str = field(\n",
    "        default='elementwise', \n",
    "        metadata={'help': 'Sampling method from hidden layers. Must be one of [\"elementwise\", \"pathwise\"]'}\n",
    "    )\n",
    "    batch_size: int = field(default=64, metadata={'help': 'Batch size'})\n",
    "    lr: float = field(default=0.01, metadata={'help': 'Learning rate'})\n",
    "    test_num_likelihood_samples: int = field(default=100, metadata={'help': 'Number of likelihood samples for test set evaluation'})\n",
    "    deep_train_num_likelihood_samples: int = field(default=3, metadata={'help': 'Number of likelihood samples used when training deep models.'})\n",
    "    optimize_projector: bool = field(default=False, metadata={'help': 'Whether to optimize the projection bias'})\n",
    "\n",
    "    def train_num_likelihood_samples(self, model) -> int:\n",
    "        if isinstance(model, DeepGP): \n",
    "            return self.deep_train_num_likelihood_samples\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def get_mll(model, dataset: UCIDataset): \n",
    "    return VariationalELBO(model.likelihood, model, num_data=dataset.train_y.size(0))\n",
    "\n",
    "\n",
    "def get_optimizer(model, projector: Projector, fit_args: FitArguments):\n",
    "    params = [\n",
    "        {'params': model.parameters()},\n",
    "    ]\n",
    "    if fit_args.optimize_projector is True: \n",
    "        params.append({'params': projector.parameters()})\n",
    "    return torch.optim.Adam(params, lr=fit_args.lr, maximize=True)\n",
    "\n",
    "\n",
    "def train_step(model, projector: Projector, dataloader: DataLoader, elbo, optimizer, num_likelihood_samples: int | None = None, prior=False): \n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "    if num_likelihood_samples is not None: \n",
    "        init_num_likelihood_samples = settings.num_likelihood_samples.value() \n",
    "        settings.num_likelihood_samples._set_value(num_likelihood_samples)\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # When training we don't rescale, or \"invert\", the predictive distribution\n",
    "        # before evaluating the ELBO.\n",
    "        pbatch_x, pbatch_y = projector(batch_x, batch_y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        poutputs = model(pbatch_x, prior=prior)\n",
    "        loss = elbo(poutputs, pbatch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if num_likelihood_samples is not None:\n",
    "        settings.num_likelihood_samples._set_value(init_num_likelihood_samples)\n",
    "    return {'elbo': total_loss / len(dataloader)}\n",
    "\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def negative_log_predictive_density(outputs: MultivariateNormal, targets: Tensor) -> Tensor:\n",
    "    mean, cov = outputs.mean.detach().numpy(), outputs.covariance_matrix.detach().numpy()\n",
    "    targets = targets.detach().numpy()\n",
    "    nlpd = -multivariate_normal.logpdf(targets, mean=mean, cov=cov)\n",
    "    return torch.tensor(nlpd) / targets.shape[0]\n",
    "\n",
    "\n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    # logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.tensor(logpdf.mean())\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean = outputs.mean.mean(0)\n",
    "    return y_std ** 2 * ((mean - targets) ** 2).mean(0)\n",
    "\n",
    "\n",
    "def test(model, projector: Projector, uci_dataset: UCIDataset, fit_args: FitArguments = None, prior=False):\n",
    "    with no_grad(), settings.num_likelihood_samples(fit_args.test_num_likelihood_samples):\n",
    "        total_mse = 0.0\n",
    "        total_tll = 0.0\n",
    "        total_nlpd = 0.0\n",
    "        model.eval() \n",
    "        dataloader = DataLoader(uci_dataset.test_dataset, batch_size=fit_args.batch_size)\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # When testing we rescale, or \"invert\", the predictive distribution\n",
    "            pbatch_x, _ = projector(batch_x, batch_y)\n",
    "            poutputs = model.likelihood(model(pbatch_x, prior=prior))\n",
    "            outputs = projector.inverse(poutputs)\n",
    "            total_mse += mean_squared_error(outputs, batch_y, y_std=uci_dataset.test_y_std).item()\n",
    "            total_tll += test_log_likelihood(outputs, batch_y, y_std=uci_dataset.test_y_std).item()\n",
    "            total_nlpd += negative_log_predictive_density(outputs, batch_y).mean(0).item()\n",
    "        return {\n",
    "            'rmse': (total_mse / len(dataloader)) ** 0.5,\n",
    "            'tll': total_tll / len(dataloader),\n",
    "            'nlpd': total_nlpd / len(dataloader),\n",
    "        }\n",
    "    \n",
    "\n",
    "def fit(model, projector, optimizer, elbo, uci_dataset: UCIDataset, train_loggers=None, fit_args: FitArguments = None, prior=False): \n",
    "    metrics = {'elbo': None}\n",
    "    pbar = tqdm(range(1, fit_args.num_steps + 1), desc=\"Fitting\")\n",
    "    for step in pbar:\n",
    "        metrics_step = train_step(\n",
    "            model=model, \n",
    "            projector=projector,\n",
    "            dataloader=DataLoader(uci_dataset.train_dataset, batch_size=fit_args.batch_size, shuffle=True),\n",
    "            elbo=elbo,\n",
    "            optimizer=optimizer,\n",
    "            num_likelihood_samples=fit_args.train_num_likelihood_samples(model),\n",
    "            prior=prior\n",
    "        )\n",
    "        # Update, log, and display metrics \n",
    "        metrics.update(metrics_step)\n",
    "        log(train_loggers, metrics=metrics, step=step)\n",
    "        pbar.set_postfix(metrics)\n",
    "        if step % 10 == 0: \n",
    "            print(test(model, projector, uci_dataset, fit_args=fit_args, prior=prior))\n",
    "            print(f\"kappa: {model.variational_strategy.kappa=}, nu: {model.variational_strategy.nu=}, sigma: {model.variational_strategy.sigma=}\")\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Kin8mn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational distribution \n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# covar\n",
    "space = Hypersphere(dim=dataset.dimension)\n",
    "base_kernel = GeometricMaternKernel(space=space, trainable_nu=False, num_eigenfunctions=5, nu=2.5)\n",
    "base_kernel.initialize(lengthscale=0.1)\n",
    "covar_module = ScaleKernel(base_kernel)\n",
    "covar_module.initialize(outputscale=1.0)\n",
    "\n",
    "# mean\n",
    "mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "# likelihood \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# model\n",
    "model = SimpleApproximateGP(\n",
    "    covar_module=covar_module,\n",
    "    mean_module=mean_module,\n",
    "    likelihood=likelihood, \n",
    "    max_ell=5,\n",
    "    jitter_val=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "projector = SphereProjector()\n",
    "\n",
    "batch_size = dataset.train_y.size(0)\n",
    "num_likelihood_samples = 3\n",
    "fit_args = FitArguments(\n",
    "    num_steps=1000, \n",
    "    batch_size=batch_size,\n",
    "    optimize_projector=False,\n",
    "    deep_train_num_likelihood_samples=num_likelihood_samples,\n",
    ")\n",
    "prior=True\n",
    "\n",
    "elbo = get_mll(model, dataset)\n",
    "optimizer = get_optimizer(model, projector, fit_args=fit_args)\n",
    "optimizer = torch.optim.Adam([p for m, p in model.named_parameters() if not m.endswith('raw_outputscale')], maximize=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.train()\n",
    "    model(projector(dataset.train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.9133e-15, -2.4821e-15, -2.1286e-15, -2.0159e-15, -1.8958e-15,\n",
      "        -1.7735e-15, -1.6801e-15, -1.6749e-15, -1.6060e-15, -1.5665e-15,\n",
      "        -1.4716e-15, -1.4037e-15, -1.3444e-15, -1.3232e-15, -1.2868e-15,\n",
      "        -1.2755e-15, -1.2440e-15, -1.1890e-15, -1.1308e-15, -1.1156e-15,\n",
      "        -1.0712e-15, -1.0500e-15, -1.0334e-15, -1.0072e-15, -9.6068e-16,\n",
      "        -9.3174e-16, -9.1668e-16, -8.9751e-16, -8.7090e-16, -8.6363e-16,\n",
      "        -8.3948e-16, -8.2650e-16, -8.0557e-16, -7.8097e-16, -7.5251e-16,\n",
      "        -7.3913e-16, -7.1350e-16, -7.0879e-16, -6.7423e-16, -6.5745e-16,\n",
      "        -6.4198e-16, -6.1202e-16, -6.0459e-16, -5.8388e-16, -5.6523e-16,\n",
      "        -5.5353e-16, -5.3081e-16, -5.1574e-16, -5.0293e-16, -4.8369e-16,\n",
      "        -4.7478e-16, -4.6069e-16, -4.4407e-16, -4.3078e-16, -3.9733e-16,\n",
      "        -3.7383e-16, -3.6706e-16, -3.4725e-16, -3.4166e-16, -3.2276e-16,\n",
      "        -3.1322e-16, -2.9707e-16, -2.8926e-16, -2.6475e-16, -2.2897e-16,\n",
      "        -2.2287e-16, -2.1290e-16, -1.9903e-16, -1.8639e-16, -1.6665e-16,\n",
      "        -1.5323e-16, -1.4660e-16, -1.3035e-16, -1.1004e-16, -9.4693e-17,\n",
      "        -8.9729e-17, -6.3423e-17, -4.7977e-17, -2.8525e-17, -1.1869e-17,\n",
      "        -7.1660e-18,  1.9452e-17,  3.4228e-17,  3.5876e-17,  5.9496e-17,\n",
      "         7.3711e-17,  8.9706e-17,  1.0636e-16,  1.2096e-16,  1.3119e-16,\n",
      "         1.3726e-16,  1.8250e-16,  1.8597e-16,  1.9417e-16,  2.1014e-16,\n",
      "         2.3432e-16,  2.4204e-16,  2.5937e-16,  2.8085e-16,  2.9149e-16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "projector = SphereProjector()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x = projector(dataset.test_x)\n",
    "    K = model.forward(x).covariance_matrix\n",
    "    # K = model.variational_strategy(x, prior=True).covariance_matrix\n",
    "    print(torch.linalg.eigvalsh(K)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0005, 0.0006, 0.0007, 0.0008, 0.0008, 0.0008, 0.0009, 0.0009, 0.0009,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0011, 0.0012, 0.0012,\n",
      "        0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013, 0.0014, 0.0014, 0.0014,\n",
      "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "        0.0017, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019, 0.0019, 0.0019,\n",
      "        0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0022, 0.0022,\n",
      "        0.0022, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025,\n",
      "        0.0025, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028,\n",
      "        0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0031, 0.0031, 0.0031, 0.0032,\n",
      "        0.0032, 0.0032, 0.0033, 0.0034, 0.0034, 0.0034, 0.0035, 0.0035, 0.0035,\n",
      "        0.0036, 0.0036, 0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039,\n",
      "        0.0039])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "covar_module = GeometricMaternKernel(\n",
    "    space=space, trainable_nu=False, num_eigenfunctions=6, nu=2.5\n",
    ")\n",
    "covar_module.initialize(lengthscale=0.1)\n",
    "\n",
    "projector = SphereProjector()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x = projector(dataset.test_x)\n",
    "    K = covar_module(x).evaluate()\n",
    "    print(torch.linalg.eigvalsh(K)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0005, 0.0006, 0.0007, 0.0008, 0.0008, 0.0008, 0.0009, 0.0009, 0.0009,\n",
      "        0.0009, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0011, 0.0011, 0.0012,\n",
      "        0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0014, 0.0014, 0.0014,\n",
      "        0.0014, 0.0014, 0.0014, 0.0015, 0.0015, 0.0015, 0.0016, 0.0016, 0.0016,\n",
      "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0018, 0.0018, 0.0019, 0.0019,\n",
      "        0.0019, 0.0019, 0.0020, 0.0020, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "from geometric_kernels.kernels import MaternGeometricKernel\n",
    "\n",
    "\n",
    "kernel = MaternGeometricKernel(space=space, num=6, normalize=True)\n",
    "params = kernel.init_params()\n",
    "params['lengthscale'] = torch.tensor(0.1)\n",
    "params['nu'] = torch.tensor(2.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    K = kernel.K(params, x)\n",
    "    print(torch.linalg.eigvalsh(K)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting:   0%|          | 0/1000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 9x7373)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 116\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, projector, optimizer, elbo, uci_dataset, train_loggers, fit_args, prior)\u001b[0m\n\u001b[1;32m    114\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, fit_args\u001b[38;5;241m.\u001b[39mnum_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 116\u001b[0m     metrics_step \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43muci_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_likelihood_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_num_likelihood_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Update, log, and display metrics \u001b[39;00m\n\u001b[1;32m    126\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mupdate(metrics_step)\n",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, projector, dataloader, elbo, optimizer, num_likelihood_samples, prior)\u001b[0m\n\u001b[1;32m     53\u001b[0m pbatch_x, pbatch_y \u001b[38;5;241m=\u001b[39m projector(batch_x, batch_y)\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 55\u001b[0m poutputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m elbo(poutputs, pbatch_y)\n\u001b[1;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/models/approximate_gp.py:108\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[0;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 144\u001b[0m, in \u001b[0;36m_VariationalStrategy.__call__\u001b[0;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Get q(f)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, MultivariateNormal):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43minducing_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariational_dist_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariational_inducing_covar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariational_dist_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[3], line 99\u001b[0m, in \u001b[0;36mVariationalStrategy.forward\u001b[0;34m(self, x, inducing_values, variational_inducing_covar, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m data_data_covar \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\n\u001b[1;32m     98\u001b[0m induc_induc_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKuu()\u001b[38;5;241m.\u001b[39madd_jitter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjitter_val)\n\u001b[0;32m---> 99\u001b[0m induc_data_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKux\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Compute interpolation terms\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# K_ZZ^{-1/2} K_ZX\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# K_ZZ^{-1/2} \\mu_Z\u001b[39;00m\n\u001b[1;32m    104\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cholesky_factor(induc_induc_covar)\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mVariationalStrategy.Kux\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mKux\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DenseLinearOperator:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DenseLinearOperator(\u001b[43mmatern_Kux\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_ell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/mdgp/utils/spherical_harmonic_features.py:136\u001b[0m, in \u001b[0;36mmatern_Kux\u001b[0;34m(x, max_ell, d)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatern_Kux\u001b[39m(x: Tensor, max_ell: \u001b[38;5;28mint\u001b[39m, d: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor: \n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspherical_harmonics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_ell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmT\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/mdgp/utils/spherical_harmonic_features.py:132\u001b[0m, in \u001b[0;36mspherical_harmonics\u001b[0;34m(x, max_ell, d)\u001b[0m\n\u001b[1;32m    129\u001b[0m f \u001b[38;5;241m=\u001b[39m SphericalHarmonics(dimension\u001b[38;5;241m=\u001b[39md, degrees\u001b[38;5;241m=\u001b[39mmax_ell \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [... * O, N, num_harmonics]\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Evaluate x and reintroduce batch dimensions\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mbatch_shape, n, total_num_harmonics(max_ell, d))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:80\u001b[0m, in \u001b[0;36mSphericalHarmonics.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mEvaluates each of the spherical harmonic level in the collection,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mand stacks the results.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m:return: [N, num harmonics in collection]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m harmonic: harmonic(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mharmonic_levels\n\u001b[1;32m     78\u001b[0m )  \u001b[38;5;66;03m# List of length `max_degree` with Tensor [num_harmonics_degree, N]\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mtranspose(B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(values), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:77\u001b[0m, in \u001b[0;36mSphericalHarmonics.__call__.<locals>.<lambda>\u001b[0;34m(harmonic)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     67\u001b[0m     x: B\u001b[38;5;241m.\u001b[39mNumeric,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Evaluates each of the spherical harmonic level in the collection,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    and stacks the results.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    :return: [N, num harmonics in collection]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m harmonic: \u001b[43mharmonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mharmonic_levels\n\u001b[1;32m     78\u001b[0m     )  \u001b[38;5;66;03m# List of length `max_degree` with Tensor [num_harmonics_degree, N]\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mtranspose(B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(values), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:151\u001b[0m, in \u001b[0;36mSphericalHarmonicsLevel.__call__\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: B\u001b[38;5;241m.\u001b[39mNumeric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    :param X: M normalised (i.e. unit) D-dimensional vector, [N, D]\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: `X` evaluated at the M spherical harmonics in the set.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m        [\\phi_m(x_i)], shape [M, N]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     VXT \u001b[38;5;241m=\u001b[39m \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [M, N]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     zonals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgegenbauer(VXT)  \u001b[38;5;66;03m# [M, N]\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mmatmul(B\u001b[38;5;241m.\u001b[39mcast(B\u001b[38;5;241m.\u001b[39mdtype(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_inv), zonals)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/plum/function.py:399\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kw_args)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw_args):\n\u001b[1;32m    398\u001b[0m     method, return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_method_with_cache(args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert(\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m, return_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/lab/shape.py:185\u001b[0m, in \u001b[0;36mdispatch_unwrap_dimensions.<locals>.unwrapped_dispatch.<locals>.f_wrapped\u001b[0;34m(*args, **kw_args)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw_args):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munwrap_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/lab/torch/linear_algebra.py:19\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, tr_a, tr_b)\u001b[0m\n\u001b[1;32m     17\u001b[0m a \u001b[38;5;241m=\u001b[39m transpose(a) \u001b[38;5;28;01mif\u001b[39;00m tr_a \u001b[38;5;28;01melse\u001b[39;00m a\n\u001b[1;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m transpose(b) \u001b[38;5;28;01mif\u001b[39;00m tr_b \u001b[38;5;28;01melse\u001b[39;00m b\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 9x7373)"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "fit(model, projector, optimizer, elbo, dataset, fit_args=fit_args, prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: Parameter containing:\n",
       "tensor([0.0035], requires_grad=True))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variational_strategy.variational_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats._multivariate import _PSD \n",
    "\n",
    "\n",
    "def test_psd(K): \n",
    "    if isinstance(K, LinearOperator):\n",
    "        K = to_dense(K)\n",
    "    if isinstance(K, Tensor):\n",
    "        K = K.clone().detach().numpy()\n",
    "    _PSD(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For only the 0-th harmonic level, and two test points, manual_seed = 0 gives non-PSD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception at manual seed 0\n",
      "mat1 and mat2 shapes cannot be multiplied (1x10 and 9x2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "for i in tqdm(range(10000)):\n",
    "    try:\n",
    "        torch.random.manual_seed(i)\n",
    "        x = torch.randn(2, 9)\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        test_psd(model(x).covariance_matrix)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception at manual seed {i}\")\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input matrix must be symmetric positive semidefinite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtest_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mtest_psd\u001b[0;34m(K)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(K, Tensor):\n\u001b[1;32m      5\u001b[0m     K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 6\u001b[0m \u001b[43m_PSD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/scipy/stats/_multivariate.py:172\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(s) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39meps:\n\u001b[1;32m    171\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input matrix must be symmetric positive semidefinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    173\u001b[0m d \u001b[38;5;241m=\u001b[39m s[s \u001b[38;5;241m>\u001b[39m eps]\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_singular:\n",
      "\u001b[0;31mValueError\u001b[0m: The input matrix must be symmetric positive semidefinite."
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "x = torch.randn(2, 9)\n",
    "x = x / x.norm(dim=-1, keepdim=True)\n",
    "test_psd(model(x).covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4128) tensor([[1.0000]]) tensor([[2.5000]])\n"
     ]
    }
   ],
   "source": [
    "test_psd(model(x, prior=True).covariance_matrix)\n",
    "test_psd(model.variational_strategy.prior_distribution.covariance_matrix)\n",
    "test_psd(model.variational_strategy.variational_distribution.covariance_matrix)\n",
    "\n",
    "print(model.covar_module.outputscale, model.covar_module.base_kernel.lengthscale, model.covar_module.base_kernel.nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0014,  0.0081])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(model(x).covariance_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
