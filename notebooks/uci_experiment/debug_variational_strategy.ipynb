{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data\n",
    "Currently supported datasets: power, protein, kin8nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import geometric_kernels.torch \n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str = DEFAULT_URL):\n",
    "        super().__init__(name='kin8nm', path=path, normalize=normalize, seed=seed)\n",
    "        self.url = url \n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model\n",
    "\n",
    "This is done using the same model arguments as for the benchmarking experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = 1. + torch.exp(torch.randn(torch.Size([])))\n",
    "        self.register_parameter('b', torch.nn.Parameter(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "from gpytorch.means import Mean\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "\n",
    "\n",
    "import torch \n",
    "from gpytorch import Module, settings\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.utils.memoize import cached, clear_cache_hook\n",
    "from linear_operator.operators import DiagLinearOperator\n",
    "from functools import cached_property\n",
    "# TODO Maybe just move the functions from spherical_harmonic_features.py into this file?\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree, matern_Kuu, matern_LT_Phi\n",
    "\n",
    "\n",
    "class SphericalHarmonicFeaturesVariationalStrategy(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ApproximateGP,\n",
    "        variational_distribution: CholeskyVariationalDistribution,\n",
    "        dimension: int, \n",
    "        num_spherical_harmonics: int, \n",
    "        jitter_val: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._jitter_val = jitter_val\n",
    "\n",
    "        # model, set via object.__setattr__ to avoid treatment as a module, parameter, or buffer\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "\n",
    "        # Variational distribution\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.register_buffer(\"variational_params_initialized\", torch.tensor(0))\n",
    "\n",
    "        # spherical harmonics \n",
    "        self.dimension = dimension \n",
    "        self.degree, self.num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "\n",
    "    @property\n",
    "    def jitter_val(self) -> float:\n",
    "        if self._jitter_val is None:\n",
    "            return settings.variational_cholesky_jitter.value(dtype=torch.get_default_dtype())\n",
    "        return self._jitter_val\n",
    "\n",
    "    @property\n",
    "    def model(self) -> ApproximateGP:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def covar_module(self) -> ScaleKernel | GeometricMaternKernel:\n",
    "        return self.model.covar_module\n",
    "    \n",
    "    @cached_property\n",
    "    def base_kernel(self) -> GeometricMaternKernel:\n",
    "        if isinstance(self.covar_module, ScaleKernel):\n",
    "            return self.covar_module.base_kernel\n",
    "        else:\n",
    "            return self.covar_module\n",
    "    \n",
    "    @property\n",
    "    def mean_module(self) -> Mean:\n",
    "        return self.model.mean_module\n",
    "\n",
    "    @property\n",
    "    def kappa(self) -> Tensor:\n",
    "        return self.base_kernel.lengthscale\n",
    "    \n",
    "    @property\n",
    "    def nu(self) -> Tensor | float:\n",
    "        return self.base_kernel.nu\n",
    "    \n",
    "    @property \n",
    "    def outputscale(self) -> Tensor:\n",
    "        return self.covar_module.outputscale if hasattr(self.covar_module, \"outputscale\") else torch.tensor(1.0)\n",
    "\n",
    "    @property \n",
    "    def sigma(self) -> Tensor: \n",
    "        return self.outputscale.sqrt()\n",
    "\n",
    "    def _clear_cache(self) -> None:\n",
    "        clear_cache_hook(self)\n",
    "\n",
    "    @property\n",
    "    @cached(name=\"prior_distribution_memo\")\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        covariance_matrix = DiagLinearOperator(torch.ones(self.num_spherical_harmonics))\n",
    "        mean = torch.zeros(self.num_spherical_harmonics)\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=covariance_matrix)\n",
    "    \n",
    "    @property \n",
    "    @cached(name=\"cholesky_factor_prior_memo\")\n",
    "    def cholesky_factor_prior(self) -> DiagLinearOperator:\n",
    "        Kuu = matern_Kuu(max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma)\n",
    "        return Kuu.cholesky() # Kuu is a DiagLinearOperator, so .cholesky() is equivalent to .sqrt() \n",
    "        \n",
    "    @property\n",
    "    @cached(name=\"variational_distribution_memo\")\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs) -> MultivariateNormal:\n",
    "        # inducing-inducing prior\n",
    "        pu = self.prior_distribution\n",
    "        invL_muu, invL_Kuu_invLt = pu.mean, pu.lazy_covariance_matrix\n",
    "\n",
    "        # input-input prior\n",
    "        px = self.model.forward(x)\n",
    "        mux, Kxx = px.mean, px.lazy_covariance_matrix\n",
    "\n",
    "        # inducing-inducing variational\n",
    "        qu = self.variational_distribution\n",
    "        invL_m, invL_S_invLt = qu.mean, qu.lazy_covariance_matrix\n",
    "\n",
    "        # Add jitter to Kxx and invL_Kuu_invLt for numerical stability\n",
    "        Kxx = Kxx.add_jitter(self.jitter_val)\n",
    "        invL_Kuu_invLt = invL_Kuu_invLt.add_jitter(self.jitter_val)\n",
    "\n",
    "        # inducing-input prior  \n",
    "        LT_Phi = matern_LT_Phi(\n",
    "            x, max_ell=self.degree, d=self.dimension, kappa=self.kappa, nu=self.nu, sigma=self.sigma,\n",
    "        ) # [..., O, num_harmonics, N]\n",
    "        \n",
    "        # Update the mean\n",
    "        mean_update = torch.einsum('...ij,...i->...j', LT_Phi, invL_m - invL_muu) # [..., O, num_harmonics, N] @ [O, num_harmonics] -> [..., O, N]\n",
    "        updated_mean = mux + mean_update # [..., O, N] + [..., O, N] -> [..., O, N]\n",
    "\n",
    "        # Update the covariance matrix\n",
    "        covariance_matrix_update = LT_Phi.mT @ (invL_S_invLt - invL_Kuu_invLt) @ LT_Phi # [O, num_harmonics, num_harmonics] @ [O, num_harmonics, num_harmonics] @ [O, num_harmonics, N] -> [O, num_harmonics, N]\n",
    "        updated_covariance_matrix = Kxx + covariance_matrix_update # [..., O, N, N] + [..., O, N, N] -> [..., O, N, N]\n",
    "\n",
    "        return MultivariateNormal(mean=updated_mean, covariance_matrix=updated_covariance_matrix)\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        with settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x: Tensor, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        # If we're in prior mode, then we're done!\n",
    "        if prior:\n",
    "            return self.model.forward(x, **kwargs)\n",
    "\n",
    "        # Delete previously cached items from the training distribution\n",
    "        if self.training:\n",
    "            self._clear_cache()\n",
    "\n",
    "        # (Maybe) initialize variational distribution\n",
    "        if not self.variational_params_initialized.item():\n",
    "            prior_dist = self.prior_distribution\n",
    "            self._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_params_initialized.fill_(1)\n",
    "\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple shallow variational GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "\n",
    "class SHApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, dimension: int, num_spherical_harmonics: int, \n",
    "                 mean: Mean, covar_module: GeometricMaternKernel, variational_distribution: CholeskyVariationalDistribution):\n",
    "        variational_strategy = SphericalHarmonicFeaturesVariationalStrategy(\n",
    "            model=self,\n",
    "            variational_distribution=variational_distribution,\n",
    "            dimension=dimension,\n",
    "            num_spherical_harmonics=num_spherical_harmonics,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = mean\n",
    "        self.covar_module = covar_module\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "class IPApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, mean: Mean, covar_module: GeometricMaternKernel, inducing_points: Tensor, variational_distribution: CholeskyVariationalDistribution):\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, \n",
    "            inducing_points=inducing_points,\n",
    "            variational_distribution=variational_distribution, \n",
    "            learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = mean\n",
    "        self.covar_module = covar_module\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Training has to be done differently from the bechmarking experiment, because we need minibatch SGD with the larger datasets and minibatch metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_smallest_eigenvalues(mvn):\n",
    "    # mvn = projector.inverse(mvn)\n",
    "    smallest_eigenvalues = torch.linalg.eigvalsh(mvn.lazy_covariance_matrix.evaluate()).min()\n",
    "    print(f\"Smallest eigenvalue: {smallest_eigenvalues.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 54, which includes all spherical harmonics up to degree 3 (incl.)\n"
     ]
    }
   ],
   "source": [
    "from geometric_kernels.spaces import Hypersphere\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree\n",
    "\n",
    "\n",
    "dataset = Kin8mn()\n",
    "\n",
    "# Generic parameters\n",
    "num_spherical_harmonics = 50\n",
    "dimension = dataset.dimension + 1\n",
    "\n",
    "degree, num_spherical_harmonics = num_spherical_harmonics_to_degree(num_spherical_harmonics, dimension)\n",
    "space = Hypersphere(dimension)\n",
    "\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Model with spherical harmonic features\n",
    "mean = gpytorch.means.ConstantMean()\n",
    "base_kernel = GeometricMaternKernel(nu=2.5, space=space, num_eigenfunctions=4, batch_shape=batch_shape)\n",
    "covar_module = ScaleKernel(base_kernel)\n",
    "variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "    num_inducing_points=num_spherical_harmonics, batch_shape=batch_shape\n",
    ")\n",
    "\n",
    "model_sh = SHApproximateGP(\n",
    "    dimension=dimension,\n",
    "    num_spherical_harmonics=num_spherical_harmonics,\n",
    "    mean=mean,\n",
    "    covar_module=covar_module,\n",
    "    variational_distribution=variational_distribution\n",
    ")\n",
    "\n",
    "# Model with inducing points \n",
    "mean = gpytorch.means.ConstantMean()\n",
    "base_kernel = GeometricMaternKernel(nu=2.5, space=space, num_eigenfunctions=4, batch_shape=batch_shape)\n",
    "covar_module = ScaleKernel(base_kernel)\n",
    "variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "    num_inducing_points=num_spherical_harmonics, batch_shape=batch_shape\n",
    ")\n",
    "\n",
    "inducing_points = torch.randn(num_spherical_harmonics, dimension)\n",
    "inducing_points = inducing_points / inducing_points.norm(dim=-1, keepdim=True)\n",
    "model_ip = IPApproximateGP(\n",
    "    mean=mean,\n",
    "    covar_module=covar_module,\n",
    "    inducing_points=inducing_points,\n",
    "    variational_distribution=variational_distribution\n",
    ")\n",
    "\n",
    "\n",
    "# Arbitrary projector \n",
    "projector = SphereProjector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue: 9.999999986902469e-07\n",
      "1.2891064465375877\n",
      "Smallest eigenvalue: -0.1357181145969312\n",
      "1.1934144881472015\n",
      "Smallest eigenvalue: -0.31078934586160606\n",
      "1.0703865792531075\n",
      "Smallest eigenvalue: -0.46751262150711514\n",
      "0.9690412907119924\n",
      "Smallest eigenvalue: -0.7028916434757697\n",
      "0.8887080844428432\n",
      "Smallest eigenvalue: -0.7881276725005424\n",
      "0.8094053092966281\n",
      "Smallest eigenvalue: -0.8454695057135702\n",
      "0.733132044177425\n",
      "Smallest eigenvalue: -0.8768659247019591\n",
      "0.6799833529956283\n",
      "Smallest eigenvalue: -1.0255301171162763\n",
      "0.652940862556185\n",
      "Smallest eigenvalue: -0.9627411756820207\n",
      "0.6183478034115152\n",
      "Smallest eigenvalue: -0.7940298870665754\n",
      "0.5922879532189999\n",
      "Smallest eigenvalue: -0.7649416935806412\n",
      "0.5508058104598297\n",
      "Smallest eigenvalue: -0.7691073318780455\n",
      "0.5176412553545229\n",
      "Smallest eigenvalue: -0.7322238661869614\n",
      "0.4622863761150729\n",
      "Smallest eigenvalue: -0.6808265091418633\n",
      "0.4410889824106476\n",
      "Smallest eigenvalue: -0.5731409060232125\n",
      "0.3973870074592462\n",
      "Smallest eigenvalue: -0.4790347345425624\n",
      "0.37233160617535827\n",
      "Smallest eigenvalue: -0.410111573771707\n",
      "0.3265024088952648\n",
      "Smallest eigenvalue: -0.3177507395921286\n",
      "0.31256139654020537\n",
      "Smallest eigenvalue: -0.29182123456083886\n",
      "0.25287716166563057\n",
      "Smallest eigenvalue: -0.29985243101004533\n",
      "0.23054489081511687\n",
      "Smallest eigenvalue: -0.30630786787890235\n",
      "0.1600620723744386\n",
      "Smallest eigenvalue: -0.36468694815246167\n",
      "0.14407048402282166\n",
      "Smallest eigenvalue: -0.4068289359636901\n",
      "0.10469809317429883\n",
      "Smallest eigenvalue: -0.45024318755286846\n",
      "0.07399778887830247\n",
      "Smallest eigenvalue: -0.498192369759199\n",
      "0.031096650635078788\n",
      "Smallest eigenvalue: -0.5495821801663101\n",
      "0.013386166407420415\n",
      "Smallest eigenvalue: -0.5846317059270612\n",
      "-0.02125625891713888\n",
      "Smallest eigenvalue: -0.42022854284940264\n",
      "-0.021439693689449545\n"
     ]
    }
   ],
   "source": [
    "model = model_sh\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_x.size(0))\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    x_batch, y_batch = projector(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_batch)\n",
    "    print_smallest_eigenvalues(output)\n",
    "    loss = -mll(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue: 9.999999955160189e-07\n",
      "1.3038401902801835\n",
      "Smallest eigenvalue: 9.999999971823377e-07\n",
      "1.168367487019407\n",
      "Smallest eigenvalue: 9.999999963133244e-07\n",
      "1.016595786504766\n",
      "Smallest eigenvalue: 9.999999983053191e-07\n",
      "0.9079352812559105\n",
      "Smallest eigenvalue: 9.999999985664043e-07\n",
      "0.841934390424427\n",
      "Smallest eigenvalue: 9.999999983287972e-07\n",
      "0.7654001050887477\n",
      "Smallest eigenvalue: 9.999999983572765e-07\n",
      "0.7631627858428819\n",
      "Smallest eigenvalue: 9.999999983287072e-07\n",
      "0.7536802053385149\n",
      "Smallest eigenvalue: 9.999999984665512e-07\n",
      "0.7057247497882502\n",
      "Smallest eigenvalue: 9.999999984865905e-07\n",
      "0.6492769598871178\n",
      "Smallest eigenvalue: 9.999999985245056e-07\n",
      "0.608138439190725\n",
      "Smallest eigenvalue: 9.999999985290089e-07\n",
      "0.5647388357474672\n",
      "Smallest eigenvalue: 9.999999985193053e-07\n",
      "0.5345302676337101\n",
      "Smallest eigenvalue: 9.999999986204065e-07\n",
      "0.4971043280994499\n",
      "Smallest eigenvalue: 9.999999985592236e-07\n",
      "0.47382076199168943\n",
      "Smallest eigenvalue: 9.999999985315982e-07\n",
      "0.4354417100359528\n",
      "Smallest eigenvalue: 9.999999985518042e-07\n",
      "0.3992544526447528\n",
      "Smallest eigenvalue: 9.999999983833846e-07\n",
      "0.37014203664464057\n",
      "Smallest eigenvalue: 9.999999983575575e-07\n",
      "0.3671051573470269\n",
      "Smallest eigenvalue: 9.999999983691428e-07\n",
      "0.2876946317322691\n",
      "Smallest eigenvalue: 9.999999986024324e-07\n",
      "0.269382884547774\n",
      "Smallest eigenvalue: 9.999999985066326e-07\n",
      "0.2028820734625726\n",
      "Smallest eigenvalue: 9.999999984142603e-07\n",
      "0.18962548031017742\n",
      "Smallest eigenvalue: 9.99999998544422e-07\n",
      "0.17502486645810425\n",
      "Smallest eigenvalue: 9.999999983868952e-07\n",
      "0.1577362096557518\n",
      "Smallest eigenvalue: 9.999999985948576e-07\n",
      "0.16594547724780676\n",
      "Smallest eigenvalue: 9.99999998531782e-07\n",
      "0.06079947561680413\n",
      "Smallest eigenvalue: 9.999999985052443e-07\n",
      "0.09237207768946651\n",
      "Smallest eigenvalue: 1.0028879817666443e-06\n",
      "0.0546417535135516\n"
     ]
    }
   ],
   "source": [
    "model = model_ip\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_x.size(0))\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    x_batch, y_batch = projector(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_batch)\n",
    "    print_smallest_eigenvalues(output)\n",
    "    loss = -mll(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def plot_kernel_vs_angle(kernel, dimension: int, n: int = 100): \n",
    "    pole = torch.zeros(1, dimension)\n",
    "    pole[:, -1] = 1.\n",
    "    theta = torch.linspace(0, torch.pi, n)\n",
    "    x = torch.cat([torch.zeros(n, dimension - 2), theta.cos().unsqueeze(-1), theta.sin().unsqueeze(-1)], dim=-1)\n",
    "    with torch.no_grad():\n",
    "        y = kernel(x).lazy_covariance_matrix[..., 0]\n",
    "        if y.ndim == 2: \n",
    "            y = y.mean(0)\n",
    "\n",
    "    data = pd.DataFrame({'theta': theta.squeeze().numpy(), 'y': y})\n",
    "    fig = px.line(data, x='theta', y='y')\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
