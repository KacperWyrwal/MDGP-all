{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the doubly stochastic variational inference paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types \n",
    "from torch import Tensor  \n",
    "\n",
    "\n",
    "# Imports \n",
    "import torch \n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "from gpytorch.metrics import negative_log_predictive_density\n",
    "from tqdm.autonotebook import tqdm\n",
    "from math import ceil\n",
    "from scipy.special import logsumexp\n",
    "from scipy.cluster.vq import kmeans2, ClusterError\n",
    "from mdgp.experiments.uci.data.datasets import UCIDataset, Power, Kin8mn, Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings\n",
    "The datasets are small and can fit on a GPU, so there is not need to move data in and out of the GPU. Thus, simply setting the default device should be a harmless way to run on GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "torch.set_default_dtype(DTYPE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings taken from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model \n",
    "LIKELIHOOD_VARIANCE = 0.01\n",
    "LENGTHSCALE = 2.0\n",
    "INNER_LAYER_VARIANCE = 1e-5\n",
    "OUTPUT_LAYER_VARIANCE = 1.0 # This is a (reasonable) guess\n",
    "NUM_INDUCING_POINTS = 100\n",
    "MAX_HIDDEN_DIMS = 30\n",
    "\n",
    "# Training \n",
    "LR = 0.01\n",
    "NUM_ITERATIONS = 20_000\n",
    "BATCH_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean deep GP initialized according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_dims(dataset: UCIDataset) -> int:\n",
    "    return min(MAX_HIDDEN_DIMS, dataset.dimension)\n",
    "\n",
    "\n",
    "def empty_cluster_safe_kmeans(x: Tensor, k: int, num_retries: int = 1000) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    for _ in range(num_retries):\n",
    "        try:\n",
    "            return torch.from_numpy(kmeans2(x, k, missing='raise')[0]).to(x.device, x.dtype)\n",
    "        except ClusterError:\n",
    "            continue \n",
    "    return torch.from_numpy(kmeans2(x, k)[0]).to(x.device, x.dtype)\n",
    "    raise ClusterError(f\"Failed to find {k} clusters in {num_retries} retries.\")\n",
    "\n",
    "\n",
    "def get_inducing_points(dataset: UCIDataset, num_inducing_points: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    return empty_cluster_safe_kmeans(dataset.train_x, num_inducing_points)\n",
    "\n",
    "\n",
    "class EuclideanDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(self, inducing_points, output_dims, hidden: bool = False):\n",
    "        input_dims = inducing_points.size(-1)\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0), \n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        base_kernel = RBFKernel(batch_shape=batch_shape)\n",
    "        base_kernel.lengthscale = LENGTHSCALE\n",
    "        # Use ard_num_dims=input_dims adds a lengthscale for each input dimension \n",
    "        # \"we choose the RBF kernel with a lengthscale for each dimension\" (from paper)\n",
    "        self.covar_module = ScaleKernel(base_kernel, batch_shape=batch_shape, ard_num_dims=input_dims)\n",
    "        if hidden:\n",
    "            self.mean_module = LinearMean(input_dims, batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = INNER_LAYER_VARIANCE\n",
    "        else:\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = OUTPUT_LAYER_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        covar = self.covar_module(x)\n",
    "        mean = self.mean_module(x)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "    \n",
    "\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "    \n",
    "\n",
    "class EuclideanDeepGP(DeepGP):\n",
    "    def __init__(self, dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS):\n",
    "        super().__init__()\n",
    "        num_hidden_dims = get_hidden_dims(dataset)\n",
    "        inducing_points = get_inducing_points(dataset, num_inducing_points)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [EuclideanDeepGPLayer(inducing_points, num_hidden_dims, hidden=True) for _ in range(num_layers - 1)] + \n",
    "            [EuclideanDeepGPLayer(inducing_points, None, hidden=False)]\n",
    "        )\n",
    "        self.likelihood = MultitaskGaussianLikelihood(num_tasks=dataset.num_outputs)\n",
    "        self.likelihood.noise = LIKELIHOOD_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "def get_model(dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS) -> EuclideanDeepGP:\n",
    "    \"\"\"\n",
    "    Creates a model and moves it to the device that the experiment is running on.\n",
    "    \"\"\"\n",
    "    return EuclideanDeepGP(dataset, num_layers, num_inducing_points).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate model according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_and_to_device(x):\n",
    "    return tuple(_x.to(DEVICE) for _x in default_collate(x))\n",
    "\n",
    "\n",
    "def batch_size(dataset):\n",
    "    return min(BATCH_SIZE, dataset.train_x.size(0))\n",
    "        \n",
    "\n",
    "def num_epochs(dataset) -> int:\n",
    "    iterations_per_epoch = ceil(dataset.train_x.size(0) / batch_size(dataset))\n",
    "    return ceil(NUM_ITERATIONS / iterations_per_epoch)\n",
    "\n",
    "\n",
    "def train_step(x: Tensor, y: Tensor, model: EuclideanDeepGP, optimizer: torch.optim.Optimizer, elbo: VariationalELBO) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset: UCIDataset, model: EuclideanDeepGP) -> list[float]: \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, maximize=True)\n",
    "    elbo = DeepApproximateMLL(VariationalELBO(model.likelihood, model, dataset.train_y.size(0)))\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_and_to_device)\n",
    "\n",
    "    losses = []\n",
    "    # for _ in (pbar := tqdm(range(num_epochs(dataset)), desc='Epochs')):\n",
    "    for _ in (pbar := tqdm(range(1000), desc='Epochs')):\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, optimizer=optimizer, elbo=elbo)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': epoch_loss})\n",
    "\n",
    "    return losses \n",
    "\n",
    "        \n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.from_numpy(logpdf).mean()\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean = outputs.mean.mean(0)\n",
    "    return y_std ** 2 * ((mean - targets) ** 2).mean(0)\n",
    "        \n",
    "\n",
    "def evaluate(dataset: UCIDataset, model: EuclideanDeepGP) -> dict[str, float]:\n",
    "    with torch.no_grad():\n",
    "        out = model.likelihood(model(dataset.test_x))\n",
    "        tll = test_log_likelihood(out, dataset.test_y, dataset.test_y_std)\n",
    "        mse = mean_squared_error(out, dataset.test_y, dataset.test_y_std)\n",
    "        metrics = {\n",
    "            'tll': tll.mean().item(), \n",
    "            'mse': mse.mean().item(),\n",
    "            'nlpd': negative_log_predictive_density(out, dataset.test_y).mean().item()\n",
    "        }\n",
    "        print(f\"TLL: {metrics['tll']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS, num_runs: int = 5):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "        torch.random.manual_seed(run)\n",
    "        model = get_model(dataset, num_layers=num_layers, num_inducing_points=num_inducing_points)\n",
    "        train(dataset, model)\n",
    "        run_metrics = evaluate(dataset, model)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Kin8mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([10, 692]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset.train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/scipy/cluster/vq.py:602: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6487c8e6edb49f7bdf1dbb98d7c85f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[692]' is invalid for input of size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(dataset, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     31\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     35\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, optimizer, elbo)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43melbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/mlls/deep_approximate_mll.py:30\u001b[0m, in \u001b[0;36mDeepApproximateMLL.forward\u001b[0;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, approximate_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_mll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproximate_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/mlls/variational_elbo.py:77\u001b[0m, in \u001b[0;36mVariationalELBO.forward\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, variational_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Calling this function will call the likelihood's :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariational_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/mlls/_approximate_mll.py:58\u001b[0m, in \u001b[0;36m_ApproximateMarginalLogLikelihood.forward\u001b[0;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Get likelihood term and KL term\u001b[39;00m\n\u001b[1;32m     57\u001b[0m num_batch \u001b[38;5;241m=\u001b[39m approximate_dist_f\u001b[38;5;241m.\u001b[39mevent_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_likelihood_term\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproximate_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(num_batch)\n\u001b[1;32m     59\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvariational_strategy\u001b[38;5;241m.\u001b[39mkl_divergence()\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Add any additional registered loss terms\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/mlls/variational_elbo.py:61\u001b[0m, in \u001b[0;36mVariationalELBO._log_likelihood_term\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_likelihood_term\u001b[39m(\u001b[38;5;28mself\u001b[39m, variational_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariational_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:47\u001b[0m, in \u001b[0;36m_GaussianLikelihoodBase.expected_log_prob\u001b[0;34m(self, target, input, *params, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shaped_noise_covar(mean\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdiagonal(dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Potentially reshape the noise to deal with the multitask case\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43mnoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m res \u001b[38;5;241m=\u001b[39m ((target \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m.\u001b[39msquare() \u001b[38;5;241m+\u001b[39m variance) \u001b[38;5;241m/\u001b[39m noise \u001b[38;5;241m+\u001b[39m noise\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)\n\u001b[1;32m     50\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[692]' is invalid for input of size 20"
     ]
    }
   ],
   "source": [
    "dataset = Energy()\n",
    "num_layers = 1\n",
    "model = get_model(dataset, num_layers=num_layers)\n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLL: -3.0224165287975486, MSE: 18.193960189819336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tll': -3.0224165287975486,\n",
       " 'mse': 18.193960189819336,\n",
       " 'nlpd': 0.17883741855621338}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
