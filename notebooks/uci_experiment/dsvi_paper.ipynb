{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the doubly stochastic variational inference paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39505/3846566182.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Types \n",
    "from torch import Tensor  \n",
    "\n",
    "\n",
    "# Imports \n",
    "import torch \n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "from gpytorch.metrics import negative_log_predictive_density\n",
    "from tqdm.autonotebook import tqdm\n",
    "from math import ceil\n",
    "from scipy.special import logsumexp\n",
    "from scipy.cluster.vq import kmeans2, ClusterError\n",
    "from mdgp.experiments.uci.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings\n",
    "The datasets are small and can fit on a GPU, so there is not need to move data in and out of the GPU. Thus, simply setting the default device should be a harmless way to run on GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "torch.set_default_dtype(DTYPE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings taken from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model \n",
    "LIKELIHOOD_VARIANCE = 0.01\n",
    "LENGTHSCALE = 2.0\n",
    "INNER_LAYER_VARIANCE = 1e-5\n",
    "OUTPUT_LAYER_VARIANCE = 1.0 # This is a (reasonable) guess\n",
    "NUM_INDUCING_POINTS = 100\n",
    "MAX_HIDDEN_DIMS = 30\n",
    "\n",
    "# Training \n",
    "LR = 0.01\n",
    "NUM_ITERATIONS = 20_000\n",
    "BATCH_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean deep GP initialized according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_dims(dataset: UCIDataset) -> int:\n",
    "    return min(MAX_HIDDEN_DIMS, dataset.dimension)\n",
    "\n",
    "\n",
    "def empty_cluster_safe_kmeans(x: Tensor, k: int, num_retries: int = 10) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    for _ in range(num_retries):\n",
    "        try:\n",
    "            return torch.from_numpy(kmeans2(x, k, missing='raise')[0]).to(x.device, x.dtype)\n",
    "        except ClusterError:\n",
    "            continue \n",
    "    raise ClusterError(f\"Failed to find {k} clusters in {num_retries} retries.\")\n",
    "\n",
    "\n",
    "def get_inducing_points(dataset: UCIDataset, num_inducing_points: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    return empty_cluster_safe_kmeans(dataset.train_x, num_inducing_points)\n",
    "\n",
    "\n",
    "class EuclideanDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(self, inducing_points, output_dims, hidden: bool = False):\n",
    "        input_dims = inducing_points.size(-1)\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0), \n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        base_kernel = RBFKernel(batch_shape=batch_shape)\n",
    "        base_kernel.lengthscale = LENGTHSCALE\n",
    "        # Use ard_num_dims=input_dims adds a lengthscale for each input dimension \n",
    "        # \"we choose the RBF kernel with a lengthscale for each dimension\" (from paper)\n",
    "        self.covar_module = ScaleKernel(base_kernel, batch_shape=batch_shape, ard_num_dims=input_dims)\n",
    "        if hidden:\n",
    "            self.mean_module = LinearMean(input_dims, batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = INNER_LAYER_VARIANCE\n",
    "        else:\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = OUTPUT_LAYER_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        covar = self.covar_module(x)\n",
    "        mean = self.mean_module(x)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "    \n",
    "\n",
    "class EuclideanDeepGP(DeepGP):\n",
    "    def __init__(self, dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS):\n",
    "        super().__init__()\n",
    "        num_hidden_dims = get_hidden_dims(dataset)\n",
    "        inducing_points = get_inducing_points(dataset, num_inducing_points)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [EuclideanDeepGPLayer(inducing_points, num_hidden_dims, hidden=True) for _ in range(num_layers - 1)] + \n",
    "            [EuclideanDeepGPLayer(inducing_points, None, hidden=False)]\n",
    "        )\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "        self.likelihood.noise = LIKELIHOOD_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "def get_model(dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS) -> EuclideanDeepGP:\n",
    "    \"\"\"\n",
    "    Creates a model and moves it to the device that the experiment is running on.\n",
    "    \"\"\"\n",
    "    return EuclideanDeepGP(dataset, num_layers, num_inducing_points).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate model according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_and_to_device(x):\n",
    "    return tuple(_x.to(DEVICE) for _x in default_collate(x))\n",
    "\n",
    "\n",
    "def batch_size(dataset):\n",
    "    return min(BATCH_SIZE, dataset.train_x.size(0))\n",
    "        \n",
    "\n",
    "def num_epochs(dataset) -> int:\n",
    "    iterations_per_epoch = ceil(dataset.train_x.size(0) / batch_size(dataset))\n",
    "    return ceil(NUM_ITERATIONS / iterations_per_epoch)\n",
    "\n",
    "\n",
    "def train_step(x: Tensor, y: Tensor, model: EuclideanDeepGP, optimizer: torch.optim.Optimizer, elbo: VariationalELBO) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset: UCIDataset, model: EuclideanDeepGP) -> list[float]: \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, maximize=True)\n",
    "    elbo = DeepApproximateMLL(VariationalELBO(model.likelihood, model, dataset.train_y.size(0)))\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_and_to_device)\n",
    "\n",
    "    losses = []\n",
    "    for _ in (pbar := tqdm(range(num_epochs(dataset)), desc='Epochs')):\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, optimizer=optimizer, elbo=elbo)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': epoch_loss})\n",
    "\n",
    "    return losses \n",
    "\n",
    "        \n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.from_numpy(logpdf).mean()\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean = outputs.mean.mean(0)\n",
    "    return y_std ** 2 * ((mean - targets) ** 2).mean(0)\n",
    "        \n",
    "\n",
    "def evaluate(dataset: UCIDataset, model: EuclideanDeepGP) -> dict[str, float]:\n",
    "    with torch.no_grad():\n",
    "        out = model.likelihood(model(dataset.test_x))\n",
    "        tll = test_log_likelihood(out, dataset.test_y, dataset.test_y_std)\n",
    "        mse = mean_squared_error(out, dataset.test_y, dataset.test_y_std)\n",
    "        metrics = {\n",
    "            'tll': tll.mean().item(), \n",
    "            'mse': mse.mean().item(),\n",
    "            'nlpd': negative_log_predictive_density(out, dataset.test_y).mean().item()\n",
    "        }\n",
    "        print(f\"TLL: {metrics['tll']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS, num_runs: int = 5):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "        torch.random.manual_seed(run)\n",
    "        model = get_model(dataset, num_layers=num_layers, num_inducing_points=num_inducing_points)\n",
    "        train(dataset, model)\n",
    "        run_metrics = evaluate(dataset, model)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Kin8mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d4444e679248b5b0950f32365c62f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(dataset, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, optimizer, elbo)\u001b[0m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m elbo(output, y)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Kin8mn()\n",
    "num_layers = 1\n",
    "model = get_model(dataset, num_layers=num_layers)\n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLL: -0.04077663126154846, MSE: 0.04912303388118744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tll': -0.04077663126154846,\n",
       " 'mse': 0.04912303388118744,\n",
       " 'nlpd': 1.1748895645141602}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
