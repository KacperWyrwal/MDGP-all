{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the doubly stochastic variational inference paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35053/28741095.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Types \n",
    "from torch import Tensor  \n",
    "\n",
    "\n",
    "# Imports \n",
    "import os\n",
    "import torch \n",
    "import pandas as pd \n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "from tqdm.autonotebook import tqdm\n",
    "from math import ceil\n",
    "from scipy.special import logsumexp\n",
    "from scipy.cluster.vq import kmeans2, ClusterError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings\n",
    "The datasets are small and can fit on a GPU, so there is not need to move data in and out of the GPU. Thus, simply setting the default device should be a harmless way to run on GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "torch.set_default_dtype(DTYPE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings taken from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model \n",
    "LIKELIHOOD_VARIANCE = 0.01\n",
    "LENGTHSCALE = 2.0\n",
    "INNER_LAYER_VARIANCE = 1e-5\n",
    "OUTPUT_LAYER_VARIANCE = 1.0 # This is a (reasonable) guess\n",
    "NUM_INDUCING_POINTS = 100\n",
    "MAX_HIDDEN_DIMS = 30\n",
    "\n",
    "# Training \n",
    "LR = 0.01\n",
    "NUM_ITERATIONS = 20_000\n",
    "BATCH_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Datasets processed according to the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x: Tensor) -> Tensor:\n",
    "    return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "\n",
    "\n",
    "def train_test_split(x: Tensor, y: Tensor, test_size: float = TEST_SIZE) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "    \"\"\"\n",
    "    Split the dataset into train and test sets.\n",
    "    \"\"\"\n",
    "    split_idx = int(test_size * x.size(0))\n",
    "    return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "def joint_shuffle(*args: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "    perm_idx = torch.randperm(args[0].size(0), generator=generator)\n",
    "    return tuple(x[perm_idx] for x in args)\n",
    "\n",
    "\n",
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, url: str, path: str = '../../data/uci/', seed: int | None = None): \n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        self.name = name \n",
    "        self.url = url\n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Load, shuffle, split, and normalize data (input and outputs).\n",
    "        x, y = self.load_data()\n",
    "        x, y = joint_shuffle(x, y, generator=self.generator)     \n",
    "        train_x, train_y, test_x, test_y = train_test_split(x, y)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(normalize, (train_x, train_y, test_x, test_y))\n",
    "\n",
    "        # Keeping the stardard deviation allows up to \"restore the output scaling for evaluation\" (from paper)\n",
    "        self.test_y_std = test_y.std(dim=0, keepdim=True)\n",
    "        \n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "    \n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values).to(DTYPE)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', seed: int | None = None, url: str | None = None):\n",
    "        url = url or Kin8mn.DEFAULT_URL\n",
    "        super().__init__(name='kin8nm', path=path, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "class Power(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = UCIDataset.UCI_BASE_URL + \"00294/CCPP.zip\"\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', seed: int | None = None, url: str | None = None):\n",
    "        url = url or Power.DEFAULT_URL\n",
    "        super().__init__(name='power', path=path, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        with urlopen(self.url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall('/tmp/')\n",
    "\n",
    "        df = pd.read_excel('/tmp/CCPP//Folds5x2_pp.xlsx')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "class Concrete(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = UCIDataset.UCI_BASE_URL + 'concrete/compressive/Concrete_Data.xls'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', seed: int | None = None, url: str | None = None):\n",
    "        url = url or Concrete.DEFAULT_URL\n",
    "        super().__init__(name='concrete', url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        df = pd.read_excel(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean deep GP initialized according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_dims(dataset: UCIDataset) -> int:\n",
    "    return min(MAX_HIDDEN_DIMS, dataset.dimension)\n",
    "\n",
    "\n",
    "def empty_cluster_safe_kmeans(x: Tensor, k: int, num_retries: int = 10) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    for _ in range(num_retries):\n",
    "        try:\n",
    "            return torch.from_numpy(kmeans2(x, k, missing='raise')[0]).to(x.device, x.dtype)\n",
    "        except ClusterError:\n",
    "            continue \n",
    "    raise ClusterError(f\"Failed to find {k} clusters in {num_retries} retries.\")\n",
    "\n",
    "\n",
    "def get_inducing_points(dataset: UCIDataset, num_inducing_points: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    return empty_cluster_safe_kmeans(dataset.train_x, num_inducing_points)\n",
    "\n",
    "\n",
    "class EuclideanDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(self, inducing_points, output_dims, hidden: bool = False):\n",
    "        input_dims = inducing_points.size(-1)\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0), \n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        base_kernel = RBFKernel(batch_shape=batch_shape)\n",
    "        base_kernel.lengthscale = LENGTHSCALE\n",
    "        # Use ard_num_dims=input_dims adds a lengthscale for each input dimension \n",
    "        # \"we choose the RBF kernel with a lengthscale for each dimension\" (from paper)\n",
    "        self.covar_module = ScaleKernel(base_kernel, batch_shape=batch_shape, ard_num_dims=input_dims)\n",
    "        if hidden:\n",
    "            self.mean_module = LinearMean(input_dims, batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = INNER_LAYER_VARIANCE\n",
    "        else:\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = OUTPUT_LAYER_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        covar = self.covar_module(x)\n",
    "        mean = self.mean_module(x)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "    \n",
    "\n",
    "class EuclideanDeepGP(DeepGP):\n",
    "    def __init__(self, dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS):\n",
    "        super().__init__()\n",
    "        num_hidden_dims = get_hidden_dims(dataset)\n",
    "        inducing_points = get_inducing_points(dataset, num_inducing_points)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [EuclideanDeepGPLayer(inducing_points, num_hidden_dims, hidden=True) for _ in range(num_layers - 1)] + \n",
    "            [EuclideanDeepGPLayer(inducing_points, None, hidden=False)]\n",
    "        )\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "        self.likelihood.noise = LIKELIHOOD_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "def get_model(dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS) -> EuclideanDeepGP:\n",
    "    \"\"\"\n",
    "    Creates a model and moves it to the device that the experiment is running on.\n",
    "    \"\"\"\n",
    "    return EuclideanDeepGP(dataset, num_layers, num_inducing_points).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate model according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_and_to_device(x):\n",
    "    return tuple(_x.to(DEVICE) for _x in default_collate(x))\n",
    "\n",
    "\n",
    "def batch_size(dataset):\n",
    "    return min(BATCH_SIZE, dataset.train_x.size(0))\n",
    "        \n",
    "\n",
    "def num_epochs(dataset) -> int:\n",
    "    iterations_per_epoch = ceil(dataset.train_x.size(0) / batch_size(dataset))\n",
    "    return ceil(NUM_ITERATIONS / iterations_per_epoch)\n",
    "\n",
    "\n",
    "def train_step(x: Tensor, y: Tensor, model: EuclideanDeepGP, optimizer: torch.optim.Optimizer, elbo: VariationalELBO) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset: UCIDataset, model: EuclideanDeepGP) -> list[float]: \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, maximize=True)\n",
    "    elbo = DeepApproximateMLL(VariationalELBO(model.likelihood, model, dataset.train_y.size(0)))\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_and_to_device)\n",
    "\n",
    "    losses = []\n",
    "    for _ in (pbar := tqdm(range(num_epochs(dataset)), desc='Epochs')):\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, optimizer=optimizer, elbo=elbo)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': epoch_loss})\n",
    "\n",
    "    return losses \n",
    "\n",
    "        \n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.from_numpy(logpdf).mean()\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean = outputs.mean.mean(0)\n",
    "    return y_std ** 2 * ((mean - targets) ** 2).mean(0)        \n",
    "        \n",
    "\n",
    "def evaluate(dataset: UCIDataset, model: EuclideanDeepGP) -> dict[str, float]:\n",
    "    with torch.no_grad():\n",
    "        out = model.likelihood(model(dataset.test_x))\n",
    "        tll = test_log_likelihood(out, dataset.test_y, dataset.test_y_std)\n",
    "        mse = mean_squared_error(out, dataset.test_y, dataset.test_y_std)\n",
    "        metrics = {\n",
    "            'tll': tll.mean().item(), \n",
    "            'mse': mse.mean().item(),\n",
    "        }\n",
    "        print(f\"TLL: {metrics['tll']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_layers: int, num_inducing_points: int = 100, num_runs: int = 5):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "        torch.random.manual_seed(run)\n",
    "        model = get_model(dataset, num_layers=num_layers, num_inducing_points=num_inducing_points)\n",
    "        train(dataset, model)\n",
    "        run_metrics = evaluate(dataset, model)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Kin8mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b534f1f83142b983694c415510fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(dataset, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, optimizer, elbo)\u001b[0m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m elbo(output, y)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/functions/_matmul.py:46\u001b[0m, in \u001b[0;36mMatmul.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     44\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (rhs\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m rhs\n\u001b[1;32m     45\u001b[0m     grad_output_matrix \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m grad_output\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m grad_output\n\u001b[0;32m---> 46\u001b[0m     arg_grads \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmatrix_args\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_bilinear_derivative(grad_output_matrix, rhs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# input_2 gradient\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/linear_operator/operators/linear_operator_representation_tree.py:42\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__call__\u001b[0;34m(self, *flattened_representation)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cls(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdifferentiable_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nondifferentiable_kwargs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munflattened_representation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nondifferentiable_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/lazy/lazy_tensor.py:46\u001b[0m, in \u001b[0;36mdeprecated_lazy_tensor.<locals>.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         new_kwargs[name] \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__orig_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Kin8mn()\n",
    "num_layers = 1\n",
    "model = get_model(dataset, num_layers=num_layers)\n",
    "train(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
