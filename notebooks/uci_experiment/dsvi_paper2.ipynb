{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the doubly stochastic variational inference paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_464102/737035116.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "# Types \n",
    "from torch import Tensor  \n",
    "\n",
    "\n",
    "# Imports \n",
    "import torch \n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, MaternKernel\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood, MultitaskGaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "from gpytorch.metrics import negative_log_predictive_density\n",
    "from tqdm.autonotebook import tqdm\n",
    "from math import ceil\n",
    "from scipy.special import logsumexp\n",
    "from scipy.cluster.vq import kmeans2, ClusterError\n",
    "from mdgp.experiments.uci.data.datasets import UCIDataset, Power, Kin8mn, Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings\n",
    "The datasets are small and can fit on a GPU, so there is not need to move data in and out of the GPU. Thus, simply setting the default device should be a harmless way to run on GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "torch.set_default_dtype(DTYPE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings taken from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model \n",
    "LIKELIHOOD_VARIANCE = 0.01\n",
    "LENGTHSCALE = 2.0\n",
    "INNER_LAYER_VARIANCE = 1e-5\n",
    "OUTPUT_LAYER_VARIANCE = 1.0 # This is a (reasonable) guess\n",
    "NUM_INDUCING_POINTS = 100\n",
    "MAX_HIDDEN_DIMS = 30\n",
    "\n",
    "# Training \n",
    "LR = 0.01\n",
    "NUM_ITERATIONS = 20_000\n",
    "BATCH_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean deep GP initialized according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational import IndependentMultitaskVariationalStrategy\n",
    "\n",
    "\n",
    "def get_hidden_dims(dataset: UCIDataset) -> int:\n",
    "    return min(MAX_HIDDEN_DIMS, dataset.dimension)\n",
    "\n",
    "\n",
    "def empty_cluster_safe_kmeans(x: Tensor, k: int, num_retries: int = 100) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    for _ in range(num_retries):\n",
    "        try:\n",
    "            return torch.from_numpy(kmeans2(x, k, missing='raise')[0]).to(x.device, x.dtype)\n",
    "        except ClusterError:\n",
    "            continue \n",
    "    return torch.from_numpy(kmeans2(x, k)[0]).to(x.device, x.dtype)\n",
    "    raise ClusterError(f\"Failed to find {k} clusters in {num_retries} retries.\")\n",
    "\n",
    "\n",
    "def get_inducing_points(dataset: UCIDataset, num_inducing_points: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Initialize inducing points using kmeans. (from paper)\n",
    "    \"\"\"\n",
    "    return empty_cluster_safe_kmeans(dataset.train_x, num_inducing_points)\n",
    "\n",
    "\n",
    "class EuclideanDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(self, inducing_points, output_dims, hidden: bool = False):\n",
    "        input_dims = inducing_points.size(-1)\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0), \n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        # base_kernel = MaternKernel(nu=1.5, batch_shape=batch_shape)\n",
    "        base_kernel = RBFKernel(batch_shape=batch_shape)\n",
    "\n",
    "        base_kernel.lengthscale = LENGTHSCALE\n",
    "        # Use ard_num_dims=input_dims adds a lengthscale for each input dimension \n",
    "        # \"we choose the RBF kernel with a lengthscale for each dimension\" (from paper)\n",
    "        self.covar_module = ScaleKernel(base_kernel, batch_shape=batch_shape, ard_num_dims=input_dims)\n",
    "        if hidden:\n",
    "            self.mean_module = LinearMean(input_dims, batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = INNER_LAYER_VARIANCE\n",
    "        else:\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "            self.covar_module.outputscale = OUTPUT_LAYER_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        covar = self.covar_module(x)\n",
    "        mean = self.mean_module(x)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "    \n",
    "\n",
    "class EuclideanDeepGP(DeepGP):\n",
    "    def __init__(self, dataset: UCIDataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS):\n",
    "        super().__init__()\n",
    "        num_hidden_dims = get_hidden_dims(dataset)\n",
    "        inducing_points = get_inducing_points(dataset, num_inducing_points)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [EuclideanDeepGPLayer(inducing_points, num_hidden_dims, hidden=True) for _ in range(num_layers - 1)] + \n",
    "            [EuclideanDeepGPLayer(inducing_points, dataset.num_outputs, hidden=False)]\n",
    "        )\n",
    "        self.likelihood = MultitaskGaussianLikelihood(dataset.num_outputs)\n",
    "        self.likelihood.noise = LIKELIHOOD_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate model according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_and_to_device(x):\n",
    "    return tuple(_x.to(DEVICE) for _x in default_collate(x))\n",
    "\n",
    "\n",
    "def batch_size(dataset):\n",
    "    return min(BATCH_SIZE, dataset.train_x.size(0))\n",
    "        \n",
    "\n",
    "def num_epochs(dataset) -> int:\n",
    "    iterations_per_epoch = ceil(dataset.train_x.size(0) / batch_size(dataset))\n",
    "    return ceil(NUM_ITERATIONS / iterations_per_epoch)\n",
    "\n",
    "\n",
    "def train_step(x: Tensor, y: Tensor, model: EuclideanDeepGP, optimizer: torch.optim.Optimizer, elbo: VariationalELBO) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset: UCIDataset, model: EuclideanDeepGP, epochs: int = 1000) -> list[float]: \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, maximize=True)\n",
    "    elbo = DeepApproximateMLL(VariationalELBO(model.likelihood, model, dataset.train_y.size(0)))\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_and_to_device)\n",
    "\n",
    "    losses = []\n",
    "    for _ in (pbar := tqdm(range(epochs), desc='Epochs')):\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, optimizer=optimizer, elbo=elbo)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': epoch_loss})\n",
    "\n",
    "    return losses \n",
    "\n",
    "        \n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    logpdf = torch.atleast_2d(logpdf)\n",
    "    logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.from_numpy(logpdf).mean()\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean = outputs.mean.mean(0) if outputs.mean.ndim > 1 else outputs.mean\n",
    "    return ((mean - targets) ** 2 * y_std ** 2).mean()\n",
    "        \n",
    "\n",
    "def evaluate(dataset: UCIDataset, model: EuclideanDeepGP) -> dict[str, float]:\n",
    "    with torch.no_grad():\n",
    "        out = model.likelihood(model(dataset.test_x)).to_data_independent_dist()\n",
    "        tll = test_log_likelihood(out, dataset.test_y, dataset.test_y_std)\n",
    "        mse = mean_squared_error(out, dataset.test_y, dataset.test_y_std)\n",
    "        metrics = {\n",
    "            'tll': tll.mean().item(), \n",
    "            'mse': mse.mean().item(),\n",
    "            'nlpd': negative_log_predictive_density(out, dataset.test_y).mean().item()\n",
    "        }\n",
    "        print(f\"TLL: {metrics['tll']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_layers: int, num_inducing_points: int = NUM_INDUCING_POINTS, num_runs: int = 5):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "        torch.random.manual_seed(run)\n",
    "        model = EuclideanDeepGP(dataset, num_layers=num_layers, num_inducing_points=num_inducing_points)\n",
    "        train(dataset, model)\n",
    "        run_metrics = evaluate(dataset, model)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Shallow Euclidean GP on Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "\n",
    "\n",
    "class SGP(ApproximateGP):\n",
    "    def __init__(self, inducing_points: Tensor):\n",
    "        batch_shape = torch.Size([])\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0), \n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(batch_shape=batch_shape), batch_shape=batch_shape)\n",
    "        self.covar_module.base_kernel.lengthscale = LENGTHSCALE\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "        self.likelihood.noise = LIKELIHOOD_VARIANCE\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return MultivariateNormal(mean, covar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/scipy/cluster/vq.py:602: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c8c49950fc4fad9ffa92dfe4228241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mdgp.experiments.uci.data.datasets import Energy\n",
    "\n",
    "dataset = Energy()\n",
    "x, y = dataset.train_x, dataset.train_y.squeeze()\n",
    "\n",
    "inducing_points = empty_cluster_safe_kmeans(x, 100)\n",
    "model = SGP(inducing_points)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, maximize=True)\n",
    "elbo = VariationalELBO(model.likelihood, model, y.size(0))\n",
    "\n",
    "\n",
    "for _ in (pbar := tqdm(range(10000), desc='Epochs')):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'ELBO': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.5059)\n",
      "tensor(182.4084)\n",
      "783.7306518554688\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(dataset.test_x)\n",
    "    mean = out.mean\n",
    "    print((dataset.test_y_std ** 2 * (mean - dataset.test_y) ** 2).mean().sqrt())\n",
    "    print(mean_squared_error(out, dataset.test_y, dataset.test_y_std))\n",
    "    print(negative_log_predictive_density(out, dataset.test_y).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing X with mean [[7.63950796e-01 6.71782200e+02 3.18748191e+02 1.76517004e+02\n",
      "  5.25759768e+00 3.47756874e+00 2.32995658e-01 2.82344428e+00]] and std [[ 0.10901257 89.79381901 41.44677061 45.98855734  1.74867228  1.16290081\n",
      "   0.13683384  1.56965762]]\n",
      "Normalizing Y with mean [[22.33689725]] and std [[9.9405047]]\n"
     ]
    }
   ],
   "source": [
    "from datasets_dsvi import Energy as EnergyDSVI\n",
    "\n",
    "\n",
    "data = EnergyDSVI().get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "x, y = torch.from_numpy(X).to(DTYPE), torch.from_numpy(Y).to(DTYPE).squeeze(-1)\n",
    "test_x, test_y = torch.from_numpy(Xs).to(DTYPE), torch.from_numpy(Ys).to(DTYPE).squeeze(-1)\n",
    "test_y_std = torch.from_numpy(Y_std).to(DTYPE).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/scipy/cluster/vq.py:602: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    }
   ],
   "source": [
    "inducing_points = empty_cluster_safe_kmeans(x, 100)\n",
    "model = SGP(inducing_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f48733173a4a928812ef5585404bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inducing_points = empty_cluster_safe_kmeans(x, 100)\n",
    "model = SGP(inducing_points)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, maximize=True)\n",
    "elbo = VariationalELBO(model.likelihood, model, y.size(0))\n",
    "\n",
    "\n",
    "for _ in (pbar := tqdm(range(20000), desc='Epochs')):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = elbo(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'ELBO': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6582)\n",
      "tensor(0.4333)\n",
      "-0.7124626636505127\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(test_x)\n",
    "    mean = out.mean\n",
    "    print((test_y_std ** 2 * (mean - test_y) ** 2).mean().sqrt())\n",
    "    print(mean_squared_error(out, test_y, test_y_std))\n",
    "    print(negative_log_predictive_density(out, test_y).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLL: -8.11833064314576, MSE: 98.90326690673828\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model.likelihood(model(test_x))\n",
    "    tll = test_log_likelihood(out, test_y, test_y_std)\n",
    "    mse = mean_squared_error(out, test_y, test_y_std)\n",
    "    metrics = {\n",
    "        'tll': tll.mean().item(), \n",
    "        'mse': mse.mean().item(),\n",
    "        'nlpd': negative_log_predictive_density(out, test_y).mean().item()\n",
    "    }\n",
    "    print(f\"TLL: {metrics['tll']}, MSE: {metrics['mse']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
