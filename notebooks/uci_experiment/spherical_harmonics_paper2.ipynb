{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the spherical harmonics paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n",
      "/tmp/ipykernel_332371/3310190072.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "\n",
    "import torch \n",
    "import gpytorch \n",
    "import geometric_kernels.torch \n",
    "from math import comb \n",
    "from spherical_harmonics import SphericalHarmonics\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from linear_operator.operators import DiagLinearOperator, LinearOperator\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import negative_log_predictive_density, mean_squared_error\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from mdgp.variational.spherical_harmonic_features.utils import * \n",
    "from mdgp.variational.spherical_harmonic_features_variational_strategy import SphericalHarmonicFeaturesVariationalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpytorchSGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, max_ell, d, max_ell_prior, epsilon_sigma=1.0, kappa=1.0, nu=2.5, sigma=1.0, batch_shape=torch.Size([]), jitter_val=1e-6, optimize_nu: bool = True):\n",
    "        m = total_num_harmonics(max_ell, d)\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=m, batch_shape=batch_shape)\n",
    "        variational_strategy = SphericalHarmonicFeaturesVariationalStrategy(self, variational_distribution, num_levels=max_ell, jitter_val=jitter_val)\n",
    "        super().__init__(variational_strategy=variational_strategy)\n",
    "\n",
    "        # constants \n",
    "        self.jitter_val = jitter_val\n",
    "        self.max_ell = max_ell\n",
    "        self.max_ell_prior = max_ell_prior\n",
    "        self.d = d\n",
    "\n",
    "        # modules \n",
    "        base_kernel = GeometricMaternKernel(\n",
    "            space=Hypersphere(d),\n",
    "            lengthscale=kappa, \n",
    "            nu=nu, \n",
    "            trainable_nu=optimize_nu, \n",
    "            num_eigenfunctions=max_ell_prior,\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "        # prior hyperparams \n",
    "        self.covar_module.outputscale = sigma ** 2\n",
    "        self.likelihood.noise = epsilon_sigma ** 2\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        p_sigma = self.covar_module(x)\n",
    "        p_mu = self.mean_module(x)\n",
    "        return MultivariateNormal(p_mu, p_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self, b: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y.squeeze(-1) / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)\n",
    "    \n",
    "\n",
    "class SphereProjectorNoY(torch.nn.Module):\n",
    "    def __init__(self, b: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        return mvn\n",
    "    \n",
    "\n",
    "class StereographicProjector(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Projects a plane R^n to S^n-1 using the stereographic projection.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        s_squared = x.square().sum(dim=-1, keepdim=True)\n",
    "        proj_x0 = (s_squared - 1) / (s_squared + 1)\n",
    "        proj_xi = 2 * x / (s_squared + 1)\n",
    "        proj_x = torch.cat([proj_x0, proj_xi], dim=-1)\n",
    "        if y is None:\n",
    "            return proj_x\n",
    "        return proj_x, y\n",
    "\n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        return mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.experiments.uci.data.datasets import Kin8mn, Power, Concrete, Energy, UCIDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters as in the spherical harmonics paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets \n",
    "kin8mn = Kin8mn()\n",
    "power = Power()\n",
    "concrete = Concrete()\n",
    "\n",
    "# Variational parameters\n",
    "dimension_to_num_harmonics_variational = {\n",
    "    4: 336,\n",
    "    6: 294,\n",
    "    8: 210,\n",
    "}\n",
    "\n",
    "# Prior parameters \n",
    "\"\"\"\n",
    "NOTE \n",
    "- It is difficult to say what number of spherical harmonics was used for the prior.\n",
    "  I set it to be the same as the number of inducing variables.\n",
    "- It is also difficult to say what lengthscale initialisation was used in the paper. \n",
    "  I set it to 1.0 for now, although a lower number, e.g. 0.001, would likely be better for higher dimensions.\n",
    "\"\"\"\n",
    "dimension_to_num_harmonics_prior = {\n",
    "    # 4: 336,\n",
    "    # 6: 294,\n",
    "    # 8: 210, \n",
    "    4: 336, \n",
    "    6: 1000,\n",
    "    8: 625,\n",
    "}\n",
    "nu = 1.5\n",
    "optimize_nu = False\n",
    "kappa = 1.0\n",
    "\n",
    "# Other model parameters\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Training parameters \n",
    "batch_size = 256 \n",
    "LR = 0.01\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    }
   ],
   "source": [
    "from mdgp.variational.spherical_harmonic_features.utils import num_spherical_harmonics_to_num_levels\n",
    "\n",
    "\n",
    "def get_model_and_projector(dataset: UCIDataset, proj: str = 'sphere'):\n",
    "    sphere_dimension = dataset.dimension\n",
    "\n",
    "    # number of levels for variational inference \n",
    "    num_spherical_harmonics = dimension_to_num_harmonics_variational[dataset.dimension]\n",
    "    max_ell, _ = num_spherical_harmonics_to_num_levels(num_spherical_harmonics, sphere_dimension)\n",
    "\n",
    "    # number of levels for prior\n",
    "    num_spherical_harmonics_prior = dimension_to_num_harmonics_prior[dataset.dimension]\n",
    "    max_ell_prior, _ = num_spherical_harmonics_to_num_levels(num_spherical_harmonics_prior, sphere_dimension)\n",
    "\n",
    "    model = gpytorchSGP(max_ell=max_ell, d=sphere_dimension, max_ell_prior=max_ell_prior, kappa=kappa, nu=nu, optimize_nu=optimize_nu, batch_shape=batch_shape)\n",
    "    if proj == 'sphere':\n",
    "        projector = SphereProjector()\n",
    "    elif proj == 'no_y':\n",
    "        projector = SphereProjectorNoY()\n",
    "    else:\n",
    "        projector = StereographicProjector()\n",
    "    return model, projector\n",
    "\n",
    "\n",
    "model, projector = get_model_and_projector(kin8mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, projector, optimizer, mll) -> float:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x, y = projector(x, y)\n",
    "    # print(x.shape, y.shape)\n",
    "    output = model(x)\n",
    "    loss = mll(output, y.squeeze(-1))\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset, model, projector, batch_size, num_epochs=NUM_EPOCHS, lr=LR, optimize_projector: bool = False) -> list[float]: \n",
    "    # optimizer and criterion\n",
    "    parameters = [{\n",
    "        'params': model.parameters()\n",
    "    }]\n",
    "    if optimize_projector:\n",
    "        parameters.append({\n",
    "            'params': projector.parameters()\n",
    "        })\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, maximize=True)\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_y.size(0))\n",
    "\n",
    "    # data\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, projector=projector, optimizer=optimizer, mll=mll)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': losses[-1]})\n",
    "\n",
    "    return losses \n",
    "\n",
    "\n",
    "def evaluate(dataset, model, projector):\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = projector(dataset.test_x), dataset.test_y\n",
    "        test_y = test_y.squeeze(-1)\n",
    "        out = model.likelihood(model(test_x))\n",
    "        out = projector.inverse(out)\n",
    "        nlpd = negative_log_predictive_density(out, test_y)\n",
    "        mse = mean_squared_error(out, test_y)\n",
    "        metrics = {\n",
    "            'nlpd': nlpd.item(), \n",
    "            'mse': mse.item(),\n",
    "        }\n",
    "        print(f\"NLPD: {metrics['nlpd']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, batch_size, num_runs: int = 5, num_epochs=NUM_EPOCHS, lr=LR):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "\n",
    "        torch.random.manual_seed(run)\n",
    "        model, projector = get_model_and_projector(dataset)\n",
    "        train(dataset, model, projector, num_epochs=num_epochs, lr=lr, batch_size=batch_size)\n",
    "        # print(\n",
    "        #     model.covar_module.base_kernel.lengthscale, \n",
    "        #     model.covar_module.base_kernel.nu,\n",
    "        #     model.covar_module.outputscale,\n",
    "        #     model.likelihood.noise,\n",
    "        #     model.variational_strategy._variational_distribution.variational_mean,\n",
    "        #     model.variational_strategy._variational_distribution.chol_variational_covar,\n",
    "        # )\n",
    "        run_metrics = evaluate(dataset, model, projector)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01214f14471b4178a87114568138c789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model, projector \u001b[38;5;241m=\u001b[39m get_model_and_projector(kin8mn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msphere\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkin8mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate(kin8mn, model, projector)\n",
      "Cell \u001b[0;32mIn[47], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, projector, batch_size, num_epochs, lr)\u001b[0m\n\u001b[1;32m     25\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     29\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[47], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, projector, optimizer, mll)\u001b[0m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m mll(output, y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "model, projector = get_model_and_projector(kin8mn, 'sphere')\n",
    "train(kin8mn, model, projector, batch_size, num_epochs=NUM_EPOCHS, lr=LR)\n",
    "evaluate(kin8mn, model, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.9604, requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n",
      "SphereProjectorNoY()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c5efeff5ec4a3ea4cb59f4e1b85a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.6415671221120341, MSE: 0.21345969458131758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nlpd': 0.6415671221120341, 'mse': 0.21345969458131758}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kin8mn = Kin8mn()\n",
    "model, projector = get_model_and_projector(kin8mn, 'no_y')\n",
    "print(projector)\n",
    "projector.b = torch.nn.Parameter(torch.tensor(2.0))\n",
    "train(kin8mn, model, projector, batch_size, num_epochs=NUM_EPOCHS, lr=LR, optimize_projector=True)\n",
    "evaluate(kin8mn, model, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.9711, requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5c31080b6d44cb8efa5997f7090120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.5998450905329317, MSE: 0.198891639633125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nlpd': 0.5998450905329317, 'mse': 0.198891639633125}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, projector = get_model_and_projector(kin8mn, 'stereo')\n",
    "kin8mn = Kin8mn()\n",
    "kin8mn.train_x *= 0.3\n",
    "kin8mn.test_x *= 0.3\n",
    "train(kin8mn, model, projector, batch_size, num_epochs=NUM_EPOCHS, lr=LR)\n",
    "evaluate(kin8mn, model, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.8433933326422332, MSE: 0.31481791472424014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nlpd': 0.8433933326422332, 'mse': 0.31481791472424014}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(kin8mn, model, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing X with mean [[7.63950796e-01 6.71782200e+02 3.18748191e+02 1.76517004e+02\n",
      "  5.25759768e+00 3.47756874e+00 2.32995658e-01 2.82344428e+00]] and std [[ 0.10901257 89.79381901 41.44677061 45.98855734  1.74867228  1.16290081\n",
      "   0.13683384  1.56965762]]\n",
      "Normalizing Y with mean [[22.33689725]] and std [[9.9405047]]\n"
     ]
    }
   ],
   "source": [
    "from datasets_dsvi import Energy as EnergyDSVI\n",
    "DTYPE = torch.get_default_dtype()\n",
    "\n",
    "data = EnergyDSVI().get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "x, y = torch.from_numpy(X).to(DTYPE), torch.from_numpy(Y).to(DTYPE).squeeze(-1)\n",
    "test_x, test_y = torch.from_numpy(Xs).to(DTYPE), torch.from_numpy(Ys).to(DTYPE).squeeze(-1)\n",
    "test_y_std = torch.from_numpy(Y_std).to(DTYPE).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7949c709b7324d5db96814036ce1c6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, projector = get_model_and_projector(kin8mn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, maximize=True)\n",
    "elbo = gpytorch.mlls.VariationalELBO(model.likelihood, model, y.size(0))\n",
    "\n",
    "\n",
    "for _ in (pbar := tqdm(range(1000), desc='Epochs')):\n",
    "    optimizer.zero_grad()\n",
    "    x_batch, y_batch = projector(x, y)\n",
    "    output = model(x_batch)\n",
    "    # output = projector.inverse(output)\n",
    "    loss = elbo(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'ELBO': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1122)\n",
      "0.045148396017277985\n",
      "12.486459883835613\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_x_proj = projector(test_x)\n",
    "    out = model(test_x_proj)\n",
    "    out = projector.inverse(out)\n",
    "    mean = out.mean\n",
    "    print((test_y_std ** 2 * (mean - test_y) ** 2).mean().sqrt())\n",
    "    print(mean_squared_error(out, test_y, test_y_std).item())\n",
    "    print(negative_log_predictive_density(out, test_y).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for power--------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76fa18e6e6545d9b6d3a2737f92ec3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.014695136020697464, MSE: 0.05854422073080649\n",
      "-------------------------------------Run 2--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4b6af2b279445db789d0438dcc903f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreproduce_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkin8mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m, in \u001b[0;36mreproduce_results\u001b[0;34m(dataset, batch_size, num_runs, num_epochs, lr)\u001b[0m\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(run)\n\u001b[1;32m     56\u001b[0m model, projector \u001b[38;5;241m=\u001b[39m get_model_and_projector(dataset)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     model.covar_module.base_kernel.lengthscale, \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     model.covar_module.base_kernel.nu,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     model.variational_strategy._variational_distribution.chol_variational_covar,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     66\u001b[0m run_metrics \u001b[38;5;241m=\u001b[39m evaluate(dataset, model, projector)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, projector, batch_size, num_epochs, lr)\u001b[0m\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     27\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, projector, optimizer, mll)\u001b[0m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m mll(output, y)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reproduce_results(power, num_epochs=1000, batch_size=kin8mn.train_x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for power--------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:20<00:00,  7.04s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.018200022938892634, MSE: 0.0604323361222259\n",
      "-------------------------------------Run 2--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:12<00:00,  6.63s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.020585911675719538, MSE: 0.06111090834631231\n",
      "-------------------------------------Run 3--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:17<00:00,  6.85s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.029410862008512097, MSE: 0.06202288396624321\n",
      "-------------------------------------Run 4--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:17<00:00,  6.89s/it, ELBO=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.011682565872741057, MSE: 0.05958469777710783\n",
      "-------------------------------------Run 5--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:14<00:00,  6.71s/it, ELBO=34.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.018373846236252843, MSE: 0.06075983045730417\n",
      "----------------------------------Metrics mean----------------------------------\n",
      "nlpd    0.019651\n",
      "mse     0.060782\n",
      "dtype: float64\n",
      "----------------------------------Metrics STD-----------------------------------\n",
      "nlpd    0.006391\n",
      "mse     0.000895\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlpd</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.060432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.061111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029411</td>\n",
       "      <td>0.062023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.059585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018374</td>\n",
       "      <td>0.060760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nlpd       mse\n",
       "0  0.018200  0.060432\n",
       "1  0.020586  0.061111\n",
       "2  0.029411  0.062023\n",
       "3  0.011683  0.059585\n",
       "4  0.018374  0.060760"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduce_results(power, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Reproducing results for concrete------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:26<00:00,  1.44it/s, ELBO=2.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3623051474366128, MSE: 0.1097215590579564\n",
      "-------------------------------------Run 2--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:32<00:00,  1.35it/s, ELBO=2.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3629351313556932, MSE: 0.10951533423101913\n",
      "-------------------------------------Run 3--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:30<00:00,  1.39it/s, ELBO=2.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.3790285364454814, MSE: 0.11759354105563866\n",
      "-------------------------------------Run 4--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:28<00:00,  1.41it/s, ELBO=2.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.37304157369488944, MSE: 0.1139838157709856\n",
      "-------------------------------------Run 5--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:28<00:00,  1.41it/s, ELBO=2.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.37610820039944604, MSE: 0.11769848257163341\n",
      "----------------------------------Metrics mean----------------------------------\n",
      "nlpd    0.370684\n",
      "mse     0.113703\n",
      "dtype: float64\n",
      "----------------------------------Metrics STD-----------------------------------\n",
      "nlpd    0.007663\n",
      "mse     0.004018\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlpd</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.362305</td>\n",
       "      <td>0.109722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.362935</td>\n",
       "      <td>0.109515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379029</td>\n",
       "      <td>0.117594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373042</td>\n",
       "      <td>0.113984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.376108</td>\n",
       "      <td>0.117698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nlpd       mse\n",
       "0  0.362305  0.109722\n",
       "1  0.362935  0.109515\n",
       "2  0.379029  0.117594\n",
       "3  0.373042  0.113984\n",
       "4  0.376108  0.117698"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduce_results(concrete, num_epochs=125)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
