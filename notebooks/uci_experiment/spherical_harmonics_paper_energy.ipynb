{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reproduces results from the spherical harmonics paper on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n",
      "/tmp/ipykernel_25927/3310190072.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "\n",
    "import torch \n",
    "import gpytorch \n",
    "import geometric_kernels.torch \n",
    "from math import comb \n",
    "from spherical_harmonics import SphericalHarmonics\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from linear_operator.operators import DiagLinearOperator, LinearOperator\n",
    "from mdgp.kernels import GeometricMaternKernel\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import negative_log_predictive_density, mean_squared_error\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from mdgp.variational.spherical_harmonic_features.utils import * \n",
    "from mdgp.variational.spherical_harmonic_features_variational_strategy import SphericalHarmonicFeaturesVariationalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpytorchSGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, max_ell, d, max_ell_prior, epsilon_sigma=1.0, kappa=1.0, nu=2.5, sigma=1.0, batch_shape=torch.Size([]), jitter_val=1e-6, optimize_nu: bool = True):\n",
    "        m = total_num_harmonics(max_ell, d)\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=m, batch_shape=batch_shape)\n",
    "        variational_strategy = SphericalHarmonicFeaturesVariationalStrategy(self, variational_distribution, num_levels=max_ell, jitter_val=jitter_val)\n",
    "        super().__init__(variational_strategy=variational_strategy)\n",
    "\n",
    "        # constants \n",
    "        self.jitter_val = jitter_val\n",
    "        self.max_ell = max_ell\n",
    "        self.max_ell_prior = max_ell_prior\n",
    "        self.d = d\n",
    "\n",
    "        # modules \n",
    "        base_kernel = GeometricMaternKernel(\n",
    "            space=Hypersphere(d),\n",
    "            lengthscale=kappa, \n",
    "            nu=nu, \n",
    "            trainable_nu=optimize_nu, \n",
    "            num_eigenfunctions=max_ell_prior,\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "        # prior hyperparams \n",
    "        self.covar_module.outputscale = sigma ** 2\n",
    "        self.likelihood.noise = epsilon_sigma ** 2\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        p_sigma = self.covar_module(x)\n",
    "        p_mu = self.mean_module(x)\n",
    "        return MultivariateNormal(p_mu, p_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class SphereProjector(torch.nn.Module):\n",
    "    def __init__(self, b: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y.squeeze(-1) / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters as in the spherical harmonics paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational parameters\n",
    "num_harmonics_variational = 210\n",
    "num_harmonics_prior = 625\n",
    "nu = 1.5\n",
    "optimize_nu = False\n",
    "kappa = 1.0\n",
    "\n",
    "# Other model parameters\n",
    "batch_shape = torch.Size([])\n",
    "\n",
    "# Training parameters \n",
    "batch_size = 256 \n",
    "LR = 0.01\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.variational.spherical_harmonic_features.utils import num_spherical_harmonics_to_num_levels\n",
    "\n",
    "\n",
    "def get_model_and_projector(dim=8):\n",
    "    # number of levels for variational inference \n",
    "    max_ell, _ = num_spherical_harmonics_to_num_levels(num_harmonics_variational, dim)\n",
    "\n",
    "    # number of levels for prior\n",
    "    max_ell_prior, _ = num_spherical_harmonics_to_num_levels(num_harmonics_prior, dim)\n",
    "\n",
    "    model = gpytorchSGP(max_ell=max_ell, d=dim, max_ell_prior=max_ell_prior, kappa=kappa, nu=nu, optimize_nu=optimize_nu, batch_shape=batch_shape)\n",
    "    projector = SphereProjector()\n",
    "    return model, projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, projector, optimizer, mll) -> float:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x, y = projector(x, y)\n",
    "    # print(x.shape, y.shape)\n",
    "    output = model(x)\n",
    "    loss = mll(output, y.squeeze(-1))\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset, model, projector, batch_size, num_epochs=NUM_EPOCHS, lr=LR, optimize_projector: bool = False) -> list[float]: \n",
    "    # optimizer and criterion\n",
    "    parameters = [{\n",
    "        'params': model.parameters()\n",
    "    }]\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, maximize=True)\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_y.size(0))\n",
    "\n",
    "    # data\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, projector=projector, optimizer=optimizer, mll=mll)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': losses[-1]})\n",
    "\n",
    "    return losses \n",
    "\n",
    "\n",
    "def train_lfbgs(train_x, train_y, model, projector, num_epochs=NUM_EPOCHS, lr=1.0) -> list[float]:\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, max_iter=20)\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, train_y.size(0))\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        x, y = projector(train_x, train_y)\n",
    "        output = model(x)\n",
    "        loss = -mll(output, y.squeeze(-1))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    losses = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = optimizer.step(closure)\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': losses[-1]})\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(test_x, test_y, model, projector):\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = projector(test_x), test_y\n",
    "        test_y = test_y.squeeze(-1)\n",
    "        out = model.likelihood(model(test_x))\n",
    "        out = projector.inverse(out)\n",
    "        nlpd = negative_log_predictive_density(out, test_y)\n",
    "        mse = mean_squared_error(out, test_y)\n",
    "        metrics = {\n",
    "            'nlpd': nlpd.item(), \n",
    "            'mse': mse.item(),\n",
    "        }\n",
    "        print(f\"NLPD: {metrics['nlpd']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def reproduce_results(dataset, batch_size, num_runs: int = 5, num_epochs=NUM_EPOCHS, lr=LR):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "\n",
    "        torch.random.manual_seed(run)\n",
    "        model, projector = get_model_and_projector(dataset)\n",
    "        train(dataset, model, projector, num_epochs=num_epochs, lr=lr, batch_size=batch_size)\n",
    "        run_metrics = evaluate(dataset, model, projector)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/MDGP/MDGP-all-bo/notebooks/uci_experiment/datasets_dsvi.py:53: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.type is 'regression':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing X with mean [[7.63950796e-01 6.71782200e+02 3.18748191e+02 1.76517004e+02\n",
      "  5.25759768e+00 3.47756874e+00 2.32995658e-01 2.82344428e+00]] and std [[ 0.10901257 89.79381901 41.44677061 45.98855734  1.74867228  1.16290081\n",
      "   0.13683384  1.56965762]]\n",
      "Normalizing Y with mean [[22.33689725]] and std [[9.9405047]]\n"
     ]
    }
   ],
   "source": [
    "from datasets_dsvi import Energy as EnergyDSVI\n",
    "DTYPE = torch.get_default_dtype()\n",
    "\n",
    "data = EnergyDSVI().get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "x, y = torch.from_numpy(X).to(DTYPE), torch.from_numpy(Y).to(DTYPE).squeeze(-1)\n",
    "test_x, test_y = torch.from_numpy(Xs).to(DTYPE), torch.from_numpy(Ys).to(DTYPE).squeeze(-1)\n",
    "test_y_std = torch.from_numpy(Y_std).to(DTYPE).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9dc8ca45e941e8b7ccac04bb8e8386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1.4869, grad_fn=<NegBackward0>),\n",
       " tensor(0.9331, grad_fn=<NegBackward0>),\n",
       " tensor(0.7039, grad_fn=<NegBackward0>),\n",
       " tensor(0.5486, grad_fn=<NegBackward0>),\n",
       " tensor(0.2744, grad_fn=<NegBackward0>),\n",
       " tensor(0.1352, grad_fn=<NegBackward0>),\n",
       " tensor(0.0258, grad_fn=<NegBackward0>),\n",
       " tensor(-0.1119, grad_fn=<NegBackward0>),\n",
       " tensor(-0.2369, grad_fn=<NegBackward0>),\n",
       " tensor(-0.3286, grad_fn=<NegBackward0>),\n",
       " tensor(-0.3950, grad_fn=<NegBackward0>),\n",
       " tensor(-0.4525, grad_fn=<NegBackward0>),\n",
       " tensor(-0.5009, grad_fn=<NegBackward0>),\n",
       " tensor(-0.5450, grad_fn=<NegBackward0>),\n",
       " tensor(-0.5872, grad_fn=<NegBackward0>),\n",
       " tensor(-0.6234, grad_fn=<NegBackward0>),\n",
       " tensor(-0.6514, grad_fn=<NegBackward0>),\n",
       " tensor(-0.6716, grad_fn=<NegBackward0>),\n",
       " tensor(-0.6863, grad_fn=<NegBackward0>),\n",
       " tensor(-0.6974, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7056, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7118, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7168, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7208, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7242, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7272, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7298, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7323, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7348, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7372, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7396, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7421, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7450, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7479, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7508, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7537, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7567, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7600, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7631, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7662, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7692, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7724, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7753, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7777, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7799, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7818, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7833, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7848, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7864, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7881, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7899, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7918, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7937, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7950, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7961, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7968, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7974, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7978, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7981, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7983, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7985, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7987, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7989, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7990, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7993, grad_fn=<NegBackward0>),\n",
       " tensor(-0.7998, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8001, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8004, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8006, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8008, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8009, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8010, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8011, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8011, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8012, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8013, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8013, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8014, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8015, grad_fn=<NegBackward0>),\n",
       " tensor(-0.8016, grad_fn=<NegBackward0>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, projector = get_model_and_projector()\n",
    "train_lfbgs(x, y, model, projector, num_epochs=80, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 0.05947370034459709, MSE: 0.04431437675793123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nlpd': 0.05947370034459709, 'mse': 0.04431437675793123}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_x, test_y, model, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spherical harmonics requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 660, which includes all spherical harmonics up to level 5 (exclusive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d878e03b29452ea204a76a4b342652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPD: 2.0815537799947337, MSE: 0.9939946234508938\n",
      "NLPD: 1.87599291875139, MSE: 0.08693688022662341\n",
      "NLPD: 1.620360506518523, MSE: 0.055414195833855456\n",
      "NLPD: 1.362619233032334, MSE: 0.05467460285310863\n",
      "NLPD: 1.1022443933201664, MSE: 0.05426147816649955\n",
      "NLPD: 0.8532727792778565, MSE: 0.05444260276696631\n",
      "NLPD: 0.6305420553232204, MSE: 0.05383040729539002\n",
      "NLPD: 0.44651796123683246, MSE: 0.0527989926172567\n",
      "NLPD: 0.3083038323576212, MSE: 0.05149217917667947\n",
      "NLPD: 0.21427132697482854, MSE: 0.04989288854232348\n",
      "NLPD: 0.15500423866140392, MSE: 0.04840779629622393\n",
      "NLPD: 0.12037733547007452, MSE: 0.047261446198796464\n",
      "NLPD: 0.1011249960878665, MSE: 0.04653117979938449\n",
      "NLPD: 0.09029520385370318, MSE: 0.04603873756211407\n",
      "NLPD: 0.08450382761332585, MSE: 0.045757410529742686\n",
      "NLPD: 0.08107958618844814, MSE: 0.04555610090843142\n",
      "NLPD: 0.07896958079252589, MSE: 0.045426193698496514\n",
      "NLPD: 0.07767795256753642, MSE: 0.045338117114614634\n",
      "NLPD: 0.07660445066081982, MSE: 0.04526225770690132\n",
      "NLPD: 0.07612299779694096, MSE: 0.045238926674275454\n",
      "NLPD: 0.07511640367965866, MSE: 0.04515082965640994\n",
      "NLPD: 0.07450104016366083, MSE: 0.045099068463790665\n",
      "NLPD: 0.07398974587993858, MSE: 0.04505582158309199\n",
      "NLPD: 0.07340029376363841, MSE: 0.0450155156208518\n",
      "NLPD: 0.07295965514707772, MSE: 0.04497742165008921\n",
      "NLPD: 0.07238059312543395, MSE: 0.044934777848524815\n",
      "NLPD: 0.07236460107326635, MSE: 0.04505930411416976\n",
      "NLPD: 0.07148019812444883, MSE: 0.04485821053265388\n",
      "NLPD: 0.07096740783738929, MSE: 0.04482481324813637\n",
      "NLPD: 0.07127973051555503, MSE: 0.044904290876851105\n",
      "NLPD: 0.07014753630510029, MSE: 0.04475409175321381\n",
      "NLPD: 0.06965679596810233, MSE: 0.04472160669683906\n",
      "NLPD: 0.06952556896687956, MSE: 0.04472280376118038\n",
      "NLPD: 0.06890539230990396, MSE: 0.04465887842253527\n",
      "NLPD: 0.06851489143238351, MSE: 0.044625759509994214\n",
      "NLPD: 0.06821753427622147, MSE: 0.044613474098213375\n",
      "NLPD: 0.06774545906445735, MSE: 0.0445699177624394\n",
      "NLPD: 0.06830604048072574, MSE: 0.04473107283471589\n",
      "NLPD: 0.06712128757172992, MSE: 0.04451832945097579\n",
      "NLPD: 0.0666795906389746, MSE: 0.04448778168710166\n",
      "NLPD: 0.0664441518787939, MSE: 0.044457040448478674\n",
      "NLPD: 0.06609763032368553, MSE: 0.044434189175434145\n",
      "NLPD: 0.06570160321682755, MSE: 0.044410729232345424\n",
      "NLPD: 0.0655574329756465, MSE: 0.044392436280477336\n",
      "NLPD: 0.0651513493555559, MSE: 0.044365255894018296\n",
      "NLPD: 0.06480532732412739, MSE: 0.0443436789983197\n",
      "NLPD: 0.06475925746161508, MSE: 0.04433430253511148\n",
      "NLPD: 0.06428715160150193, MSE: 0.04429588527441098\n",
      "NLPD: 0.06442803348685056, MSE: 0.044306918270711225\n",
      "NLPD: 0.06376885751901161, MSE: 0.04425816094398631\n",
      "NLPD: 0.06344139624359814, MSE: 0.04423181067537552\n",
      "NLPD: 0.06347443042263264, MSE: 0.0442202350733913\n",
      "NLPD: 0.06294244804848637, MSE: 0.044193949513321035\n",
      "NLPD: 0.06271818978892349, MSE: 0.04416913224331652\n",
      "NLPD: 0.06266640339426513, MSE: 0.04415528583524391\n",
      "NLPD: 0.06217321257045627, MSE: 0.04413515393173348\n",
      "NLPD: 0.06204658149148066, MSE: 0.04412265939800922\n",
      "NLPD: 0.06197309397362707, MSE: 0.04412259599753409\n",
      "NLPD: 0.061529527581327015, MSE: 0.04407869830897237\n",
      "NLPD: 0.061293185500798104, MSE: 0.044061469309677075\n",
      "NLPD: 0.06133881194134356, MSE: 0.04404805295972502\n",
      "NLPD: 0.06088252770262331, MSE: 0.04402834619342576\n",
      "NLPD: 0.06065075260588133, MSE: 0.04401382009132148\n",
      "NLPD: 0.06066706369607141, MSE: 0.043998791568555955\n",
      "NLPD: 0.060227247784678445, MSE: 0.04397968062914635\n",
      "NLPD: 0.060040115785158846, MSE: 0.0439625776500079\n",
      "NLPD: 0.06032330992449351, MSE: 0.0439811614814228\n",
      "NLPD: 0.05964149516096232, MSE: 0.04393566485993736\n",
      "NLPD: 0.05942525596403457, MSE: 0.043917445899071136\n",
      "NLPD: 0.05955865372797058, MSE: 0.04390504029322018\n",
      "NLPD: 0.05909792523252897, MSE: 0.04389220838117898\n",
      "NLPD: 0.05889674974604666, MSE: 0.043873294369985\n",
      "NLPD: 0.05896239065050181, MSE: 0.04386202357021034\n",
      "NLPD: 0.058617291123047206, MSE: 0.043844146242451934\n",
      "NLPD: 0.05845190586237055, MSE: 0.04385214313756294\n",
      "NLPD: 0.05835931817913031, MSE: 0.04384393855609132\n",
      "NLPD: 0.0580862158357513, MSE: 0.043804268358983114\n",
      "NLPD: 0.05788044965382698, MSE: 0.04380192840213307\n",
      "NLPD: 0.05789728269750503, MSE: 0.0438063569331005\n",
      "NLPD: 0.057513175520529765, MSE: 0.043764324877934024\n",
      "NLPD: 0.057350181231520116, MSE: 0.043750842865726045\n",
      "NLPD: 0.05804611696379277, MSE: 0.04382598135796486\n",
      "NLPD: 0.05705089218892752, MSE: 0.04372333198606073\n",
      "NLPD: 0.056824216013028075, MSE: 0.04371312987159655\n",
      "NLPD: 0.05722667650549654, MSE: 0.04370611042372079\n",
      "NLPD: 0.05655756218792756, MSE: 0.04368982324070765\n",
      "NLPD: 0.05641433433439774, MSE: 0.043680351139598464\n",
      "NLPD: 0.05642706555928753, MSE: 0.04368563498381499\n",
      "NLPD: 0.056122336161217624, MSE: 0.043655148754212204\n",
      "NLPD: 0.05606989971348506, MSE: 0.043655570685991026\n",
      "NLPD: 0.05586776699294217, MSE: 0.04363660488321924\n",
      "NLPD: 0.05561901151237968, MSE: 0.04361693551631544\n",
      "NLPD: 0.055459569624525744, MSE: 0.043595182341963946\n",
      "NLPD: 0.0553436032207905, MSE: 0.04359276758927512\n",
      "NLPD: 0.05519723992037078, MSE: 0.04358338967015988\n",
      "NLPD: 0.05508903779810361, MSE: 0.04357022360013991\n",
      "NLPD: 0.05557313620687225, MSE: 0.04363254405878816\n",
      "NLPD: 0.054837670307241945, MSE: 0.04355229997868846\n",
      "NLPD: 0.05464099862240475, MSE: 0.043540096460445696\n",
      "NLPD: 0.05454544722802545, MSE: 0.04351535304490699\n",
      "NLPD: 0.05436549838208293, MSE: 0.04351047367626518\n",
      "NLPD: 0.054288497539436896, MSE: 0.043505561688526415\n",
      "NLPD: 0.05434568368174793, MSE: 0.043508339654817395\n",
      "NLPD: 0.053976973105305914, MSE: 0.04348907505135394\n",
      "NLPD: 0.05385386516180001, MSE: 0.04347817210252118\n",
      "NLPD: 0.05423388083710869, MSE: 0.043459139287087355\n",
      "NLPD: 0.05370546376713738, MSE: 0.043465072201185735\n",
      "NLPD: 0.05353927115225323, MSE: 0.04344832977033884\n",
      "NLPD: 0.05334536206539916, MSE: 0.043437355005542085\n",
      "NLPD: 0.0542098628967265, MSE: 0.04347369679205416\n",
      "NLPD: 0.0531580732367945, MSE: 0.0434230819221795\n",
      "NLPD: 0.05297081235539791, MSE: 0.04340581971049476\n",
      "NLPD: 0.05607571263161362, MSE: 0.04383059478824724\n",
      "NLPD: 0.052864170511047144, MSE: 0.043389618971191514\n",
      "NLPD: 0.052657341124593325, MSE: 0.04337942664323956\n",
      "NLPD: 0.05246538678185624, MSE: 0.043368944063897616\n",
      "NLPD: 0.05307099577297147, MSE: 0.04347065580629312\n",
      "NLPD: 0.05229192325206789, MSE: 0.043344545129311685\n",
      "NLPD: 0.05214252337306354, MSE: 0.043340571047298375\n",
      "NLPD: 0.05204293322007452, MSE: 0.04332274243543021\n",
      "NLPD: 0.05213135584528252, MSE: 0.04334928590397011\n",
      "NLPD: 0.05179877970784502, MSE: 0.04331224929170714\n",
      "NLPD: 0.053575940786689216, MSE: 0.043613701739233464\n",
      "NLPD: 0.05159562859897024, MSE: 0.04329625033069177\n",
      "NLPD: 0.051527485098791785, MSE: 0.04328616107829771\n",
      "NLPD: 0.051696999181470346, MSE: 0.04334062094316844\n",
      "NLPD: 0.05141674074523924, MSE: 0.043275840483857846\n",
      "NLPD: 0.05118849313997727, MSE: 0.043261977346928035\n",
      "NLPD: 0.05100034927605991, MSE: 0.0432513059854005\n",
      "NLPD: 0.05095536569692799, MSE: 0.04324655933443606\n",
      "NLPD: 0.051180940561275694, MSE: 0.0432728383349476\n",
      "NLPD: 0.050685006663846954, MSE: 0.04323130293906231\n",
      "NLPD: 0.05055590786280771, MSE: 0.043217475889623554\n",
      "NLPD: 0.05052383680012034, MSE: 0.04321520783806503\n",
      "NLPD: 0.05057383805627795, MSE: 0.04319486259780159\n",
      "NLPD: 0.05027494041542161, MSE: 0.043192504778608636\n",
      "NLPD: 0.05010519289214385, MSE: 0.043183949366711646\n",
      "NLPD: 0.05069424220584877, MSE: 0.04329426116664594\n",
      "NLPD: 0.04994198238219346, MSE: 0.04317271678373475\n",
      "NLPD: 0.04980170773835184, MSE: 0.043161207915208895\n",
      "NLPD: 0.05030607306786485, MSE: 0.04321345650060692\n",
      "NLPD: 0.0495895929152526, MSE: 0.04314251896924139\n",
      "NLPD: 0.049644910725167954, MSE: 0.04313923895843848\n",
      "NLPD: 0.049448483591297485, MSE: 0.04313336580497807\n",
      "NLPD: 0.049478172765176597, MSE: 0.043112058624157054\n",
      "NLPD: 0.0493301151197954, MSE: 0.04310954569832563\n",
      "NLPD: 0.04919705685087176, MSE: 0.04311801032511569\n",
      "NLPD: 0.0491025396645117, MSE: 0.04310437221922505\n",
      "NLPD: 0.049053768072734844, MSE: 0.04307327541407738\n",
      "NLPD: 0.0488383414231247, MSE: 0.04307404912447103\n",
      "NLPD: 0.04885684068927448, MSE: 0.0430770028660202\n",
      "NLPD: 0.050166893720963224, MSE: 0.04329932203697269\n",
      "NLPD: 0.048657316703119444, MSE: 0.04306687990268525\n",
      "NLPD: 0.04843139631100036, MSE: 0.04305081977303509\n",
      "NLPD: 0.04867512886296802, MSE: 0.04308717266219342\n",
      "NLPD: 0.04832615943932344, MSE: 0.043049469643767574\n",
      "NLPD: 0.04820156319893892, MSE: 0.043032009533208584\n",
      "NLPD: 0.05063804300883504, MSE: 0.04327446560529876\n",
      "NLPD: 0.04795811174056786, MSE: 0.04301682477563177\n",
      "NLPD: 0.04792414888438263, MSE: 0.043010473332728796\n",
      "NLPD: 0.048878442317254626, MSE: 0.04307333005555119\n",
      "NLPD: 0.04782401306936459, MSE: 0.042984291661488955\n",
      "NLPD: 0.04769026623875979, MSE: 0.04299298812113848\n",
      "NLPD: 0.04761111796811302, MSE: 0.042983986990371655\n",
      "NLPD: 0.047706166553198995, MSE: 0.04301398346180916\n",
      "NLPD: 0.047567311781842564, MSE: 0.042968754441603474\n",
      "NLPD: 0.04738453497228356, MSE: 0.04296466119183015\n",
      "NLPD: 0.047230500178853704, MSE: 0.042957389257951284\n",
      "NLPD: 0.04713330351499807, MSE: 0.0429271124392289\n",
      "NLPD: 0.047203499536693116, MSE: 0.04293274611176924\n",
      "NLPD: 0.046971825774455744, MSE: 0.042937715008459346\n",
      "NLPD: 0.04706711102414515, MSE: 0.04294276496710702\n",
      "NLPD: 0.04692688610710094, MSE: 0.04292412971026203\n",
      "NLPD: 0.04676927981862236, MSE: 0.04291880547567642\n",
      "NLPD: 0.04671437442672127, MSE: 0.04291486250273645\n",
      "NLPD: 0.046737647357861005, MSE: 0.04295212550951266\n",
      "NLPD: 0.04647138403265819, MSE: 0.04289661223151188\n",
      "NLPD: 0.0464261405903217, MSE: 0.0428908614524343\n",
      "NLPD: 0.04690599082488049, MSE: 0.04291518593236943\n",
      "NLPD: 0.046308190205820665, MSE: 0.04287774087070105\n",
      "NLPD: 0.04614441328655332, MSE: 0.04287277043243005\n",
      "NLPD: 0.04771093830026246, MSE: 0.042999021481002646\n",
      "NLPD: 0.04600591463044057, MSE: 0.042862589976485024\n",
      "NLPD: 0.04596318404345417, MSE: 0.04285572472033748\n",
      "NLPD: 0.04636876420057871, MSE: 0.04293278714040672\n",
      "NLPD: 0.04584940648273929, MSE: 0.042844405732899105\n",
      "NLPD: 0.04577144954932528, MSE: 0.04283998776538539\n",
      "NLPD: 0.04623397647977436, MSE: 0.04284351562181786\n",
      "NLPD: 0.04568279970156117, MSE: 0.04282229231303617\n",
      "NLPD: 0.045449393468127756, MSE: 0.042820612685113016\n",
      "NLPD: 0.04550805518978193, MSE: 0.042823684755836394\n",
      "NLPD: 0.04549634902279303, MSE: 0.04281102777989127\n",
      "NLPD: 0.04524739930016589, MSE: 0.04280765048827286\n",
      "NLPD: 0.04522066457098094, MSE: 0.04279713920120017\n",
      "NLPD: 0.0461768770004872, MSE: 0.042812885503594224\n",
      "NLPD: 0.0452174254039667, MSE: 0.04278774818027397\n",
      "NLPD: 0.04499339807470331, MSE: 0.04277990790620858\n",
      "NLPD: 0.04670981669396157, MSE: 0.043153267836460854\n",
      "NLPD: 0.044843661980889224, MSE: 0.042764440233036195\n",
      "NLPD: 0.044787530647270514, MSE: 0.04276377732406051\n"
     ]
    }
   ],
   "source": [
    "model, projector = get_model_and_projector()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, maximize=True)\n",
    "elbo = gpytorch.mlls.VariationalELBO(model.likelihood, model, y.size(0))\n",
    "\n",
    "\n",
    "for step in (pbar := tqdm(range(10000), desc='Epochs')):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_batch, y_batch = projector(x, y)\n",
    "    output = model(x_batch)\n",
    "    loss = elbo(output, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'ELBO': loss.item()})\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_x_proj = projector(test_x)\n",
    "            out = model.likelihood(model(test_x_proj))\n",
    "            out = projector.inverse(out)\n",
    "            nlpd = negative_log_predictive_density(out, test_y)\n",
    "            mse = mean_squared_error(out, test_y)\n",
    "            print(f\"NLPD: {nlpd}, MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0545016590434723\n",
      "5.432272166524743\n"
     ]
    }
   ],
   "source": [
    "from mdgp.utils import test_psd\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_x_proj = projector(test_x)\n",
    "    out = model(test_x_proj)\n",
    "    out = projector.inverse(out)\n",
    "    test_psd(out.lazy_covariance_matrix)\n",
    "    mean = out.mean\n",
    "    print(mean_squared_error(out, test_y).item())\n",
    "    print(negative_log_predictive_density(out, test_y).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35842fb788b49cb896906f355e8cea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGP(x, y, likelihood)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, maximize=True)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "\n",
    "for step in (pbar := tqdm(range(10000), desc='Epochs')):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = mll(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'MLL': loss.item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0017146845907218108\n",
      "NLPD: -1.5748066209556104\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    out = model(test_x)\n",
    "    test_psd(out.lazy_covariance_matrix)\n",
    "    mean = out.mean\n",
    "    print(f\"MSE: {mean_squared_error(out, test_y).item()}\")\n",
    "    print(f\"NLPD: {negative_log_predictive_density(out, test_y).mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
