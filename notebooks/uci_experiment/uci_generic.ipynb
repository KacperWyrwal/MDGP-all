{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data\n",
    "Currently supported datasets: power, protein, kin8nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types \n",
    "from torch import Tensor \n",
    "\n",
    "# Backends \n",
    "import geometric_kernels.torch \n",
    "\n",
    "\n",
    "import os\n",
    "import torch \n",
    "import pandas as pd \n",
    "from math import comb \n",
    "from spherical_harmonics import SphericalHarmonics\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_harmonics_single(ell: int, d: int) -> int:\n",
    "    r\"\"\"\n",
    "    Number of spherical harmonics of degree ell on S^{d - 1}.\n",
    "    \"\"\"\n",
    "    if ell == 0:\n",
    "        return 1\n",
    "    if d == 3:\n",
    "        return 2 * ell + 1\n",
    "    else:\n",
    "        return (2 * ell + d - 2) * comb(ell + d - 3, ell - 1) // ell\n",
    "\n",
    "\n",
    "def num_harmonics(ell: Tensor | int, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized version of num_harmonics_single\n",
    "    \"\"\"\n",
    "    if isinstance(ell, int):\n",
    "        return num_harmonics_single(ell, d)\n",
    "    return ell.apply_(lambda e: num_harmonics_single(ell=e, d=d))\n",
    "\n",
    "\n",
    "def total_num_harmonics(max_ell: int, d: int) -> int:\n",
    "    \"\"\"\n",
    "    Total number of spherical harmonics on S^{d-1} with degree <= max_ell\n",
    "    \"\"\"\n",
    "    return int(sum(num_harmonics(ell=torch.arange(max_ell + 1), d=d)))\n",
    "\n",
    "\n",
    "def eigenvalue_laplacian(ell: Tensor, d: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Eigenvalue of the Laplace-Beltrami operator for a spherical harmonic of degree ell on S_{d-1}\n",
    "    ell: [...]\n",
    "    d: []\n",
    "    return: [...]\n",
    "    \"\"\"\n",
    "    return ell * (ell + d - 2)\n",
    "\n",
    "\n",
    "def unnormalized_matern_spectral_density(n: Tensor, d: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor | float: \n",
    "    \"\"\"\n",
    "    compute (unnormalized) spectral density of the matern kernel on S_{d-1}\n",
    "    n: [N]\n",
    "    d: []\n",
    "    kappa: [O, 1, 1]\n",
    "    nu: [O, 1, 1]\n",
    "    return: [O, 1, N]\n",
    "    \"\"\"\n",
    "    # Squared exponential kernel \n",
    "    if torch.all(nu.isinf()):\n",
    "        exponent = -kappa ** 2 / 2 * eigenvalue_laplacian(ell=n, d=d) # [O, N, 1]\n",
    "        return torch.exp(exponent)\n",
    "    # Matern kernel\n",
    "    else:\n",
    "        base = (\n",
    "            2.0 * nu / kappa**2 + # [O, 1, 1]\n",
    "            eigenvalue_laplacian(ell=n, d=d).unsqueeze(-1) # [N, 1]\n",
    "        ) # [O, N, 1]\n",
    "        exponent = -nu - (d - 1) / 2.0 # [O, 1, 1]\n",
    "        return base ** exponent # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density_normalizer(d: int, max_ell: int, kappa: Tensor | float, nu: Tensor | float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalizing constant for the spectral density of the Matern kernel on S^{d-1}. \n",
    "    Depends on kappa and nu. Also depends on max_ell, as truncation of the infinite \n",
    "    sum from Karhunen-Loeve decomposition. \n",
    "    \"\"\"\n",
    "    n = torch.arange(max_ell + 1)\n",
    "    spectral_values = unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) # [O, max_ell + 1, 1]\n",
    "    num_harmonics_per_level = num_harmonics(torch.arange(max_ell + 1), d=d).type(spectral_values.dtype) # [max_ell + 1]\n",
    "    normalizer = spectral_values.mT @ num_harmonics_per_level # [O, 1, max_ell + 1] @ [max_ell + 1] -> [O, 1]\n",
    "    return normalizer.unsqueeze(-2) # [O, 1, 1]\n",
    "\n",
    "\n",
    "def matern_spectral_density(n: Tensor, d: int, kappa: Tensor, nu: Tensor, max_ell: int, sigma: float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Spectral density of the Matern kernel on S^{d-1}\n",
    "    \"\"\"\n",
    "    return (\n",
    "        unnormalized_matern_spectral_density(n=n, d=d, kappa=kappa, nu=nu) / # [O, N, 1]\n",
    "        matern_spectral_density_normalizer(d=d, max_ell=max_ell, kappa=kappa, nu=nu) * # [O, 1, 1]\n",
    "        (sigma ** 2)[..., *(None,) * (kappa.ndim - 1)] # [O, 1, 1]\n",
    "    ) # [O, N, 1] / [O, 1, 1] * [O, 1, 1] -> [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_ahat(ell: Tensor, d: int, max_ell: int, kappa: Tensor | float, nu: Tensor | float, \n",
    "                m: int | None = None, sigma: Tensor | float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    :math: `\\hat{a} = \\rho(\\ell)` where :math: `\\rho` is the spectral density on S^{d-1}\n",
    "    \"\"\"\n",
    "    return matern_spectral_density(n=ell, d=d, kappa=kappa, nu=nu, max_ell=max_ell, sigma=sigma) # [O, N, 1]\n",
    "\n",
    "\n",
    "def matern_repeated_ahat(max_ell: int, d: int, kappa: Tensor | float, nu: Tensor | float, sigma: Tensor | float = 1.0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of repeated ahat values for each ell. \n",
    "    \"\"\"\n",
    "    ells = torch.arange(max_ell + 1) # [max_ell + 1]\n",
    "    ahat = matern_ahat(ell=ells, d=d, max_ell=max_ell, kappa=kappa, nu=nu, sigma=sigma) # [O, max_ell + 1, 1]\n",
    "    repeats = num_harmonics(ell=ells, d=d) # [max_ell + 1]\n",
    "    return torch.repeat_interleave(ahat, repeats=repeats, dim=-2) # [O, num_harmonics, 1]\n",
    "\n",
    "\n",
    "def matern_Kuu(max_ell: int, d: int, kappa: float, nu: float, sigma: float = 1.0) -> Tensor: \n",
    "    \"\"\"\n",
    "    Returns the covariance matrix, which is a diagonal matrix with entries \n",
    "    equal to inv_ahat of the corresponding ell. \n",
    "    \"\"\"\n",
    "    return torch.diag(1 / matern_repeated_ahat(max_ell, d, kappa, nu, sigma=sigma).squeeze(-1)) # [O, num_harmonics, num_harmonics]\n",
    "\n",
    "\n",
    "def spherical_harmonics(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    # Make sure that x is at least 2d and flatten it\n",
    "    x = torch.atleast_2d(x)\n",
    "    batch_shape, n = x.shape[:-2], x.shape[-2]\n",
    "    x = x.flatten(0, -2)\n",
    "\n",
    "    # Get spherical harmonics callable\n",
    "    f = SphericalHarmonics(dimension=d, degrees=max_ell + 1) # [... * O, N, num_harmonics]\n",
    "\n",
    "    # Evaluate x and reintroduce batch dimensions\n",
    "    return f(x).reshape(*batch_shape, n, total_num_harmonics(max_ell, d)) # [..., O, N, num_harmonics]\n",
    "\n",
    "\n",
    "def matern_Kux(x: Tensor, max_ell: int, d: int) -> Tensor: \n",
    "    return spherical_harmonics(x, max_ell=max_ell, d=d).mT # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def num_spherical_harmonics_to_degree(num_spherical_harmonics: int, dimension: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the minimum degree for which there are at least\n",
    "    `num_eigenfunctions` in the collection.\n",
    "    \"\"\"\n",
    "    n, degree = 0, 0  # n: number of harmonics, d: degree (or level)\n",
    "    while n < num_spherical_harmonics:\n",
    "        n += num_harmonics(d=dimension, ell=degree)\n",
    "        degree += 1\n",
    "\n",
    "    if n > num_spherical_harmonics:\n",
    "        print(\n",
    "            \"The number of spherical harmonics requested does not lead to complete \"\n",
    "            \"levels of spherical harmonics. We have thus increased the number to \"\n",
    "            f\"{n}, which includes all spherical harmonics up to degree {degree} (incl.)\"\n",
    "        )\n",
    "    return degree - 1, n\n",
    "\n",
    "\n",
    "from gpytorch.kernels import Kernel, ScaleKernel\n",
    "\n",
    "\n",
    "def matern_LT_Phi(x: Tensor, max_ell: int, d: int, kappa: float, nu: float, sigma: float = 1.0) -> Tensor: \n",
    "    Kux = matern_Kux(x, max_ell=max_ell, d=d) # [..., O, num_harmonics, N]\n",
    "    ahat_sqrt = matern_repeated_ahat(max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma).sqrt() # [O, num_harmonics, 1]\n",
    "    return Kux * ahat_sqrt # [..., O, num_harmonics, N]\n",
    "\n",
    "\n",
    "def matern_LT_Phi_from_kernel(x: Tensor, covar_module: Kernel, num_levels: int) -> Tensor: \n",
    "    # Extract kernel parameters  \n",
    "    if isinstance(covar_module, ScaleKernel):\n",
    "        sigma = covar_module.outputscale.sqrt()\n",
    "        base_kernel = covar_module.base_kernel\n",
    "    else:\n",
    "        sigma = torch.tensor(1.0, dtype=x.dtype, device=x.device)\n",
    "        base_kernel = covar_module\n",
    "    kappa = base_kernel.lengthscale\n",
    "    nu = base_kernel.nu \n",
    "\n",
    "    # Extract constants \n",
    "    d = base_kernel.space.dimension + 1\n",
    "    max_ell = num_levels\n",
    "    \n",
    "    return matern_LT_Phi(x, max_ell=max_ell, d=d, kappa=kappa, nu=nu, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCIDataset:\n",
    "\n",
    "    UCI_BASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "    def __init__(self, name: str, url: str, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None): \n",
    "        self.name = name \n",
    "        self.url = url\n",
    "        self.path = path \n",
    "        self.csv_path = os.path.join(self.path, self.name + '.csv')\n",
    "\n",
    "        # Set generator if seed is provided.\n",
    "        self.generator = torch.Generator()\n",
    "        if seed is not None: \n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "        # Load, shuffle, split, and normalize data. TODO except for load, these don't need to be object methods. \n",
    "        # We keep the standard deviation of the test set for log-likelihood evaluation.\n",
    "        x, y = self.load_data()\n",
    "        x, y = self.shuffle(x, y, generator=self.generator)     \n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y)\n",
    "        self.test_y_std = self.test_y.std(dim=0, keepdim=True)\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = map(\n",
    "            self.normalize, (self.train_x, self.train_y, self.test_x, self.test_y))\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        return self.train_x.shape[-1]\n",
    "\n",
    "    @property \n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.train_x, self.train_y)\n",
    "    \n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return TensorDataset(self.test_x, self.test_y)\n",
    "\n",
    "    def read_data(self) -> tuple[Tensor, Tensor]:\n",
    "        xy = torch.from_numpy(pd.read_csv(self.csv_path).values)\n",
    "        return xy[:, :-1], xy[:, -1]\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        NotImplementedError\n",
    "\n",
    "    def load_data(self, overwrite: bool = False) -> tuple[Tensor, Tensor]:\n",
    "        if overwrite or not os.path.isfile(self.csv_path):\n",
    "            self.download_data()\n",
    "        return self.read_data()\n",
    "\n",
    "    def normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - x.mean(dim=0)) / x.std(dim=0, keepdim=True)\n",
    "    \n",
    "    def shuffle(self, x: Tensor, y: Tensor, generator: torch.Generator) -> tuple[Tensor, Tensor]:\n",
    "        perm_idx = torch.randperm(x.size(0), generator=generator)\n",
    "        return x[perm_idx], y[perm_idx]\n",
    "    \n",
    "    def split(self, x: Tensor, y: Tensor, test_size: float = 0.1) -> tuple[Tensor, Tensor, Tensor, Tensor]: \n",
    "        \"\"\"\n",
    "        Split the dataset into train and test sets.\n",
    "        \"\"\"\n",
    "        split_idx = int(test_size * x.size(0))\n",
    "        return x[split_idx:], y[split_idx:], x[:split_idx], y[:split_idx]\n",
    "\n",
    "\n",
    "class Kin8mn(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = 'https://raw.githubusercontent.com/liusiyan/UQnet/master/datasets/UCI_datasets/kin8nm/dataset_2175_kin8nm.csv'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Kin8mn.DEFAULT_URL\n",
    "        super().__init__(name='kin8nm', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self) -> None:\n",
    "        df = pd.read_csv(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "class Power(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = UCIDataset.UCI_BASE_URL + \"00294/CCPP.zip\"\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Power.DEFAULT_URL\n",
    "        super().__init__(name='power', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        with urlopen(self.url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall('/tmp/')\n",
    "\n",
    "        df = pd.read_excel('/tmp/CCPP//Folds5x2_pp.xlsx')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "class Concrete(UCIDataset):\n",
    "\n",
    "    DEFAULT_URL = UCIDataset.UCI_BASE_URL + 'concrete/compressive/Concrete_Data.xls'\n",
    "\n",
    "    def __init__(self, path: str = '../../data/uci/', normalize: bool = True, seed: int | None = None, url: str | None = None):\n",
    "        url = url or Concrete.DEFAULT_URL\n",
    "        super().__init__(name='concrete', path=path, normalize=normalize, url=url, seed=seed)\n",
    "\n",
    "    def download_data(self):\n",
    "        df = pd.read_excel(self.url)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "\n",
    "default_lr = {\n",
    "    'kin8nm': 0.01,\n",
    "    'power': 0.01,\n",
    "    'concrete': 0.01,\n",
    "}\n",
    "\n",
    "default_num_epochs = {\n",
    "    'kin8nm': 20,\n",
    "    'power': 20,\n",
    "    'concrete': 125,\n",
    "}\n",
    "\n",
    "dimension_to_prior_num_eigenfunctions = {\n",
    "    4: 336,\n",
    "    6: 294,\n",
    "    8: 210, \n",
    "}\n",
    "\n",
    "dimension_to_num_inducing = {\n",
    "    4: 336,\n",
    "    6: 294,\n",
    "    8: 210, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model\n",
    "\n",
    "This is done using the same model arguments as for the benchmarking experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator\n",
    "from abc import ABC, abstractmethod\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class Projector(torch.nn.Module, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdentityProjector(Projector):\n",
    "\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor]:\n",
    "        if y is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x, y\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        return mvn\n",
    "\n",
    "\n",
    "class ConstantProjector(Projector):\n",
    "    def __init__(self, b: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "        self.norm = None \n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor | None = None) -> tuple[Tensor, Tensor] | Tensor:\n",
    "        b = self.b.expand(*x.shape[:-1], 1)\n",
    "        x_cat_b = torch.cat([x, b], dim=-1)\n",
    "        self.norm = x_cat_b.norm(dim=-1, keepdim=True)\n",
    "        if y is None:\n",
    "            return x_cat_b / self.norm\n",
    "        else:\n",
    "            return x_cat_b / self.norm, y / self.norm.squeeze(-1)\n",
    "    \n",
    "    def inverse(self, mvn: MultivariateNormal) -> MultivariateNormal:\n",
    "        L = DiagLinearOperator(self.norm.squeeze(-1))\n",
    "        mean = mvn.mean @ L\n",
    "        cov = L @ mvn.lazy_covariance_matrix @ L\n",
    "        return MultivariateNormal(mean=mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch \n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "\n",
    "\n",
    "class SphericalHarmonicsVariationalStrategy(gpytorch.Module):\n",
    "    def __init__(self, model: gpytorch.Module, variational_distribution: CholeskyVariationalDistribution, num_levels, jitter_val: float | None = None):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"_model\", model)\n",
    "        self._variational_distribution = variational_distribution\n",
    "        self.jitter_val = jitter_val or gpytorch.settings.cholesky_jitter.value(variational_distribution.dtype)\n",
    "        self.num_levels = num_levels\n",
    "\n",
    "    @property\n",
    "    def model(self) -> gpytorch.Module:\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def variational_distribution(self) -> MultivariateNormal:\n",
    "        return self._variational_distribution()\n",
    "    \n",
    "    @property\n",
    "    def prior_distribution(self) -> MultivariateNormal:\n",
    "        zeros = torch.zeros(\n",
    "            self._variational_distribution.shape(),\n",
    "            dtype=self._variational_distribution.dtype,\n",
    "            device=self._variational_distribution.device,\n",
    "        )\n",
    "        ones = torch.ones_like(zeros)\n",
    "        res = MultivariateNormal(zeros, DiagLinearOperator(ones))\n",
    "        return res\n",
    "\n",
    "    def forward(self, x) -> MultivariateNormal:\n",
    "        # prior at x \n",
    "        px = self.model.forward(x)\n",
    "        mu_x, Kxx = px.mean, px.lazy_covariance_matrix\n",
    "\n",
    "        # whitened prior at u\n",
    "        Linv_pu = self.prior_distribution\n",
    "        Linv_mu, Linv_Kuu_LTinv = Linv_pu.mean, Linv_pu.lazy_covariance_matrix\n",
    "\n",
    "        # whitened variational posterior at u\n",
    "        Linv_qu = self.variational_distribution\n",
    "        Linv_m, Linv_S_LTinv = Linv_qu.mean, Linv_qu.lazy_covariance_matrix\n",
    "\n",
    "        # unwhitening + projection and vice-versa\n",
    "        LT_Phiux = matern_LT_Phi_from_kernel(x, self.model.covar_module, num_levels=self.num_levels)\n",
    "        Phixu_L = LT_Phiux.mT\n",
    "\n",
    "        # posterior at x \n",
    "        qx_sigma = Kxx + Phixu_L @ (Linv_S_LTinv - Linv_Kuu_LTinv) @ LT_Phiux\n",
    "        qx_sigma = qx_sigma.add_jitter(self.jitter_val)\n",
    "        qx_mu = mu_x + Phixu_L @ (Linv_m - Linv_mu)\n",
    "        return MultivariateNormal(qx_mu, qx_sigma)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        with gpytorch.settings.max_preconditioner_size(0):\n",
    "            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n",
    "        return kl_divergence\n",
    "    \n",
    "    def __call__(self, x, prior: bool = False, **kwargs) -> MultivariateNormal:\n",
    "        if prior:\n",
    "            return self.model.forward(x)\n",
    "        return super().__call__(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "from geometric_kernels.spaces import Space\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from geometric_kernels.spaces import Hypersphere\n",
    "from mdgp.utils.spherical_harmonic_features import num_spherical_harmonics_to_degree\n",
    "from mdgp.variational.inducing_points import initialize_kmeans\n",
    "\n",
    "\n",
    "\n",
    "class VariationalStrategyFactory:\n",
    "    def __init__(self, name: str, num_inducing: int, inputs: Tensor | None = None, learn_inducing_locations: bool = False) -> None: \n",
    "        self.name = name \n",
    "        self.learn_inducing_locations = learn_inducing_locations\n",
    "        self.num_inducing_variables = num_inducing\n",
    "\n",
    "        if name == 'points': \n",
    "            assert inputs is not None, 'Must provide inputs to use points variational strategy.'\n",
    "        self.inputs = inputs \n",
    "\n",
    "    @staticmethod\n",
    "    def make_variational_strategy(\n",
    "        name: str, model: ApproximateGP, space: Space, num_inducing_variables: int, inputs: Tensor, \n",
    "        learn_inducing_locations: bool = False, batch_shape: torch.Size = torch.Size([]), jitter_val: float | None = None\n",
    "    ) -> VariationalStrategy | SphericalHarmonicsVariationalStrategy: \n",
    "        if name == 'points': \n",
    "            inducing_points = initialize_kmeans(x=inputs, n=num_inducing_variables, space=space)\n",
    "            variational_distribution = CholeskyVariationalDistribution(\n",
    "                num_inducing_points=num_inducing_variables,\n",
    "                batch_shape=batch_shape,\n",
    "            )\n",
    "            variational_strategy = VariationalStrategy(\n",
    "                model=model, inducing_points=inducing_points, \n",
    "                variational_distribution=variational_distribution, \n",
    "                learn_inducing_locations=learn_inducing_locations,\n",
    "            )\n",
    "        elif name == 'harmonics': \n",
    "            assert isinstance(space, Hypersphere), f'Harmonic features only implemented for hyperspheres, not {space}.'\n",
    "            dimension = space.dim + 1\n",
    "            degree, num_spherical_harmonics = num_spherical_harmonics_to_degree(\n",
    "                num_spherical_harmonics=num_inducing_variables, dimension=dimension\n",
    "            )\n",
    "            variational_distribution = CholeskyVariationalDistribution(\n",
    "                num_inducing_points=num_spherical_harmonics,\n",
    "                batch_shape=batch_shape,\n",
    "            )\n",
    "            variational_strategy = SphericalHarmonicsVariationalStrategy(\n",
    "                model=model, \n",
    "                variational_distribution=variational_distribution, \n",
    "                num_levels=degree,\n",
    "                jitter_val=jitter_val,\n",
    "            )\n",
    "        else: \n",
    "            raise ValueError(f'Variational strategy {name} not recognized. Must be one of [\"points\", \"harmonics\"].')\n",
    "        \n",
    "        return variational_strategy\n",
    "    \n",
    "    def __call__(self, model: ApproximateGP, space: Space, batch_shape: torch.Size = torch.Size([])) -> VariationalStrategy:\n",
    "        return self.make_variational_strategy(\n",
    "            name=self.name, model=model, space=space, num_inducing_variables=self.num_inducing_variables, \n",
    "            inputs=self.inputs, learn_inducing_locations=self.learn_inducing_locations, batch_shape=batch_shape,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from dataclasses import dataclass, field\n",
    "from mdgp.models import *\n",
    "from geometric_kernels.spaces import Hypersphere, Euclidean\n",
    "from gpytorch.priors import GammaPrior\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    dataset: UCIDataset\n",
    "\n",
    "    # deep model specs \n",
    "    model_name: str = field(default='residual_geometric', \n",
    "                            metadata={'help': 'Name of the model. Must be one of [\"residual_geometric\", \"euclidean\", \"geometric_head\"]'})\n",
    "    num_hidden: int = field(default=1, metadata={'help': 'Number of hidden layers'})\n",
    "    hidden_dims: int | None = field(default=None, metadata={'help': 'Number of output dimensions of the hidden layers.'})\n",
    "    output_dims: int | None = field(default=None, metadata={'help': 'Number of output dimensions of the final layer.'})\n",
    "    to_tangent: str = field(default='project', init=False, repr=False)\n",
    "\n",
    "    # kernel specs \n",
    "    nu: float = field(default=1.5, metadata={'help': 'Smoothness parameter'})\n",
    "    optimize_nu: bool = field(default=True, metadata={'help': 'Whether to optimize the smoothness parameter'})\n",
    "    lengthscale: float = field(default=1.0, metadata={'help': 'Lengthscale of the kernel'})\n",
    "    outputscale_mean: float = field(default=1.0, metadata={'help': 'Mean of the outputscale'})\n",
    "    prior_num_eigenfunctions: int | None = field(default=None)\n",
    "\n",
    "    # variational specs \n",
    "    variational_strategy_name: str = field(default='points', metadata={'help': 'Name of the variational strategy. Must be one of [\"points\", \"harmonics\"]'})\n",
    "    learn_inducing_locations: bool = field(default=False, metadata={'help': 'Whether to learn the inducing locations'})\n",
    "    num_inducing: int | None = field(default=None)\n",
    "\n",
    "    # sampler specs\n",
    "    sampler_inv_jitter: None = field(default=None, init=False, repr=False)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.model_name in ['residual_geometric', 'residual_euclidean', 'euclidean', 'geometric_head', 'exact']\n",
    "        assert self.variational_strategy_name in ['points', 'harmonics']\n",
    "\n",
    "        if self.variational_strategy_name == 'harmonics': \n",
    "            assert self.model_name in ['residual_geometric', 'residual_euclidean', 'geometric_head', 'exact']\n",
    "\n",
    "        # If number of eigenfunctions is None, then use the default for the dataset dimension\n",
    "        self.num_inducing = self.num_inducing or dimension_to_num_inducing[self.dataset.dimension]\n",
    "        self.prior_num_eigenfunctions = self.prior_num_eigenfunctions or dimension_to_prior_num_eigenfunctions[self.dataset.dimension]\n",
    "\n",
    "        # Set the space for the model\n",
    "        if self.model_name in ['residual_geometric', 'geometric_head']:\n",
    "            self.space = Hypersphere(dim=self.dataset.dimension)\n",
    "        if self.model_name in ['euclidean']:\n",
    "            self.space = Euclidean(dim=self.dataset.dimension)\n",
    "    \n",
    "    @property\n",
    "    def outputscale_prior(self) -> GammaPrior:\n",
    "        return GammaPrior(concentration=1.0, rate=1 / self.outputscale_mean)\n",
    "    \n",
    "\n",
    "def get_projector(model_args: ModelArguments) -> Projector:\n",
    "    if model_args.model_name in ['residual_geometric', 'geometric_head']:\n",
    "        return ConstantProjector()\n",
    "    else:\n",
    "        return IdentityProjector()\n",
    "\n",
    "\n",
    "def create_model(model_args: ModelArguments, dataset: UCIDataset, projector: Projector | None = None):\n",
    "    inputs = dataset.train_x\n",
    "    degree = num_spherical_harmonics_to_degree(model_args.prior_num_eigenfunctions, model_args.space.dim + 1)[0] + 1\n",
    "    if projector is not None:\n",
    "        inputs = projector(inputs)\n",
    "    variational_strategy_factory = VariationalStrategyFactory(\n",
    "        name=model_args.variational_strategy_name, \n",
    "        num_inducing=model_args.num_inducing,\n",
    "        learn_inducing_locations=model_args.learn_inducing_locations,\n",
    "        inputs=inputs,\n",
    "    )\n",
    "    if model_args.model_name == 'residual_geometric':\n",
    "        return ResidualGeometricDeepGP(\n",
    "            space=model_args.space, \n",
    "            num_hidden=model_args.num_hidden,\n",
    "            variational_strategy_factory=variational_strategy_factory,\n",
    "            output_dims=model_args.output_dims,\n",
    "            to_tangent=model_args.to_tangent,\n",
    "            nu=model_args.nu,\n",
    "            optimize_nu=model_args.optimize_nu,\n",
    "            outputscale_prior=model_args.outputscale_prior,\n",
    "            num_eigenfunctions=degree,\n",
    "            sampler_inv_jitter=model_args.sampler_inv_jitter,\n",
    "        )\n",
    "    if model_args.model_name == 'geometric_head':\n",
    "        return GeometricHeadDeepGP(\n",
    "            space=model_args.space, \n",
    "            num_hidden=model_args.num_hidden,\n",
    "            variational_strategy_factory=variational_strategy_factory,\n",
    "            output_dims=model_args.output_dims,\n",
    "            nu=model_args.nu,\n",
    "            optimize_nu=model_args.optimize_nu,\n",
    "            outputscale_prior=model_args.outputscale_prior,\n",
    "            num_eigenfunctions=degree,\n",
    "            sampler_inv_jitter=model_args.sampler_inv_jitter,\n",
    "        )\n",
    "    if model_args.model_name == 'euclidean': \n",
    "        return EuclideanDeepGP(\n",
    "            space=model_args.space,\n",
    "            num_hidden=model_args.num_hidden,\n",
    "            variational_strategy_factory=variational_strategy_factory,\n",
    "            output_dims=model_args.output_dims,\n",
    "            nu=model_args.nu,\n",
    "            outputscale_prior=model_args.outputscale_prior,\n",
    "            num_eigenfunctions=degree,\n",
    "            sampler_inv_jitter=model_args.sampler_inv_jitter,\n",
    "        )\n",
    "    raise ValueError((\n",
    "        f'Unknown model name: {model_args.model_name}.'\n",
    "        f'Must be one of [\"residual_geometric\", \"euclidean\", \"geometric_head\"]'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Training has to be done differently from the bechmarking experiment, because we need minibatch SGD with the larger datasets and minibatch metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import LinearOperator\n",
    "\n",
    "\n",
    "def test_psd(S: LinearOperator | Tensor, tol=1e-8):\n",
    "    if isinstance(S, LinearOperator):\n",
    "        S = S.to_dense()\n",
    "\n",
    "    if S.ndim > 2:\n",
    "        S = S.flatten(end_dim=-3)\n",
    "        for S_row in S:\n",
    "            test_psd(S_row, tol=tol)\n",
    "        return\n",
    "\n",
    "    assert torch.allclose(S, S.mT), \"K should be symmetric.\"\n",
    "\n",
    "    eigs = torch.linalg.eigvalsh(S)\n",
    "    assert (eigs > -tol).all(), f\"K should be positive definite. K has shape {S.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch import settings \n",
    "from torch import no_grad \n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.autonotebook import tqdm \n",
    "from gpytorch.metrics import mean_squared_error\n",
    "from mdgp.experiments.experiment_utils.logging import log \n",
    "from gpytorch.mlls import VariationalELBO, DeepApproximateMLL\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FitArguments: \n",
    "    num_epochs: int = field(default=20, metadata={'help': 'Number of epochs to train for'})\n",
    "    sample_hidden: str = field(default='elementwise', init=False, repr=False)\n",
    "    batch_size: int = field(default=64, metadata={'help': 'Batch size'})\n",
    "    lr: float = field(default=0.01, init=False, repr=False)\n",
    "    test_num_samples: int = field(default=100, metadata={'help': 'Number of likelihood samples for test set evaluation'})\n",
    "    train_num_samples: int = field(default=3, metadata={'help': 'Number of likelihood samples used when training deep models.'})\n",
    "    optimize_projector: bool = field(default=False, metadata={'help': 'Whether to optimize the projection bias'})\n",
    "\n",
    "\n",
    "def get_mll(model, dataset: UCIDataset): \n",
    "    return DeepApproximateMLL(VariationalELBO(model.likelihood, model, num_data=dataset.train_y.size(0)))\n",
    "\n",
    "\n",
    "def get_optimizer(model, projector: Projector, fit_args: FitArguments):\n",
    "    params = [\n",
    "        {'params': model.parameters()},\n",
    "    ]\n",
    "    if fit_args.optimize_projector is True: \n",
    "        params.append({'params': projector.parameters()})\n",
    "    return torch.optim.Adam(params, lr=fit_args.lr, maximize=True)\n",
    "\n",
    "\n",
    "def train_step(x, y, model, projector, elbo, optimizer, sample_hidden):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x, y = projector(x, y)\n",
    "    y_hat = model(x, sample_hidden=sample_hidden)\n",
    "\n",
    "    test_psd(y_hat.lazy_covariance_matrix)\n",
    "\n",
    "    loss = elbo(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_epoch(model, projector: Projector, dataloader: DataLoader, elbo, optimizer, sample_hidden, num_samples): \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    with settings.num_likelihood_samples(num_samples):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            epoch_loss += train_step(batch_x, batch_y, model, projector, elbo, optimizer, sample_hidden)\n",
    "    return epoch_loss \n",
    "\n",
    "\n",
    "from gpytorch.metrics import negative_log_predictive_density\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def test_log_likelihood(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    mean, stddev = outputs.mean, outputs.stddev\n",
    "    logpdf = torch.distributions.Normal(loc=mean, scale=stddev).log_prob(targets) - torch.log(y_std)\n",
    "    # average over likelihood samples \n",
    "    logpdf = logsumexp(logpdf.numpy(), axis=0, b=1 / mean.size(0))\n",
    "    # average over data points\n",
    "    return torch.mean(torch.from_numpy(logpdf))\n",
    "\n",
    "\n",
    "def mean_squared_error(outputs: MultivariateNormal, targets: Tensor, y_std: Tensor) -> Tensor:\n",
    "    # TODO add handling for multiple samples \n",
    "    mean = outputs.mean.mean(0)\n",
    "    return y_std ** 2 * ((mean - targets) ** 2).mean(0)\n",
    "\n",
    "\n",
    "def test(model, projector: Projector, uci_dataset: UCIDataset, sample_hidden='elementwise', fit_args: FitArguments = None):\n",
    "    with no_grad(), settings.num_likelihood_samples(fit_args.test_num_samples):\n",
    "        total_mse = 0.0\n",
    "        total_tll = 0.0\n",
    "        total_nlpd = 0.0\n",
    "        model.eval() \n",
    "        dataloader = DataLoader(uci_dataset.test_dataset, batch_size=fit_args.batch_size)\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # When testing we rescale, or \"invert\", the predictive distribution\n",
    "            batch_x = projector(batch_x)\n",
    "            batch_y_hat = model.likelihood(model(batch_x, sample_hidden=sample_hidden))\n",
    "            batch_y_hat = projector.inverse(batch_y_hat)\n",
    "            total_mse += mean_squared_error(batch_y_hat, batch_y, y_std=uci_dataset.test_y_std).item()\n",
    "            total_tll += test_log_likelihood(batch_y_hat, batch_y, y_std=uci_dataset.test_y_std).item()\n",
    "            total_nlpd += negative_log_predictive_density(batch_y_hat, batch_y).mean(0).item()\n",
    "        return {\n",
    "            'mse': total_mse / len(dataloader),\n",
    "            'rmse': (total_mse / len(dataloader)) ** 0.5,\n",
    "            'tll': total_tll / len(dataloader),\n",
    "            'nlpd': total_nlpd / len(dataloader),\n",
    "        }\n",
    "    \n",
    "\n",
    "def fit(model, projector, optimizer, elbo, uci_dataset: UCIDataset, train_loggers=None, fit_args: FitArguments = None): \n",
    "    metrics = {'elbo': None}\n",
    "    pbar = tqdm(range(1, fit_args.num_epochs + 1), desc=\"Fitting\")\n",
    "    for step in pbar:\n",
    "        metrics['elbo'] = train_epoch(\n",
    "            model=model, \n",
    "            projector=projector,\n",
    "            dataloader=DataLoader(uci_dataset.train_dataset, batch_size=fit_args.batch_size, shuffle=True),\n",
    "            elbo=elbo,\n",
    "            optimizer=optimizer,\n",
    "            sample_hidden=fit_args.sample_hidden,\n",
    "            num_samples=fit_args.train_num_samples,\n",
    "        )\n",
    "        # Update, log, and display metrics \n",
    "        log(train_loggers, metrics=metrics, step=step)\n",
    "        pbar.set_postfix(metrics)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Euclidean deep GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Kin8mn(normalize=False)\n",
    "model_args = ModelArguments(\n",
    "    dataset=dataset,\n",
    "    model_name='euclidean',\n",
    "    optimize_nu=False,\n",
    "    learn_inducing_locations=False, \n",
    "    num_hidden=0, \n",
    "    variational_strategy_name='points',\n",
    "    outputscale_mean=1.,\n",
    "    num_inducing=100,\n",
    "    nu=torch.inf,\n",
    ")\n",
    "fit_args = FitArguments(\n",
    "    num_epochs=50,\n",
    "    batch_size=512,\n",
    "    test_num_samples=1,\n",
    "    train_num_samples=1,\n",
    "    optimize_projector=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/scipy/cluster/vq.py:602: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n",
      "Fitting: 100%|██████████| 50/50 [00:20<00:00,  2.43it/s, elbo=-13.8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.021189513122918436,\n",
       " 'rmse': 0.14556618124728846,\n",
       " 'tll': 0.5019268527251693,\n",
       " 'nlpd': 0.810272167794537}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector = get_projector(model_args)\n",
    "model = create_model(model_args, dataset, projector)\n",
    "elbo = get_mll(model, dataset)\n",
    "optimizer = get_optimizer(model, projector, fit_args)\n",
    "\n",
    "fit(model, projector, optimizer, elbo, dataset, fit_args=fit_args)\n",
    "test(model, projector, dataset, fit_args=fit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test shallow GP with spherical harmonic variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Kin8mn()\n",
    "model_args = ModelArguments(\n",
    "    dataset=dataset,\n",
    "    model_name='residual_geometric',\n",
    "    optimize_nu=False,\n",
    "    learn_inducing_locations=False, \n",
    "    num_hidden=0, \n",
    "    variational_strategy_name='harmonics',\n",
    "    outputscale_mean=1.,\n",
    ")\n",
    "fit_args = FitArguments(\n",
    "    num_epochs=20,\n",
    "    batch_size=256,\n",
    "    test_num_samples=1,\n",
    "    train_num_samples=1,\n",
    "    optimize_projector=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = get_projector(model_args)\n",
    "model = create_model(model_args, dataset, projector)\n",
    "elbo = get_mll(model, dataset)\n",
    "optimizer = get_optimizer(model, projector, fit_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting: 100%|██████████| 20/20 [01:36<00:00,  4.80s/it, elbo=15.1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.014120858337625566,\n",
       " 'rmse': 0.11883121785804253,\n",
       " 'tll': 0.731592262108463,\n",
       " 'nlpd': 0.6019806334492757}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, projector, optimizer, elbo, dataset, fit_args=fit_args)\n",
    "test(model, projector, dataset, fit_args=fit_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, projector, optimizer, mll) -> float:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x, y = projector(x, y)\n",
    "    output = model(x)\n",
    "    test_psd(output.lazy_covariance_matrix)\n",
    "    loss = mll(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train(dataset, model, projector, num_epochs=20, lr=0.01) -> list[float]: \n",
    "    # optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, maximize=True)\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, dataset.train_y.size(0))\n",
    "\n",
    "    # data\n",
    "    train_loader = DataLoader(dataset.train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            loss = train_step(x=x_batch, y=y_batch, model=model, projector=projector, optimizer=optimizer, mll=mll)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        pbar.set_postfix({'ELBO': losses[-1]})\n",
    "\n",
    "    return losses \n",
    "\n",
    "\n",
    "def evaluate(dataset, model, projector):\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = projector(dataset.test_x), dataset.test_y\n",
    "        out = model.likelihood(model(test_x))\n",
    "        out = projector.inverse(out)\n",
    "        nlpd = negative_log_predictive_density(out, test_y)\n",
    "        mse = mean_squared_error(out, test_y)\n",
    "        metrics = {\n",
    "            'nlpd': nlpd.item(), \n",
    "            'mse': mse.item(),\n",
    "        }\n",
    "        print(f\"NLPD: {metrics['nlpd']}, MSE: {metrics['mse']}\")\n",
    "    return metrics \n",
    "\n",
    "\n",
    "def get_model_and_projector(dataset: UCIDataset):\n",
    "    sphere_dimension = dataset.dimension + 1\n",
    "\n",
    "    # number of levels for variational inference \n",
    "    num_spherical_harmonics = dimension_to_num_inducing[dataset.dimension]\n",
    "    max_ell, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics, sphere_dimension)\n",
    "\n",
    "    # number of levels for prior\n",
    "    num_spherical_harmonics_prior = dimension_to_prior_num_eigenfunctions[dataset.dimension]\n",
    "    max_ell_prior, _ = num_spherical_harmonics_to_degree(num_spherical_harmonics_prior, sphere_dimension)\n",
    "\n",
    "    model = gpytorchSGP(max_ell=max_ell, d=sphere_dimension, max_ell_prior=max_ell_prior, kappa=1.0, nu=1.5, optimize_nu=False, batch_shape=torch.Size([]))\n",
    "    projector = ConstantProjector()\n",
    "    return model, projector\n",
    "\n",
    "\n",
    "def reproduce_results(dataset, num_runs: int = 5, num_epochs=20, lr=0.01):\n",
    "    print(f\"Reproducing results for {dataset.name}\".center(80, '-') + '\\n')\n",
    "\n",
    "    metrics = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}\".center(80, '-'))\n",
    "\n",
    "        torch.random.manual_seed(run)\n",
    "        model, projector = get_model_and_projector(dataset)\n",
    "        train(dataset, model, projector, num_epochs=num_epochs, lr=lr)\n",
    "        run_metrics = evaluate(dataset, model, projector)\n",
    "        metrics.append(run_metrics)\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(\"Metrics mean\".center(80, '-'))\n",
    "    print(df.mean())\n",
    "\n",
    "    print(\"Metrics STD\".center(80, '-'))\n",
    "    print(df.std())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Reproducing results for kin8nm-------------------------\n",
      "\n",
      "-------------------------------------Run 1--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:46,  5.59s/it, ELBO=-37.3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreproduce_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 74\u001b[0m, in \u001b[0;36mreproduce_results\u001b[0;34m(dataset, num_runs, num_epochs, lr)\u001b[0m\n\u001b[1;32m     72\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(run)\n\u001b[1;32m     73\u001b[0m model, projector \u001b[38;5;241m=\u001b[39m get_model_and_projector(dataset)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m run_metrics \u001b[38;5;241m=\u001b[39m evaluate(dataset, model, projector)\n\u001b[1;32m     76\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(run_metrics)\n",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, projector, num_epochs, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, projector, optimizer, mll)\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m x, y \u001b[38;5;241m=\u001b[39m projector(x, y)\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_psd(output\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m mll(output, y)\n",
      "Cell \u001b[0;32mIn[31], line 38\u001b[0m, in \u001b[0;36mgpytorchSGP.__call__\u001b[0;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/models/approximate_gp.py:108\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[0;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m, in \u001b[0;36mSphericalHarmonicsVariationalStrategy.__call__\u001b[0;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prior:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mSphericalHarmonicsVariationalStrategy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m Linv_m, Linv_S_LTinv \u001b[38;5;241m=\u001b[39m Linv_qu\u001b[38;5;241m.\u001b[39mmean, Linv_qu\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# unwhitening + projection and vice-versa\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m LT_Phiux \u001b[38;5;241m=\u001b[39m \u001b[43mmatern_LT_Phi_from_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_levels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m Phixu_L \u001b[38;5;241m=\u001b[39m LT_Phiux\u001b[38;5;241m.\u001b[39mmT\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# posterior at x \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 172\u001b[0m, in \u001b[0;36mmatern_LT_Phi_from_kernel\u001b[0;34m(x, covar_module, num_levels)\u001b[0m\n\u001b[1;32m    169\u001b[0m d \u001b[38;5;241m=\u001b[39m base_kernel\u001b[38;5;241m.\u001b[39mspace\u001b[38;5;241m.\u001b[39mdimension \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    170\u001b[0m max_ell \u001b[38;5;241m=\u001b[39m num_levels\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatern_LT_Phi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_ell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 152\u001b[0m, in \u001b[0;36mmatern_LT_Phi\u001b[0;34m(x, max_ell, d, kappa, nu, sigma)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatern_LT_Phi\u001b[39m(x: Tensor, max_ell: \u001b[38;5;28mint\u001b[39m, d: \u001b[38;5;28mint\u001b[39m, kappa: \u001b[38;5;28mfloat\u001b[39m, nu: \u001b[38;5;28mfloat\u001b[39m, sigma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor: \n\u001b[0;32m--> 152\u001b[0m     Kux \u001b[38;5;241m=\u001b[39m \u001b[43mmatern_Kux\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_ell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [..., O, num_harmonics, N]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     ahat_sqrt \u001b[38;5;241m=\u001b[39m matern_repeated_ahat(max_ell\u001b[38;5;241m=\u001b[39mmax_ell, d\u001b[38;5;241m=\u001b[39md, kappa\u001b[38;5;241m=\u001b[39mkappa, nu\u001b[38;5;241m=\u001b[39mnu, sigma\u001b[38;5;241m=\u001b[39msigma)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;66;03m# [O, num_harmonics, 1]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Kux \u001b[38;5;241m*\u001b[39m ahat_sqrt\n",
      "Cell \u001b[0;32mIn[2], line 126\u001b[0m, in \u001b[0;36mmatern_Kux\u001b[0;34m(x, max_ell, d)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatern_Kux\u001b[39m(x: Tensor, max_ell: \u001b[38;5;28mint\u001b[39m, d: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor: \n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspherical_harmonics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_ell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmT\n",
      "Cell \u001b[0;32mIn[2], line 122\u001b[0m, in \u001b[0;36mspherical_harmonics\u001b[0;34m(x, max_ell, d)\u001b[0m\n\u001b[1;32m    119\u001b[0m f \u001b[38;5;241m=\u001b[39m SphericalHarmonics(dimension\u001b[38;5;241m=\u001b[39md, degrees\u001b[38;5;241m=\u001b[39mmax_ell \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [... * O, N, num_harmonics]\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Evaluate x and reintroduce batch dimensions\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mbatch_shape, n, total_num_harmonics(max_ell, d))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:80\u001b[0m, in \u001b[0;36mSphericalHarmonics.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mEvaluates each of the spherical harmonic level in the collection,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mand stacks the results.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m:return: [N, num harmonics in collection]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m harmonic: harmonic(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mharmonic_levels\n\u001b[1;32m     78\u001b[0m )  \u001b[38;5;66;03m# List of length `max_degree` with Tensor [num_harmonics_degree, N]\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mtranspose(B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(values), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:77\u001b[0m, in \u001b[0;36mSphericalHarmonics.__call__.<locals>.<lambda>\u001b[0;34m(harmonic)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     67\u001b[0m     x: B\u001b[38;5;241m.\u001b[39mNumeric,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Evaluates each of the spherical harmonic level in the collection,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    and stacks the results.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    :return: [N, num harmonics in collection]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m harmonic: \u001b[43mharmonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mharmonic_levels\n\u001b[1;32m     78\u001b[0m     )  \u001b[38;5;66;03m# List of length `max_degree` with Tensor [num_harmonics_degree, N]\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mtranspose(B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(values), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/spherical_harmonics/spherical_harmonics.py:151\u001b[0m, in \u001b[0;36mSphericalHarmonicsLevel.__call__\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: B\u001b[38;5;241m.\u001b[39mNumeric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B\u001b[38;5;241m.\u001b[39mNumeric:\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    :param X: M normalised (i.e. unit) D-dimensional vector, [N, D]\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: `X` evaluated at the M spherical harmonics in the set.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m        [\\phi_m(x_i)], shape [M, N]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     VXT \u001b[38;5;241m=\u001b[39m \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [M, N]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     zonals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgegenbauer(VXT)  \u001b[38;5;66;03m# [M, N]\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\u001b[38;5;241m.\u001b[39mmatmul(B\u001b[38;5;241m.\u001b[39mcast(B\u001b[38;5;241m.\u001b[39mdtype(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_inv), zonals)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/plum/function.py:399\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kw_args)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw_args):\n\u001b[1;32m    398\u001b[0m     method, return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_method_with_cache(args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert(\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m, return_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/lab/shape.py:185\u001b[0m, in \u001b[0;36mdispatch_unwrap_dimensions.<locals>.unwrapped_dispatch.<locals>.f_wrapped\u001b[0;34m(*args, **kw_args)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw_args):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munwrap_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdgp_requirements_test1/lib/python3.11/site-packages/lab/torch/linear_algebra.py:19\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, tr_a, tr_b)\u001b[0m\n\u001b[1;32m     17\u001b[0m a \u001b[38;5;241m=\u001b[39m transpose(a) \u001b[38;5;28;01mif\u001b[39;00m tr_a \u001b[38;5;28;01melse\u001b[39;00m a\n\u001b[1;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m transpose(b) \u001b[38;5;28;01mif\u001b[39;00m tr_b \u001b[38;5;28;01melse\u001b[39;00m b\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reproduce_results(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
