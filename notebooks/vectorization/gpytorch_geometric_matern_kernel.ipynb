{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mdgp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import gpytorch \n",
    "import geometric_kernels.torch \n",
    "from geometric_kernels.frontends.pytorch.gpytorch import GPytorchGeometricKernel\n",
    "from geometric_kernels.kernels.geometric_kernels import MaternKarhunenLoeveKernel\n",
    "from geometric_kernels.spaces import Hypersphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometricMaternKernel(GPytorchGeometricKernel): \n",
    "    def __init__(self, space, lengthscale=1.0, nu=2.5, trainable_nu=True, num_eigenfunctions=20, normalize=True, **kwargs): \n",
    "        geometric_kernel = MaternKarhunenLoeveKernel(\n",
    "            space=space, \n",
    "            num_eigenfunctions=num_eigenfunctions, \n",
    "            normalize=normalize, \n",
    "        )\n",
    "        super().__init__(geometric_kernel, lengthscale=lengthscale, nu=nu, trainable_nu=trainable_nu, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = Hypersphere(2)\n",
    "batch_shape = torch.Size([2])\n",
    "ard_num_dims = None\n",
    "nu = 2.5\n",
    "kernel = GeometricMaternKernel(space=space, batch_shape=batch_shape, ard_num_dims=ard_num_dims, nu=nu)\n",
    "rbf = gpytorch.kernels.MaternKernel(nu=nu, batch_shape=batch_shape, ard_num_dims=ard_num_dims)\n",
    "\n",
    "x1 = torch.randn(2, 11, 3)\n",
    "x2 = torch.randn(12, 3)\n",
    "x3 = torch.randn(3, 7, 3)\n",
    "x4 = x1.expand(10, *x1.shape)\n",
    "x5 = x2.expand(10, 2, *x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from geometric_kernels.spaces import Space\n",
    "from mdgp.frames import HypersphereFrame\n",
    "\n",
    "\n",
    "class ProjectToTangentIntrinsic(torch.nn.Module): \n",
    "    def __init__(self, space: Space, get_normal_vector=None) -> None: \n",
    "        super().__init__()\n",
    "        self.frame = HypersphereFrame(dim=space.dim, get_normal_vector=get_normal_vector)\n",
    "\n",
    "    def forward(self, x, coeff): \n",
    "        return self.frame.coeff_to_tangent(x=x, coeff=coeff)\n",
    "    \n",
    "\n",
    "class ProjectToTangentExtrinsic(torch.nn.Module):\n",
    "    def __init__(self, space: Space) -> None:\n",
    "        super().__init__()\n",
    "        self.manifold = space_to_manifold(space)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, coeff: torch.Tensor) -> torch.Tensor:\n",
    "        return self.manifold.proju(x=x, u=coeff)\n",
    "    \n",
    "\n",
    "def space_to_manifold(space: Space): \n",
    "    from geoopt import Sphere \n",
    "    if isinstance(space, Hypersphere): \n",
    "        return Sphere(torch.eye(space.dim + 1))\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "class ExponentialMap(torch.nn.Module): \n",
    "    def __init__(self, space: Space) -> None:\n",
    "        super().__init__()\n",
    "        self.manifold = space_to_manifold(space)\n",
    "\n",
    "    def forward(self, x, u): \n",
    "        return self.manifold.expmap(x=x, u=u)\n",
    "    \n",
    "\n",
    "class Retraction(torch.nn.Module): \n",
    "    def __init__(self, space: Space) -> None:\n",
    "        super().__init__()\n",
    "        self.manifold = space_to_manifold(space)\n",
    "\n",
    "    def forward(self, x, u): \n",
    "        return self.manifold.retr(x=x, u=u)\n",
    "    \n",
    "\n",
    "\n",
    "from torch import Tensor \n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy, VariationalStrategy\n",
    "from mdgp.samplers import RFFSampler, VISampler, PosteriorSampler, sample_naive\n",
    "\n",
    "\n",
    "class DeepGPLayer(gpytorch.models.deep_gps.DeepGPLayer):\n",
    "    def __init__(self, mean_module, covar_module, inducing_points, output_dims, learn_inducing_locations=False, whitened_variational_strategy=True):\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "        num_inducing_points, input_dims = inducing_points.shape\n",
    "\n",
    "        # Variational Parameters\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing_points,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "        variational_strategy_class = VariationalStrategy if whitened_variational_strategy else UnwhitenedVariationalStrategy\n",
    "        variational_strategy = variational_strategy_class(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=learn_inducing_locations\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        self.mean_module = mean_module\n",
    "        self.covar_module = covar_module\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)  \n",
    "    \n",
    "    def sample_naive(self, inputs, are_samples=False, **kwargs):\n",
    "        return sample_naive(super().__call__(inputs, are_samples=are_samples, **kwargs))\n",
    "\n",
    "    def sample_pathwise(self, inputs, are_samples=False):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, inputs, are_samples=False, sample=False, mean=False, **kwargs):\n",
    "        if mean: \n",
    "            with gpytorch.settings.num_likelihood_samples(1):\n",
    "                return super().__call__(inputs, are_samples=are_samples, **kwargs).mean[0]\n",
    "        if sample is None or sample is False: \n",
    "            return super().__call__(inputs, are_samples=are_samples, **kwargs)\n",
    "        if sample == 'naive':\n",
    "            return self.sample_naive(inputs=inputs, are_samples=are_samples, **kwargs)\n",
    "        if sample == 'pathwise': \n",
    "            return self.sample_pathwise(inputs=inputs, are_samples=are_samples)\n",
    "        raise NotImplementedError(f\"Expected sample argument to be either 'naive', 'pathwise', False, or None. Got {sample}\")\n",
    "        \n",
    "\n",
    "class GeometricDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        space, \n",
    "        num_eigenfunctions: int,\n",
    "        output_dims: int,\n",
    "        inducing_points: torch.Tensor,\n",
    "        nu: float = 2.5, \n",
    "        optimize_nu: bool = False,\n",
    "        feature_map: str = 'deterministic', \n",
    "        learn_inducing_locations: bool = False,\n",
    "        whitened_variational_strategy=False, \n",
    "        sampler_inv_jitter=10e-8,\n",
    "        outputscale_prior=None,\n",
    "    ) -> None: \n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "\n",
    "        # Initialize mean and kernel modules \n",
    "        mean_module = gpytorch.means.ConstantMean(\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        base_kernel = GeometricMaternKernel(\n",
    "            space=space, nu=nu, num_eigenfunctions=num_eigenfunctions, batch_shape=batch_shape, optimize_nu=optimize_nu\n",
    "        )\n",
    "        covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            base_kernel=base_kernel,\n",
    "            batch_shape=batch_shape,\n",
    "            outputscale_prior=outputscale_prior,\n",
    "        )\n",
    "        if outputscale_prior is not None: \n",
    "            covar_module.initialize(outputscale=outputscale_prior.mean)\n",
    "\n",
    "        super().__init__(mean_module=mean_module, covar_module=covar_module, inducing_points=inducing_points, output_dims=output_dims, learn_inducing_locations=learn_inducing_locations, whitened_variational_strategy=whitened_variational_strategy)\n",
    "\n",
    "        # Set up posterior sampler. VISampler needs the VariationalDistribution object for that the changing parameters are tracked properly\n",
    "        rff_sampler = None # RFFSampler(covar_module=covar_module, mean_module=mean_module, feature_map=feature_map)\n",
    "        vi_sampler = None # VISampler(variational_distribution=self.variational_strategy._variational_distribution)\n",
    "        self.sampler = None # PosteriorSampler(rff_sampler=rff_sampler, vi_sampler=vi_sampler, inducing_points=inducing_points, whitened_variational_strategy=whitened_variational_strategy, inv_jitter=sampler_inv_jitter)\n",
    "\n",
    "    def sample_pathwise(self, inputs, are_samples=False):\n",
    "        # Clear cache if training, since otherwise we risk \"trying to backward through the graph a second time\" errors \n",
    "        if self.training: \n",
    "            self.variational_strategy._clear_cache()\n",
    "        # Maybe initialize variational distribution (Taken from gpytorch.variational._VariationalStrategy.__call__)\n",
    "        if not self.variational_strategy.variational_params_initialized.item():\n",
    "            prior_dist = self.variational_strategy.prior_distribution\n",
    "            self.variational_strategy._variational_distribution.initialize_variational_distribution(prior_dist)\n",
    "            self.variational_strategy.variational_params_initialized.fill_(1)\n",
    "\n",
    "        # Take sample \n",
    "        sample_shape = torch.Size([gpytorch.settings.num_likelihood_samples.value()])\n",
    "        if are_samples: # [S, N, D]\n",
    "            inputs_head_shape = inputs.shape[1:-1]\n",
    "            inputs = inputs.flatten(start_dim=1, end_dim=-2)\n",
    "            sample = torch.stack([self.sampler(inputs_, sample_shape=torch.Size([])) for inputs_ in inputs.unbind(0)], dim=0)\n",
    "        else:\n",
    "            inputs_head_shape = inputs.shape[:-1]\n",
    "            inputs = inputs.flatten(start_dim=0, end_dim=-2)\n",
    "            sample = self.sampler(inputs, sample_shape=sample_shape)\n",
    "\n",
    "        # Reshape to [S, N, O]\n",
    "        if sample.dim() == 1: # [*N]\n",
    "            return sample.reshape(*inputs_head_shape) # [N]\n",
    "        if sample.dim() == 2: # [S, *N]. Sidenote: [O, *N] cannot happen because num_likelihood_samples is at least 1 \n",
    "            return sample.reshape(*sample_shape, *inputs_head_shape) # [S, N]\n",
    "        return sample.mT.reshape(*sample_shape, *inputs_head_shape, -1) # [S, O, *N] -> [S, *N, O] -> [S, N, O]\n",
    "\n",
    "\n",
    "class EuclideanDeepGPLayer(DeepGPLayer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            inducing_points, \n",
    "            output_dims, \n",
    "            mean_type='constant', \n",
    "            learn_inducing_locations=False, \n",
    "            nu=2.5,\n",
    "            constant_prior=None, \n",
    "            whitened_variational_strategy=True,\n",
    "            outputscale_prior=None,\n",
    "        ) -> None:\n",
    "        batch_shape = torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "        input_dims = inducing_points.size(-1)\n",
    "\n",
    "        # Mean \n",
    "        if mean_type == 'constant':\n",
    "            mean_module = gpytorch.means.ConstantMean(batch_shape=batch_shape, constant_prior=constant_prior)\n",
    "        else:\n",
    "            mean_module = gpytorch.means.LinearMean(input_dims)\n",
    "\n",
    "        # Covariance \n",
    "        base_kernel = gpytorch.kernels.MaternKernel(nu=nu, batch_shape=batch_shape, ard_num_dims=input_dims)\n",
    "        covar_module = gpytorch.kernels.ScaleKernel(base_kernel=base_kernel, batch_shape=batch_shape, ard_num_dims=None, outputscale_prior=outputscale_prior)\n",
    "        if outputscale_prior is not None: \n",
    "            covar_module.initialize(outputscale=outputscale_prior.mean)\n",
    "\n",
    "        super().__init__(mean_module=mean_module, covar_module=covar_module, inducing_points=inducing_points, output_dims=output_dims, learn_inducing_locations=learn_inducing_locations, whitened_variational_strategy=whitened_variational_strategy)\n",
    "\n",
    "\n",
    "class ManifoldToManifoldDeepGPLayer(torch.nn.Module): \n",
    "    def __init__(self, gp, space, project_to_tangent: str = 'intrinsic', tangent_to_manifold: str = 'exp', get_normal_vector='nn'): \n",
    "        assert project_to_tangent in {'intrinsic', 'extrinsic'}\n",
    "        assert tangent_to_manifold in {'exp', 'retr'}\n",
    "        super().__init__()\n",
    "        self.gp = gp \n",
    "\n",
    "        if project_to_tangent == 'intrinsic': \n",
    "            self.project_to_tangent = ProjectToTangentIntrinsic(space=space, get_normal_vector=get_normal_vector)\n",
    "        if project_to_tangent == 'extrinsic': \n",
    "            self.project_to_tangent = ProjectToTangentExtrinsic(space=space)\n",
    "\n",
    "        if tangent_to_manifold == 'exp': \n",
    "            self.tangent_to_manifold = ExponentialMap(space=space)\n",
    "        if tangent_to_manifold == 'retr': \n",
    "            self.tangent_to_manifold = Retraction(space=space)\n",
    "\n",
    "    def forward(self, x, are_samples=False, return_hidden=False, mean=False, sample='naive'): \n",
    "        coeff = self.gp(x, mean=mean, sample=sample, are_samples=are_samples)\n",
    "        u = self.project_to_tangent(x=x, coeff=coeff)\n",
    "        y = self.tangent_to_manifold(x=x, u=u)\n",
    "        if return_hidden: \n",
    "            return {'coefficients': coeff, 'tangent': u, 'manifold': y}\n",
    "        return y\n",
    "\n",
    "\n",
    "from geometric_kernels.spaces import Space\n",
    "from mdgp.utils import extrinsic_dimension\n",
    "\n",
    "\n",
    "class ManifoldDeepGP(gpytorch.models.deep_gps.DeepGP): \n",
    "\n",
    "    def __init__(self, hidden_gps, output_gp, space, project_to_tangent='instrinsic', tangent_to_manifold='exp', parametrised_frame=False):\n",
    "        get_normal_vector = 'nn' if parametrised_frame is True else None\n",
    "        super().__init__()\n",
    "        self.hidden_layers = torch.nn.ModuleList([\n",
    "            ManifoldToManifoldDeepGPLayer(gp=gp, space=space, project_to_tangent=project_to_tangent, tangent_to_manifold=tangent_to_manifold, get_normal_vector=get_normal_vector)\n",
    "            for gp in hidden_gps\n",
    "        ])\n",
    "        self.output_layer = output_gp\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    def forward_return_hidden(self, x: Tensor, are_samples: bool = False, sample_hidden: str = 'naive', sample_output=False, mean=False):\n",
    "        hidden_factors = []\n",
    "        for hidden_layer in self.hidden_layers: \n",
    "            hidden_dict = hidden_layer(x=x, are_samples=are_samples, sample=sample_hidden, mean=mean, return_hidden=True)\n",
    "            hidden_factors.append(hidden_dict)\n",
    "            x = hidden_dict['manifold']\n",
    "            are_samples = False if mean else True \n",
    "        y = self.output_layer(x, are_samples=are_samples, sample=sample_output, mean=mean)\n",
    "        return hidden_factors, y \n",
    "\n",
    "    def forward(self, x: Tensor, are_samples: bool = False, sample_hidden: str = 'naive', sample_output=False, mean=False):\n",
    "        for hidden_layer in self.hidden_layers: \n",
    "            x = hidden_layer(x, are_samples=are_samples, sample=sample_hidden, mean=mean)\n",
    "            are_samples = False if mean else True \n",
    "        return self.output_layer(x, are_samples=are_samples, sample=sample_output, mean=mean)\n",
    "    \n",
    "\n",
    "class GeometricManifoldDeepGP(ManifoldDeepGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        space, \n",
    "        num_hidden: int,\n",
    "        inducing_points, # [N, 3]\n",
    "        output_dims=None, \n",
    "        num_eigenfunctions: int = 20, \n",
    "        nu: float = 2.5, \n",
    "        feature_map = 'deterministic',\n",
    "        learn_inducing_locations: bool = False, \n",
    "        project_to_tangent: str = 'intrinsic',\n",
    "        tangent_to_manifold: str = 'exp',\n",
    "        optimize_nu: bool = False,\n",
    "        whitened_variational_strategy=True, \n",
    "        sampler_inv_jitter=10e-8,\n",
    "        outputscale_prior=None,\n",
    "        parametrised_frame=False, \n",
    "        ) -> None:\n",
    "\n",
    "        # Dimension of the manifold is the last dimension of the inducing points\n",
    "        if project_to_tangent == 'intrinsic': \n",
    "            hidden_output_dims = space.dim \n",
    "        elif project_to_tangent == 'extrinsic': \n",
    "            hidden_output_dims = extrinsic_dimension(space)\n",
    "        else: \n",
    "            raise NotImplementedError(f\"Expected project_to_tangent either 'intrinsic' or 'extrinsic'. Got {project_to_tangent}.\")\n",
    "\n",
    "\n",
    "        hidden_gps = [\n",
    "            GeometricDeepGPLayer(\n",
    "                space=space,\n",
    "                num_eigenfunctions=num_eigenfunctions,\n",
    "                output_dims=hidden_output_dims,\n",
    "                inducing_points=inducing_points,\n",
    "                nu=nu, \n",
    "                feature_map=feature_map,\n",
    "                learn_inducing_locations=learn_inducing_locations,\n",
    "                optimize_nu=optimize_nu, \n",
    "                whitened_variational_strategy=whitened_variational_strategy,\n",
    "                sampler_inv_jitter=sampler_inv_jitter, \n",
    "                outputscale_prior=outputscale_prior,\n",
    "            )\n",
    "            for _ in range(num_hidden)\n",
    "        ]\n",
    "\n",
    "        output_gp = GeometricDeepGPLayer(\n",
    "            space=space,\n",
    "            num_eigenfunctions=num_eigenfunctions,\n",
    "            output_dims=output_dims,\n",
    "            inducing_points=inducing_points,\n",
    "            nu=nu, \n",
    "            feature_map=feature_map,\n",
    "            learn_inducing_locations=learn_inducing_locations,\n",
    "            optimize_nu=optimize_nu, \n",
    "            whitened_variational_strategy=whitened_variational_strategy,\n",
    "            sampler_inv_jitter=sampler_inv_jitter,\n",
    "        )\n",
    "\n",
    "        super().__init__(hidden_gps=hidden_gps, output_gp=output_gp, project_to_tangent=project_to_tangent, tangent_to_manifold=tangent_to_manifold, space=space, parametrised_frame=parametrised_frame)\n",
    "\n",
    "\n",
    "class EuclideanManifoldDeepGP(ManifoldDeepGP):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        space: Space,\n",
    "        num_hidden: int,\n",
    "        inducing_points,\n",
    "        output_dims=None, \n",
    "        nu: float = 2.5, \n",
    "        learn_inducing_locations: bool = False, \n",
    "        project_to_tangent='intrinsic', \n",
    "        tangent_to_manifold='exp',\n",
    "        outputscale_prior=None,\n",
    "        parametrised_frame=False,\n",
    "        ) -> None:\n",
    "        if project_to_tangent == 'intrinsic': \n",
    "            hidden_output_dims = space.dim \n",
    "        elif project_to_tangent == 'extrinsic': \n",
    "            hidden_output_dims = extrinsic_dimension(space)\n",
    "        else: \n",
    "            raise NotImplementedError(f\"Expected project_to_tangent either 'intrinsic' or 'extrinsic'. Got {project_to_tangent}.\")\n",
    "\n",
    "        hidden_gps = [\n",
    "            EuclideanDeepGPLayer(\n",
    "                output_dims=hidden_output_dims,\n",
    "                inducing_points=inducing_points,\n",
    "                nu=nu, \n",
    "                learn_inducing_locations=learn_inducing_locations,\n",
    "                mean_type='constant',\n",
    "                outputscale_prior=outputscale_prior,\n",
    "            )\n",
    "            for _ in range(num_hidden)\n",
    "        ]\n",
    "\n",
    "        output_gp = EuclideanDeepGPLayer(\n",
    "            output_dims=output_dims,\n",
    "            inducing_points=inducing_points,\n",
    "            nu=nu, \n",
    "            learn_inducing_locations=learn_inducing_locations,\n",
    "            mean_type='constant',\n",
    "        )\n",
    "        super().__init__(hidden_gps=hidden_gps, output_gp=output_gp, project_to_tangent=project_to_tangent, tangent_to_manifold=tangent_to_manifold, space=space, parametrised_frame=parametrised_frame)\n",
    "\n",
    "\n",
    "class EuclideanDeepGP:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden: int,\n",
    "        inducing_points,\n",
    "        output_dims = None, \n",
    "        nu: float = 2.5, \n",
    "        learn_inducing_locations: bool = False, \n",
    "        outputscale_prior=None,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        # Dimension of the manifold is the last dimension of the inducing points\n",
    "        hidden_output_dims = inducing_points.shape[-1]\n",
    "\n",
    "        self.hidden_gp_layers = [\n",
    "            EuclideanDeepGPLayer(\n",
    "                output_dims=hidden_output_dims,\n",
    "                inducing_points=inducing_points,\n",
    "                nu=nu, \n",
    "                learn_inducing_locations=learn_inducing_locations,\n",
    "                mean_type='linear',\n",
    "                outputscale_prior=outputscale_prior,\n",
    "            )\n",
    "            for _ in range(num_hidden)\n",
    "        ]\n",
    "        self.output_gp_layer = EuclideanDeepGPLayer(\n",
    "            output_dims=output_dims,\n",
    "            inducing_points=inducing_points,\n",
    "            nu=nu, \n",
    "            learn_inducing_locations=learn_inducing_locations,\n",
    "            mean_type='constant',\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs: Tensor, are_samples: bool = False, sample_hidden: str = 'naive', sample_output=False, mean=False, **kwargs):\n",
    "        for gp_layer in self.hidden_gp_layers:\n",
    "            inputs = gp_layer(inputs, are_samples=are_samples, sample=sample_hidden, mean=mean)\n",
    "            are_samples = False if mean else True\n",
    "        return self.output_gp_layer(inputs, are_samples=are_samples, sample=sample_output, mean=mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgp.utils import sphere_uniform_grid\n",
    "\n",
    "\n",
    "space = Hypersphere(2)\n",
    "inducing_points = sphere_uniform_grid(60)\n",
    "\n",
    "models = [\n",
    "    GeometricManifoldDeepGP(space=space, num_hidden=h, inducing_points=inducing_points)\n",
    "    for h in range(4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layers: 0\n",
      "13.4 ms ± 3.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 1\n",
      "64.4 ms ± 4.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 2\n",
      "130 ms ± 9.39 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 3\n",
      "173 ms ± 9.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = sphere_uniform_grid(400)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "for h, model in enumerate(models):\n",
    "    print(f\"Hidden layers: {h}\")\n",
    "    %timeit -n 10 model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layers: 0\n",
      "13.2 ms ± 3.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 1\n",
      "The slowest run took 13.95 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "338 ms ± 531 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 2\n",
      "329 ms ± 20.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Hidden layers: 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x = sphere_uniform_grid(400)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "for h, model in enumerate(models):\n",
    "    print(f\"Hidden layers: {h}\")\n",
    "    %timeit -n 10 model(x).mean.mean().backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
